{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16e2a45a-659a-4de3-bbd1-204b1da5456c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2105cf9-4115-45cc-a055-33e02c9340e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import copernicusmarine\n",
    "import dask\n",
    "from datetime import datetime, timedelta\n",
    "import exactextract as ee\n",
    "from exactextract import exact_extract\n",
    "import gc\n",
    "import geopandas as gpd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.features import geometry_mask\n",
    "import rioxarray as rxr\n",
    "from shapely.geometry import mapping, shape\n",
    "from shapely.geometry import mapping, Point\n",
    "from scipy.spatial import cKDTree\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "\n",
    "os.chdir(\"/media/marieke/Shared/Chap-1/Model/Scripts/Chap_1_2018-2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15621d65-5f08-4e38-be69-f94ffa2d7949",
   "metadata": {},
   "source": [
    "# Get Copernicus data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311e03f5-6e1b-4221-a1cd-44a4ebf95509",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chlorophyll 1km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57932f92-d636-48f7-aff3-aa68fa91c962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2025-10-17T15:11:26Z - Selected dataset version: \"202311\"\n",
      "INFO - 2025-10-17T15:11:26Z - Selected dataset part: \"default\"\n",
      "INFO - 2025-10-17T15:11:26Z - Downloading Copernicus Marine data requires a Copernicus Marine username and password, sign up for free at: https://data.marine.copernicus.eu/register\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copernicus Marine username:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  mschultz2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copernicus Marine password:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "data_request = {\n",
    "   \"dataset_id\" : \"cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D\",\n",
    "   \"longitude\" : [3, 9.65], \n",
    "   \"latitude\" : [41.2, 44],\n",
    "   \"time\" : [\"2013-01-01\", \"2025-01-01\"],\n",
    "   \"variables\" : [\"CHL\"]\n",
    "}\n",
    "\n",
    "# Load xarray dataset\n",
    "chl = copernicusmarine.open_dataset(\n",
    "    dataset_id = data_request[\"dataset_id\"],\n",
    "    minimum_longitude = data_request[\"longitude\"][0],\n",
    "    maximum_longitude = data_request[\"longitude\"][1],\n",
    "    minimum_latitude = data_request[\"latitude\"][0],\n",
    "    maximum_latitude = data_request[\"latitude\"][1],\n",
    "    start_datetime = data_request[\"time\"][0],\n",
    "    end_datetime = data_request[\"time\"][1],\n",
    "    variables = data_request[\"variables\"]\n",
    ")\n",
    "\n",
    "# Export to NCDF \n",
    "chl.to_netcdf(\"./data/raw_data/predictors/Chl/cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D_20130101-20250101.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57523d3e-005b-4360-a5c0-fff65854b487",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### pH, oxygen 4.2 km"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7305b40c-efb8-4944-8fa1-2545f9032314",
   "metadata": {},
   "source": [
    "https://data.marine.copernicus.eu/product/MEDSEA_MULTIYEAR_BGC_006_008/description\n",
    "\n",
    "O2 # STOPS at 2023-05-31\n",
    "med-ogs-bio-rean-d \n",
    "\n",
    "pH\n",
    "med-ogs-car-rean-d # STOPS at 2023-05-31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff4567-f1e4-4716-accf-5f6e90147c73",
   "metadata": {},
   "source": [
    "#### pH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009ecb64-5aea-48d4-b127-2d80a5a7153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2025-10-20T15:09:34Z - Selected dataset version: \"202105\"\n",
      "INFO - 2025-10-20T15:09:34Z - Selected dataset part: \"default\"\n",
      "WARNING - 2025-10-20T15:09:34Z - You are using the dataset med-ogs-bio-rean-d, version '202105', part 'default'. This exact version and part of the dataset will be retired on the 2025-11-25T00:00:00.000Z. For more information you can check: https://marine.copernicus.eu/user-corner/product-roadmap/transition-information\n",
      "INFO - 2025-10-20T15:09:34Z - Downloading Copernicus Marine data requires a Copernicus Marine username and password, sign up for free at: https://data.marine.copernicus.eu/register\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copernicus Marine username:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  mschultz2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copernicus Marine password:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 2025-10-20T15:09:43Z - Some of your subset selection [2013-01-01 00:00:00+00:00, 2025-01-01 00:00:00+00:00] for the time dimension exceed the dataset coordinates [1999-01-01 00:00:00+00:00, 2023-05-31 00:00:00+00:00]\n",
      "WARNING - 2025-10-20T15:09:43Z - Some of your subset selection [2013-01-01 00:00:00+00:00, 2025-01-01 00:00:00+00:00] for the time dimension exceed the dataset coordinates [1999-01-01 00:00:00+00:00, 2023-05-31 00:00:00+00:00]\n"
     ]
    },
    {
     "ename": "VariableDoesNotExistInTheDataset",
     "evalue": "The variable '02' is neither a variable or a standard name in the dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mVariableDoesNotExistInTheDataset\u001b[0m          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m data_request \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_id\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmed-ogs-bio-rean-d\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m9.65\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m02\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load xarray dataset\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m ox \u001b[38;5;241m=\u001b[39m \u001b[43mcopernicusmarine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminimum_longitude\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlongitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximum_longitude\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlongitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminimum_latitude\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximum_latitude\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_datetime\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_datetime\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvariables\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Export to NCDF \u001b[39;00m\n\u001b[1;32m     23\u001b[0m ph\u001b[38;5;241m.\u001b[39mto_netcdf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/raw_data/predictors/oxygen/med-ogs-bio-rean-d _20130101-20250101.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/core_functions/deprecated_options.py:78\u001b[0m, in \u001b[0;36mdeprecated_python_option.<locals>.deco.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     77\u001b[0m     rename_kwargs(f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, kwargs, aliases)\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/python_interface/exception_handler.py:17\u001b[0m, in \u001b[0;36mlog_exception_and_exit.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbort\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/python_interface/exception_handler.py:13\u001b[0m, in \u001b[0;36mlog_exception_and_exit.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m click\u001b[38;5;241m.\u001b[39mAbort:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbort\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/python_interface/open_dataset.py:195\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(dataset_id, dataset_version, dataset_part, username, password, variables, minimum_longitude, maximum_longitude, minimum_latitude, maximum_latitude, maximum_x, minimum_x, maximum_y, minimum_y, minimum_depth, maximum_depth, vertical_axis, start_datetime, end_datetime, coordinates_selection_method, service, credentials_file, chunk_size_limit)\u001b[0m\n\u001b[1;32m    153\u001b[0m geographicalparameters \u001b[38;5;241m=\u001b[39m GeographicalParameters(\n\u001b[1;32m    154\u001b[0m     y_axis_parameters\u001b[38;5;241m=\u001b[39mYParameters(\n\u001b[1;32m    155\u001b[0m         minimum_y\u001b[38;5;241m=\u001b[39mminimum_y_axis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m     ),\n\u001b[1;32m    171\u001b[0m )\n\u001b[1;32m    173\u001b[0m load_request \u001b[38;5;241m=\u001b[39m LoadRequest(\n\u001b[1;32m    174\u001b[0m     dataset_id\u001b[38;5;241m=\u001b[39mdataset_id,\n\u001b[1;32m    175\u001b[0m     force_dataset_version\u001b[38;5;241m=\u001b[39mdataset_version,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     credentials_file\u001b[38;5;241m=\u001b[39mcredentials_file,\n\u001b[1;32m    193\u001b[0m )\n\u001b[0;32m--> 195\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_object_from_load_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopen_dataset_from_arco_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks_factor_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommand_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCommandType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOPEN_DATASET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/python_interface/load_utils.py:126\u001b[0m, in \u001b[0;36mload_data_object_from_load_request\u001b[0;34m(load_request, arco_series_load_function, chunks_factor_size_limit, command_type)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         optimum_dask_chunking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43marco_series_load_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43musername\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musername\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeographical_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeographical_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemporal_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemporal_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdepth_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoordinates_selection_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoordinates_selection_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimum_dask_chunking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimum_dask_chunking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceNotSupported(retrieval_service\u001b[38;5;241m.\u001b[39mservice_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/download_functions/download_zarr.py:281\u001b[0m, in \u001b[0;36mopen_dataset_from_arco_series\u001b[0;34m(username, password, dataset_url, variables, geographical_parameters, temporal_parameters, depth_parameters, coordinates_selection_method, optimum_dask_chunking)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m dataset[variable]\u001b[38;5;241m.\u001b[39mencoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 281\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43msubset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeographical_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeographical_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemporal_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemporal_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoordinates_selection_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoordinates_selection_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcoords \u001b[38;5;129;01mand\u001b[39;00m optimum_dask_chunking:\n\u001b[1;32m    290\u001b[0m     optimum_chunks_depth \u001b[38;5;241m=\u001b[39m deepcopy(optimum_dask_chunking)\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/download_functions/subset_xarray.py:588\u001b[0m, in \u001b[0;36msubset\u001b[0;34m(dataset, variables, geographical_parameters, temporal_parameters, depth_parameters, coordinates_selection_method)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubset\u001b[39m(\n\u001b[1;32m    580\u001b[0m     dataset: xarray\u001b[38;5;241m.\u001b[39mDataset,\n\u001b[1;32m    581\u001b[0m     variables: Optional[List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    585\u001b[0m     coordinates_selection_method: CoordinatesSelectionMethod,\n\u001b[1;32m    586\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xarray\u001b[38;5;241m.\u001b[39mDataset:\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m variables:\n\u001b[0;32m--> 588\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43m_variables_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m _y_axis_subset(\n\u001b[1;32m    590\u001b[0m         dataset,\n\u001b[1;32m    591\u001b[0m         geographical_parameters\u001b[38;5;241m.\u001b[39my_axis_parameters,\n\u001b[1;32m    592\u001b[0m         coordinates_selection_method,\n\u001b[1;32m    593\u001b[0m     )\n\u001b[1;32m    594\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m _x_axis_subset(\n\u001b[1;32m    595\u001b[0m         dataset,\n\u001b[1;32m    596\u001b[0m         geographical_parameters\u001b[38;5;241m.\u001b[39mx_axis_parameters,\n\u001b[1;32m    597\u001b[0m         coordinates_selection_method,\n\u001b[1;32m    598\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/download_functions/subset_xarray.py:520\u001b[0m, in \u001b[0;36m_variables_subset\u001b[0;34m(dataset, variables)\u001b[0m\n\u001b[1;32m    516\u001b[0m             dataset_variables_filter\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    517\u001b[0m                 variable_name_from_standard_name\n\u001b[1;32m    518\u001b[0m             )\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m VariableDoesNotExistInTheDataset(variable)\n\u001b[1;32m    521\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset[numpy\u001b[38;5;241m.\u001b[39marray(dataset_variables_filter)]\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _update_variables_attributes(dataset, dataset_variables_filter)\n",
      "\u001b[0;31mVariableDoesNotExistInTheDataset\u001b[0m: The variable '02' is neither a variable or a standard name in the dataset."
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "data_request = {\n",
    "   \"dataset_id\" : \"med-ogs-bio-rean-d\",\n",
    "   \"longitude\" : [3, 9.65], \n",
    "   \"latitude\" : [41.2, 44],\n",
    "   \"time\" : [\"2013-01-01\", \"2025-01-01\"],\n",
    "   \"variables\" : [\"02\"]\n",
    "}\n",
    "\n",
    "# Load xarray dataset\n",
    "ox = copernicusmarine.open_dataset(\n",
    "    dataset_id = data_request[\"dataset_id\"],\n",
    "    minimum_longitude = data_request[\"longitude\"][0],\n",
    "    maximum_longitude = data_request[\"longitude\"][1],\n",
    "    minimum_latitude = data_request[\"latitude\"][0],\n",
    "    maximum_latitude = data_request[\"latitude\"][1],\n",
    "    start_datetime = data_request[\"time\"][0],\n",
    "    end_datetime = data_request[\"time\"][1],\n",
    "    variables = data_request[\"variables\"]\n",
    ")\n",
    "\n",
    "# Export to NCDF \n",
    "ph.to_netcdf(\"./data/raw_data/predictors/oxygen/med-ogs-bio-rean-d _20130101-20250101.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbc633-cea7-4940-9018-b55473123d49",
   "metadata": {},
   "source": [
    "#### oxygen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712014e1-9e08-41cb-bccc-e1bcabd02422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "data_request = {\n",
    "   \"dataset_id\" : \"cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D\",\n",
    "   \"longitude\" : [3, 9.65], \n",
    "   \"latitude\" : [41.2, 44],\n",
    "   \"time\" : [\"2013-01-01\", \"2025-01-01\"],\n",
    "   \"variables\" : [\"CHL\"]\n",
    "}\n",
    "\n",
    "# Load xarray dataset\n",
    "chl = copernicusmarine.open_dataset(\n",
    "    dataset_id = data_request[\"dataset_id\"],\n",
    "    minimum_longitude = data_request[\"longitude\"][0],\n",
    "    maximum_longitude = data_request[\"longitude\"][1],\n",
    "    minimum_latitude = data_request[\"latitude\"][0],\n",
    "    maximum_latitude = data_request[\"latitude\"][1],\n",
    "    start_datetime = data_request[\"time\"][0],\n",
    "    end_datetime = data_request[\"time\"][1],\n",
    "    variables = data_request[\"variables\"]\n",
    ")\n",
    "\n",
    "# Export to NCDF \n",
    "chl.to_netcdf(\"./data/raw_data/predictors/Chl/cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D_20130101-20250101.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f685ee-b6bf-45fc-817a-e7c7d3078cb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### SST 1km (2008-2025)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2859e0f4-300a-4c4b-b657-adf4100b336e",
   "metadata": {},
   "source": [
    "https://data.marine.copernicus.eu/product/SST_MED_SST_L4_NRT_OBSERVATIONS_010_004/description\n",
    "Analysis : 0\n",
    "SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2\n",
    "\n",
    "Anomaly : \n",
    "SST_MED_SSTA_L4_NRT_OBSERVATIONS_010_004_d \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2221348f-654d-4b66-b049-ac0f22da3dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2025-10-20T15:15:14Z - Selected dataset version: \"202311\"\n",
      "INFO - 2025-10-20T15:15:14Z - Selected dataset part: \"default\"\n",
      "INFO - 2025-10-20T15:15:14Z - Downloading Copernicus Marine data requires a Copernicus Marine username and password, sign up for free at: https://data.marine.copernicus.eu/register\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copernicus Marine username:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  mschultz2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copernicus Marine password:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marieke/miniconda3/envs/spyder-env/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:660: UserWarning: endian-ness of dtype and endian kwarg do not match, using endian kwarg\n",
      "  nc4_var = self.ds.createVariable(**default_args)\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "data_request = {\n",
    "   \"dataset_id\" : \"SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2\",\n",
    "   \"longitude\" : [3, 9.65], \n",
    "   \"latitude\" : [41.2, 44],\n",
    "   \"time\" : [\"2013-01-01\", \"2025-01-01\"],\n",
    "   \"variables\" : [\"analysed_sst\"]\n",
    "}\n",
    "\n",
    "# Load xarray dataset\n",
    "sst = copernicusmarine.open_dataset(\n",
    "    dataset_id = data_request[\"dataset_id\"],\n",
    "    minimum_longitude = data_request[\"longitude\"][0],\n",
    "    maximum_longitude = data_request[\"longitude\"][1],\n",
    "    minimum_latitude = data_request[\"latitude\"][0],\n",
    "    maximum_latitude = data_request[\"latitude\"][1],\n",
    "    start_datetime = data_request[\"time\"][0],\n",
    "    end_datetime = data_request[\"time\"][1],\n",
    "    variables = data_request[\"variables\"]\n",
    ")\n",
    "\n",
    "# Export to NCDF \n",
    "sst.to_netcdf(\"./data/raw_data/predictors/SST/SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2_SST_20130101-20250101.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8997d777-8170-4c50-8f57-1865979df745",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Ocean mixed layer thickness 4.2km (1987-2025)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b0cdf0b-7f0c-423d-ae13-48f12aa6ab42",
   "metadata": {},
   "source": [
    "https://data.marine.copernicus.eu/product/MEDSEA_MULTIYEAR_PHY_006_004/description\n",
    "\n",
    "\n",
    "# Monthly data\n",
    "med-cmcc-mld-rean-m # until 2023\n",
    "med-cmcc-mld-int-m # 2021-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69705e96-12de-49e4-b772-92c0ced07e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "data_request = {\n",
    "   \"dataset_id\" : \"SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2\",\n",
    "   \"longitude\" : [3, 9.65], \n",
    "   \"latitude\" : [41.2, 44],\n",
    "   \"time\" : [\"2013-01-01\", \"2025-01-01\"],\n",
    "   \"variables\" : [\"analysed_sst\"]\n",
    "}\n",
    "\n",
    "# Load xarray dataset\n",
    "sst = copernicusmarine.open_dataset(\n",
    "    dataset_id = data_request[\"dataset_id\"],\n",
    "    minimum_longitude = data_request[\"longitude\"][0],\n",
    "    maximum_longitude = data_request[\"longitude\"][1],\n",
    "    minimum_latitude = data_request[\"latitude\"][0],\n",
    "    maximum_latitude = data_request[\"latitude\"][1],\n",
    "    start_datetime = data_request[\"time\"][0],\n",
    "    end_datetime = data_request[\"time\"][1],\n",
    "    variables = data_request[\"variables\"]\n",
    ")\n",
    "\n",
    "# Export to NCDF \n",
    "sst.to_netcdf(\"./data/raw_data/predictors/SST/SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2_SST_20130101-20250101.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d22960e-0a76-4e9b-945f-49cb6ba7c83c",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745b96b-ae63-475d-91c4-911ebbdcd543",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c7a4aa-f2d4-4d1c-8e40-952f40fbbb64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline with exactextract\n",
    "\n",
    "def get_dates(date, time_step):\n",
    "    \"\"\"\n",
    "    Calculate the range of dates for a given time step relative to the provided date.\n",
    "    \"\"\"\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "\n",
    "    end_date = date - timedelta(days=1)\n",
    "\n",
    "    time_deltas = {\n",
    "        'day': 1,\n",
    "        'week': 7,\n",
    "        'month': 30,\n",
    "        'year': 365,\n",
    "        '5years': 5 * 365 + 1,\n",
    "    }\n",
    "\n",
    "    if time_step not in time_deltas:\n",
    "        raise ValueError(\"Unsupported time step. Choose from 'day', 'week', 'month', 'year', or '5years'.\")\n",
    "\n",
    "    start_date = date - timedelta(days=time_deltas[time_step])\n",
    "    return start_date, end_date\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_stats(data_array, shape_geometry):\n",
    "    \"\"\"\n",
    "    Extract values and compute weighted mean - automatically with exactextract- , min and max.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        feature = {\"type\": \"Feature\", \"geometry\": mapping(shape_geometry), \"properties\": {}}\n",
    "        res = exact_extract(data_array, [feature], [\"mean\", \"min\", \"max\"])\n",
    "\n",
    "\n",
    "        if not res or len(res) == 0:\n",
    "            return None, None, None\n",
    "\n",
    "        props = res[0][\"properties\"]\n",
    "\n",
    "        # Multi-band keys: band_1_mean, band_2_mean, etc.\n",
    "        mean_vals = [v for k, v in props.items() if k.endswith(\"_mean\")]\n",
    "        min_vals  = [v for k, v in props.items() if k.endswith(\"_min\")]\n",
    "        max_vals  = [v for k, v in props.items() if k.endswith(\"_max\")]\n",
    "\n",
    "         # Single-band keys: mean, min, max\n",
    "        if not mean_vals:\n",
    "            if \"mean\" in props:\n",
    "                mean_vals = [props[\"mean\"]]\n",
    "        if not min_vals:\n",
    "            if \"min\" in props:\n",
    "                min_vals = [props[\"min\"]]\n",
    "        if not max_vals:\n",
    "            if \"max\" in props:\n",
    "                max_vals = [props[\"max\"]]\n",
    "   \n",
    "\n",
    "        if not mean_vals or not min_vals or not max_vals:\n",
    "            return None, None, None\n",
    "\n",
    "        mean_val = float(np.nanmean(mean_vals))\n",
    "        min_val  = float(np.nanmin(min_vals))\n",
    "        max_val  = float(np.nanmax(max_vals))\n",
    "\n",
    "        return mean_val, min_val, max_val\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"compute_stats ERROR: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def open_nc(shape_geometry, date, netcdf_path, variable=\"CHL\"):\n",
    "    \"\"\"\n",
    "    Compute NCDF statistics for a given geometry and date using a netCDF file.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    try:\n",
    "        if isinstance(date, str):\n",
    "            date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "\n",
    "        ds = xr.open_dataset(netcdf_path)\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "\n",
    "        target_date = date - timedelta(days=1)\n",
    "        time_steps = [\"day\", \"week\", \"month\", \"year\", \"5years\"]\n",
    "        date_ranges = {label: get_dates(date, label) for label in time_steps}\n",
    "\n",
    "\n",
    "        for label, (start_date, end_date) in date_ranges.items():\n",
    "            ds_time_range = ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "            if ds_time_range.time.size == 0:\n",
    "                results[label] = (None, None, None, 0)\n",
    "                continue\n",
    "\n",
    "            chl_data = ds_time_range[variable]\n",
    "            valid_data = chl_data.dropna(dim=\"time\", how=\"all\")\n",
    "\n",
    "\n",
    "            if valid_data.size > 0:                 \n",
    "                mean_val, min_val, max_val = compute_stats(valid_data, shape_geometry)                \n",
    "                results[label] = (mean_val, min_val, max_val)\n",
    "            else:\n",
    "                print(\"valid_data.size == 0\")\n",
    "                results[label] = (None, None, None)\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing shape with target date: {date}: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_geojson(geojson_path, netcdf_path, output_path, variable=\"CHL\"):\n",
    "    \"\"\"\n",
    "    Process the GeoJSON file and compute statistics for each shape using a netCDF file.\n",
    "    \"\"\"\n",
    "    shapes = gpd.read_file(geojson_path)\n",
    "    shapes = shapes.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(shapes.iterrows(), total=shapes.shape[0], desc=\"Processing shapes\"):\n",
    "        shape_geometry = row.geometry\n",
    "        date = row[\"date\"]\n",
    "        polygon_id = row.get(\"replicates\", None)\n",
    "\n",
    "        nc_stats = open_nc(shape_geometry, date, netcdf_path, variable)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        result_entry = {\"replicates\": polygon_id}\n",
    "        for label, (mean, min_val, max_val) in nc_stats.items():\n",
    "            result_entry[f\"Cop_{variable}_{label}_mean\"] = mean\n",
    "            result_entry[f\"Cop_{variable}_{label}_min\"] = min_val\n",
    "            result_entry[f\"Cop_{variable}_{label}_max\"] = max_val\n",
    "\n",
    "        results.append(result_entry)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb251dc5-180e-44ab-8cd8-3fb2c13e02ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(date, time_step):\n",
    "    from datetime import datetime, timedelta\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    end_date = date - timedelta(days=1)\n",
    "    time_deltas = {\n",
    "        'day': 1,\n",
    "        'week': 7,\n",
    "        'month': 30,\n",
    "        'year': 365,\n",
    "        '5years': 5 * 365 + 1,\n",
    "    }\n",
    "    if time_step not in time_deltas:\n",
    "        raise ValueError(\"Unsupported time step.\")\n",
    "    start_date = date - timedelta(days=time_deltas[time_step])\n",
    "    return start_date, end_date\n",
    "\n",
    "\n",
    "def compute_stats_from_2d_raster(raster_2d, shape_geometry, agg_name=\"mean\"):\n",
    "    \"\"\"\n",
    "    raster_2d: 2D numpy array or xarray DataArray (single band, lat x lon)\n",
    "    shape_geometry: shapely geometry\n",
    "    agg_name: just for debug\n",
    "    Returns: (mean, min, max) for the polygon based on exact_extract output\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If xarray DataArray, get numpy\n",
    "        if hasattr(raster_2d, \"values\"):\n",
    "            arr = raster_2d.values\n",
    "        else:\n",
    "            arr = raster_2d\n",
    "\n",
    "        # exact_extract expects raster input with spatial metadata, but many python bindings accept numpy + transform.\n",
    "        # The existing code used exact_extract(data_array, [feature], [\"mean\",\"min\",\"max\"])\n",
    "        # We'll try to keep the same interface by passing the xarray.DataArray if available.\n",
    "        feature = {\"type\": \"Feature\", \"geometry\": mapping(shape_geometry), \"properties\": {}}\n",
    "        res = exact_extract(raster_2d, [feature], [\"mean\", \"min\", \"max\"])\n",
    "\n",
    "        if not res or len(res) == 0:\n",
    "            return None, None, None\n",
    "        props = res[0][\"properties\"]\n",
    "\n",
    "        # Support multi-band keys if present\n",
    "        mean_vals = [v for k, v in props.items() if k.endswith(\"_mean\")]\n",
    "        min_vals  = [v for k, v in props.items() if k.endswith(\"_min\")]\n",
    "        max_vals  = [v for k, v in props.items() if k.endswith(\"_max\")]\n",
    "\n",
    "        # Single-band fallback\n",
    "        if not mean_vals:\n",
    "            if \"mean\" in props:\n",
    "                mean_vals = [props[\"mean\"]]\n",
    "        if not min_vals:\n",
    "            if \"min\" in props:\n",
    "                min_vals = [props[\"min\"]]\n",
    "        if not max_vals:\n",
    "            if \"max\" in props:\n",
    "                max_vals = [props[\"max\"]]\n",
    "\n",
    "        if not mean_vals or not min_vals or not max_vals:\n",
    "            return None, None, None\n",
    "\n",
    "        mean_val = float(np.nanmean(mean_vals))\n",
    "        min_val  = float(np.nanmin(min_vals))\n",
    "        max_val  = float(np.nanmax(max_vals))\n",
    "\n",
    "        return mean_val, min_val, max_val\n",
    "\n",
    "    except Exception as e:\n",
    "        # keep error small but informative\n",
    "        print(f\"compute_stats_from_2d_raster ERROR: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def process_geojson(geojson_path, netcdf_path, output_path, variable=\"CHL\",\n",
    "                              time_steps=None, xr_chunks=None):\n",
    "    \"\"\"\n",
    "    Memory-optimized processing:\n",
    "      - open netCDF once (with dask chunks)\n",
    "      - for each polygon: for each time window compute temporal aggregates (mean/min/max) -> pass 2D to exact_extract\n",
    "    Parameters:\n",
    "      xr_chunks: dictionary passed to xr.open_dataset(..., chunks=xr_chunks)\n",
    "                 e.g. {'time': 10, 'lat': 512, 'lon': 512}\n",
    "    \"\"\"\n",
    "\n",
    "    if time_steps is None:\n",
    "        time_steps = [\"day\", \"week\", \"month\", \"year\", \"5years\"]\n",
    "\n",
    "    shapes = gpd.read_file(geojson_path)\n",
    "    shapes = shapes.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "\n",
    "    # open dataset once, with dask chunking\n",
    "    if xr_chunks is None:\n",
    "        xr_chunks = {'time': 10}  # minimal sensible chunking — tune to your data and memory\n",
    "\n",
    "    ds = xr.open_dataset(netcdf_path, chunks=xr_chunks)\n",
    "    ds = ds.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "    results = []\n",
    "    total = shapes.shape[0]\n",
    "    for _, row in tqdm(shapes.iterrows(), total=total, desc=\"Processing shapes\"):\n",
    "        shape_geometry = row.geometry\n",
    "        date = row[\"date\"]\n",
    "        polygon_id = row.get(\"replicates\", None)\n",
    "\n",
    "        # ensure date is datetime\n",
    "        if isinstance(date, str):\n",
    "            date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "\n",
    "        # Precompute date ranges dictionary for this polygon's date\n",
    "        date_ranges = {label: get_dates(date, label) for label in time_steps}\n",
    "\n",
    "        result_entry = {\"replicates\": polygon_id}\n",
    "\n",
    "        for label, (start_date, end_date) in date_ranges.items():\n",
    "            try:\n",
    "                # Select time slice lazily\n",
    "                ds_slice = ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "                # If no times in slice -> short-circuit\n",
    "                if getattr(ds_slice, \"time\", None) is None or ds_slice.time.size == 0:\n",
    "                    result_entry[f\"Cop_{variable}_{label}_mean\"] = None\n",
    "                    result_entry[f\"Cop_{variable}_{label}_min\"]  = None\n",
    "                    result_entry[f\"Cop_{variable}_{label}_max\"]  = None\n",
    "                    # cleanup\n",
    "                    del ds_slice\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "\n",
    "                da = ds_slice[variable]\n",
    "\n",
    "                # Compute temporal aggregates lazily (these are still dask-backed if chunks were set)\n",
    "                da_mean = da.mean(dim=\"time\", skipna=True)\n",
    "                da_min  = da.min(dim=\"time\", skipna=True)\n",
    "                da_max  = da.max(dim=\"time\", skipna=True)\n",
    "\n",
    "                # Evaluate only the aggregated 2D arrays into memory (smaller than the full 3D)\n",
    "                # Convert to numpy arrays *once* and pass to exact_extract\n",
    "                # If exact_extract accepts xarray.DataArray directly, you can pass da_mean.compute()\n",
    "                da_mean_np = da_mean.compute()\n",
    "                da_min_np = da_min.compute()\n",
    "                da_max_np = da_max.compute()\n",
    "\n",
    "                # now compute zonal stats using the 2D arrays\n",
    "                mean_val, _, _ = compute_stats_from_2d_raster(da_mean_np, shape_geometry, agg_name='mean')\n",
    "                _, min_val, _ = compute_stats_from_2d_raster(da_min_np, shape_geometry, agg_name='min')\n",
    "                _, _, max_val = compute_stats_from_2d_raster(da_max_np, shape_geometry, agg_name='max')\n",
    "\n",
    "                result_entry[f\"Cop_{variable}_{label}_mean\"] = mean_val\n",
    "                result_entry[f\"Cop_{variable}_{label}_min\"]  = min_val\n",
    "                result_entry[f\"Cop_{variable}_{label}_max\"]  = max_val\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error for polygon {polygon_id} label {label}: {e}\")\n",
    "                result_entry[f\"Cop_{variable}_{label}_mean\"] = None\n",
    "                result_entry[f\"Cop_{variable}_{label}_min\"]  = None\n",
    "                result_entry[f\"Cop_{variable}_{label}_max\"]  = None\n",
    "\n",
    "            finally:\n",
    "                # ensure we release references\n",
    "                for v in (\"ds_slice\", \"da\", \"da_mean\", \"da_min\", \"da_max\", \"da_mean_np\", \"da_min_np\", \"da_max_np\"):\n",
    "                    if v in locals():\n",
    "                        try:\n",
    "                            del locals()[v]\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                gc.collect()\n",
    "\n",
    "        results.append(result_entry)\n",
    "\n",
    "    # close dataset and free resources\n",
    "    try:\n",
    "        ds.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "    gc.collect()\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038df6f-cd89-42fc-9859-2125965fb734",
   "metadata": {},
   "source": [
    "## Run extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d9145-bdad-4dbe-8268-7fc7a866e530",
   "metadata": {},
   "source": [
    "### Chlorophyll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b984b43-db14-4d36-b8aa-e2a61ac57cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file saved to ./data/processed_data/eDNA/mtdt_5.geojson\n"
     ]
    }
   ],
   "source": [
    "# 17/10/2025 : Extract CHL from cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D \n",
    "\n",
    "# 1. Convert .shp to .geojson \n",
    "# Load the file with buffer for extraction\n",
    "gdf = gpd.read_file(\"./data/processed_data/eDNA/mtdt_5.gpkg\")\n",
    "\n",
    "# Save as GeoJSON\n",
    "geojson_path = \"./data/processed_data/eDNA/mtdt_5.geojson\"\n",
    "gdf.to_file(geojson_path, driver=\"GeoJSON\")\n",
    "\n",
    "print(f\"GeoJSON file saved to {geojson_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc85924-e33e-47e5-8179-65f6b645e591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:   0%|                                | 0/788 [00:00<?, ?it/s]/home/marieke/miniconda3/envs/spyder-env/lib/python3.11/site-packages/dask/array/reductions.py:621: RuntimeWarning: All-NaN slice encountered\n",
      "  return np.nanmin(x_chunk, axis=axis, keepdims=keepdims)\n",
      "Processing shapes:   6%|█▎                     | 46/788 [01:18<24:23,  1.97s/it]"
     ]
    }
   ],
   "source": [
    "# 2. Make extraction (using Fct 4 and max buffer size = 0)\n",
    "\n",
    "geojson_path=\"./data/processed_data/eDNA/mtdt_5.geojson\"\n",
    "netcdf_path=\"./data/raw_data/predictors/Chl/cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D_20130101-20250101.nc\"\n",
    "output_path=\"./data/processed_data/predictors/mtdt_5_CHL_exactextract.csv\"\n",
    "\n",
    "\n",
    "process_geojson(\n",
    "    geojson_path=geojson_path,\n",
    "    netcdf_path=netcdf_path,\n",
    "    output_path=output_path,\n",
    "    variable=\"CHL\"  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef91164-6ef2-4c21-8c5e-39f457833190",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4de06761-45fa-49e1-aad7-de49e75636fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_geojson' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m netcdf_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/raw_data/predictors/SST/SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2_SST_20130101-20250101.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/processed_data/predictors/mtdt_5_SST.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mprocess_geojson\u001b[49m(\n\u001b[1;32m     11\u001b[0m     geojson_path\u001b[38;5;241m=\u001b[39mgeojson_path,\n\u001b[1;32m     12\u001b[0m     netcdf_path\u001b[38;5;241m=\u001b[39mnetcdf_path,\n\u001b[1;32m     13\u001b[0m     output_path\u001b[38;5;241m=\u001b[39moutput_path,\n\u001b[1;32m     14\u001b[0m     variable\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSST\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'process_geojson' is not defined"
     ]
    }
   ],
   "source": [
    "#  Extract SST from SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2_SST_20130101-20250101.nc \n",
    "\n",
    "# 2. Make extraction (using Fct 4 and max buffer size = 0)\n",
    "\n",
    "geojson_path=\"./data/processed_data/eDNA/mtdt_5.geojson\"\n",
    "netcdf_path=\"./data/raw_data/predictors/SST/SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2_SST_20130101-20250101.nc\"\n",
    "output_path=\"./data/processed_data/predictors/mtdt_5_SST.csv\"\n",
    "\n",
    "\n",
    "process_geojson(\n",
    "    geojson_path=geojson_path,\n",
    "    netcdf_path=netcdf_path,\n",
    "    output_path=output_path,\n",
    "    variable=\"SST\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8e030-f587-445a-8f63-15a90ca9f33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da59f4a-b164-42d0-8f73-06e2ceecd54a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
