{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16e2a45a-659a-4de3-bbd1-204b1da5456c",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2105cf9-4115-45cc-a055-33e02c9340e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "import copernicusmarine\n",
    "import dask\n",
    "from datetime import datetime, timedelta\n",
    "import exactextract as ee\n",
    "from exactextract import exact_extract\n",
    "import geopandas as gpd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.features import geometry_mask\n",
    "import rioxarray as rxr\n",
    "from shapely.geometry import mapping, shape\n",
    "from shapely.geometry import mapping, Point\n",
    "from scipy.spatial import cKDTree\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "\n",
    "os.chdir(\"/media/marieke/Shared/Chap-1/Model/Scripts/Chap_1_2018-2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15621d65-5f08-4e38-be69-f94ffa2d7949",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Get Copernicus data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311e03f5-6e1b-4221-a1cd-44a4ebf95509",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chlorophyll 1km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57932f92-d636-48f7-aff3-aa68fa91c962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2025-10-17T15:11:26Z - Selected dataset version: \"202311\"\n",
      "INFO - 2025-10-17T15:11:26Z - Selected dataset part: \"default\"\n",
      "INFO - 2025-10-17T15:11:26Z - Downloading Copernicus Marine data requires a Copernicus Marine username and password, sign up for free at: https://data.marine.copernicus.eu/register\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copernicus Marine username:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  mschultz2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copernicus Marine password:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "data_request = {\n",
    "   \"dataset_id\" : \"cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D\",\n",
    "   \"longitude\" : [3, 9.65], \n",
    "   \"latitude\" : [41.2, 44],\n",
    "   \"time\" : [\"2013-01-01\", \"2025-01-01\"],\n",
    "   \"variables\" : [\"CHL\"]\n",
    "}\n",
    "\n",
    "# Load xarray dataset\n",
    "chl = copernicusmarine.open_dataset(\n",
    "    dataset_id = data_request[\"dataset_id\"],\n",
    "    minimum_longitude = data_request[\"longitude\"][0],\n",
    "    maximum_longitude = data_request[\"longitude\"][1],\n",
    "    minimum_latitude = data_request[\"latitude\"][0],\n",
    "    maximum_latitude = data_request[\"latitude\"][1],\n",
    "    start_datetime = data_request[\"time\"][0],\n",
    "    end_datetime = data_request[\"time\"][1],\n",
    "    variables = data_request[\"variables\"]\n",
    ")\n",
    "\n",
    "# Export to NCDF \n",
    "chl.to_netcdf(\"./data/raw_data/predictors/Chl/cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D_20130101-20250101.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57523d3e-005b-4360-a5c0-fff65854b487",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### pH, oxygen 4.2 km"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7305b40c-efb8-4944-8fa1-2545f9032314",
   "metadata": {},
   "source": [
    "https://data.marine.copernicus.eu/product/MEDSEA_MULTIYEAR_BGC_006_008/description\n",
    "\n",
    "O2 # STOPS at 2023-05-31\n",
    "med-ogs-bio-rean-d \n",
    "\n",
    "pH\n",
    "med-ogs-car-rean-d # STOPS at 2023-05-31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff4567-f1e4-4716-accf-5f6e90147c73",
   "metadata": {},
   "source": [
    "#### pH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009ecb64-5aea-48d4-b127-2d80a5a7153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2025-10-20T15:09:34Z - Selected dataset version: \"202105\"\n",
      "INFO - 2025-10-20T15:09:34Z - Selected dataset part: \"default\"\n",
      "WARNING - 2025-10-20T15:09:34Z - You are using the dataset med-ogs-bio-rean-d, version '202105', part 'default'. This exact version and part of the dataset will be retired on the 2025-11-25T00:00:00.000Z. For more information you can check: https://marine.copernicus.eu/user-corner/product-roadmap/transition-information\n",
      "INFO - 2025-10-20T15:09:34Z - Downloading Copernicus Marine data requires a Copernicus Marine username and password, sign up for free at: https://data.marine.copernicus.eu/register\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copernicus Marine username:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  mschultz2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copernicus Marine password:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 2025-10-20T15:09:43Z - Some of your subset selection [2013-01-01 00:00:00+00:00, 2025-01-01 00:00:00+00:00] for the time dimension exceed the dataset coordinates [1999-01-01 00:00:00+00:00, 2023-05-31 00:00:00+00:00]\n",
      "WARNING - 2025-10-20T15:09:43Z - Some of your subset selection [2013-01-01 00:00:00+00:00, 2025-01-01 00:00:00+00:00] for the time dimension exceed the dataset coordinates [1999-01-01 00:00:00+00:00, 2023-05-31 00:00:00+00:00]\n"
     ]
    },
    {
     "ename": "VariableDoesNotExistInTheDataset",
     "evalue": "The variable '02' is neither a variable or a standard name in the dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mVariableDoesNotExistInTheDataset\u001b[0m          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m data_request \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_id\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmed-ogs-bio-rean-d\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m9.65\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m02\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load xarray dataset\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m ox \u001b[38;5;241m=\u001b[39m \u001b[43mcopernicusmarine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminimum_longitude\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlongitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximum_longitude\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlongitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminimum_latitude\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximum_latitude\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_datetime\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_datetime\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvariables\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Export to NCDF \u001b[39;00m\n\u001b[1;32m     23\u001b[0m ph\u001b[38;5;241m.\u001b[39mto_netcdf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/raw_data/predictors/oxygen/med-ogs-bio-rean-d _20130101-20250101.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/core_functions/deprecated_options.py:78\u001b[0m, in \u001b[0;36mdeprecated_python_option.<locals>.deco.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     77\u001b[0m     rename_kwargs(f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, kwargs, aliases)\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/python_interface/exception_handler.py:17\u001b[0m, in \u001b[0;36mlog_exception_and_exit.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbort\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/python_interface/exception_handler.py:13\u001b[0m, in \u001b[0;36mlog_exception_and_exit.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m click\u001b[38;5;241m.\u001b[39mAbort:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbort\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/python_interface/open_dataset.py:195\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(dataset_id, dataset_version, dataset_part, username, password, variables, minimum_longitude, maximum_longitude, minimum_latitude, maximum_latitude, maximum_x, minimum_x, maximum_y, minimum_y, minimum_depth, maximum_depth, vertical_axis, start_datetime, end_datetime, coordinates_selection_method, service, credentials_file, chunk_size_limit)\u001b[0m\n\u001b[1;32m    153\u001b[0m geographicalparameters \u001b[38;5;241m=\u001b[39m GeographicalParameters(\n\u001b[1;32m    154\u001b[0m     y_axis_parameters\u001b[38;5;241m=\u001b[39mYParameters(\n\u001b[1;32m    155\u001b[0m         minimum_y\u001b[38;5;241m=\u001b[39mminimum_y_axis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m     ),\n\u001b[1;32m    171\u001b[0m )\n\u001b[1;32m    173\u001b[0m load_request \u001b[38;5;241m=\u001b[39m LoadRequest(\n\u001b[1;32m    174\u001b[0m     dataset_id\u001b[38;5;241m=\u001b[39mdataset_id,\n\u001b[1;32m    175\u001b[0m     force_dataset_version\u001b[38;5;241m=\u001b[39mdataset_version,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     credentials_file\u001b[38;5;241m=\u001b[39mcredentials_file,\n\u001b[1;32m    193\u001b[0m )\n\u001b[0;32m--> 195\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_object_from_load_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopen_dataset_from_arco_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks_factor_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommand_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCommandType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOPEN_DATASET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/python_interface/load_utils.py:126\u001b[0m, in \u001b[0;36mload_data_object_from_load_request\u001b[0;34m(load_request, arco_series_load_function, chunks_factor_size_limit, command_type)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         optimum_dask_chunking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43marco_series_load_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43musername\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musername\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeographical_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeographical_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemporal_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemporal_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdepth_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoordinates_selection_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoordinates_selection_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimum_dask_chunking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimum_dask_chunking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceNotSupported(retrieval_service\u001b[38;5;241m.\u001b[39mservice_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/download_functions/download_zarr.py:281\u001b[0m, in \u001b[0;36mopen_dataset_from_arco_series\u001b[0;34m(username, password, dataset_url, variables, geographical_parameters, temporal_parameters, depth_parameters, coordinates_selection_method, optimum_dask_chunking)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m dataset[variable]\u001b[38;5;241m.\u001b[39mencoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 281\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43msubset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeographical_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeographical_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemporal_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemporal_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoordinates_selection_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoordinates_selection_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcoords \u001b[38;5;129;01mand\u001b[39;00m optimum_dask_chunking:\n\u001b[1;32m    290\u001b[0m     optimum_chunks_depth \u001b[38;5;241m=\u001b[39m deepcopy(optimum_dask_chunking)\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/download_functions/subset_xarray.py:588\u001b[0m, in \u001b[0;36msubset\u001b[0;34m(dataset, variables, geographical_parameters, temporal_parameters, depth_parameters, coordinates_selection_method)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubset\u001b[39m(\n\u001b[1;32m    580\u001b[0m     dataset: xarray\u001b[38;5;241m.\u001b[39mDataset,\n\u001b[1;32m    581\u001b[0m     variables: Optional[List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    585\u001b[0m     coordinates_selection_method: CoordinatesSelectionMethod,\n\u001b[1;32m    586\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xarray\u001b[38;5;241m.\u001b[39mDataset:\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m variables:\n\u001b[0;32m--> 588\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43m_variables_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m _y_axis_subset(\n\u001b[1;32m    590\u001b[0m         dataset,\n\u001b[1;32m    591\u001b[0m         geographical_parameters\u001b[38;5;241m.\u001b[39my_axis_parameters,\n\u001b[1;32m    592\u001b[0m         coordinates_selection_method,\n\u001b[1;32m    593\u001b[0m     )\n\u001b[1;32m    594\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m _x_axis_subset(\n\u001b[1;32m    595\u001b[0m         dataset,\n\u001b[1;32m    596\u001b[0m         geographical_parameters\u001b[38;5;241m.\u001b[39mx_axis_parameters,\n\u001b[1;32m    597\u001b[0m         coordinates_selection_method,\n\u001b[1;32m    598\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/spyder-env/lib/python3.11/site-packages/copernicusmarine/download_functions/subset_xarray.py:520\u001b[0m, in \u001b[0;36m_variables_subset\u001b[0;34m(dataset, variables)\u001b[0m\n\u001b[1;32m    516\u001b[0m             dataset_variables_filter\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    517\u001b[0m                 variable_name_from_standard_name\n\u001b[1;32m    518\u001b[0m             )\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m VariableDoesNotExistInTheDataset(variable)\n\u001b[1;32m    521\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset[numpy\u001b[38;5;241m.\u001b[39marray(dataset_variables_filter)]\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _update_variables_attributes(dataset, dataset_variables_filter)\n",
      "\u001b[0;31mVariableDoesNotExistInTheDataset\u001b[0m: The variable '02' is neither a variable or a standard name in the dataset."
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "data_request = {\n",
    "   \"dataset_id\" : \"med-ogs-bio-rean-d\",\n",
    "   \"longitude\" : [3, 9.65], \n",
    "   \"latitude\" : [41.2, 44],\n",
    "   \"time\" : [\"2013-01-01\", \"2025-01-01\"],\n",
    "   \"variables\" : [\"02\"]\n",
    "}\n",
    "\n",
    "# Load xarray dataset\n",
    "ox = copernicusmarine.open_dataset(\n",
    "    dataset_id = data_request[\"dataset_id\"],\n",
    "    minimum_longitude = data_request[\"longitude\"][0],\n",
    "    maximum_longitude = data_request[\"longitude\"][1],\n",
    "    minimum_latitude = data_request[\"latitude\"][0],\n",
    "    maximum_latitude = data_request[\"latitude\"][1],\n",
    "    start_datetime = data_request[\"time\"][0],\n",
    "    end_datetime = data_request[\"time\"][1],\n",
    "    variables = data_request[\"variables\"]\n",
    ")\n",
    "\n",
    "# Export to NCDF \n",
    "ph.to_netcdf(\"./data/raw_data/predictors/oxygen/med-ogs-bio-rean-d _20130101-20250101.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbc633-cea7-4940-9018-b55473123d49",
   "metadata": {},
   "source": [
    "#### oxygen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712014e1-9e08-41cb-bccc-e1bcabd02422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "data_request = {\n",
    "   \"dataset_id\" : \"cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D\",\n",
    "   \"longitude\" : [3, 9.65], \n",
    "   \"latitude\" : [41.2, 44],\n",
    "   \"time\" : [\"2013-01-01\", \"2025-01-01\"],\n",
    "   \"variables\" : [\"CHL\"]\n",
    "}\n",
    "\n",
    "# Load xarray dataset\n",
    "chl = copernicusmarine.open_dataset(\n",
    "    dataset_id = data_request[\"dataset_id\"],\n",
    "    minimum_longitude = data_request[\"longitude\"][0],\n",
    "    maximum_longitude = data_request[\"longitude\"][1],\n",
    "    minimum_latitude = data_request[\"latitude\"][0],\n",
    "    maximum_latitude = data_request[\"latitude\"][1],\n",
    "    start_datetime = data_request[\"time\"][0],\n",
    "    end_datetime = data_request[\"time\"][1],\n",
    "    variables = data_request[\"variables\"]\n",
    ")\n",
    "\n",
    "# Export to NCDF \n",
    "chl.to_netcdf(\"./data/raw_data/predictors/Chl/cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D_20130101-20250101.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f685ee-b6bf-45fc-817a-e7c7d3078cb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### SST 1km (2008-2025)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2859e0f4-300a-4c4b-b657-adf4100b336e",
   "metadata": {},
   "source": [
    "https://data.marine.copernicus.eu/product/SST_MED_SST_L4_NRT_OBSERVATIONS_010_004/description\n",
    "Analysis : 0\n",
    "SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2\n",
    "\n",
    "Anomaly : \n",
    "SST_MED_SSTA_L4_NRT_OBSERVATIONS_010_004_d \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2221348f-654d-4b66-b049-ac0f22da3dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2025-10-20T15:15:14Z - Selected dataset version: \"202311\"\n",
      "INFO - 2025-10-20T15:15:14Z - Selected dataset part: \"default\"\n",
      "INFO - 2025-10-20T15:15:14Z - Downloading Copernicus Marine data requires a Copernicus Marine username and password, sign up for free at: https://data.marine.copernicus.eu/register\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copernicus Marine username:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  mschultz2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copernicus Marine password:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marieke/miniconda3/envs/spyder-env/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:660: UserWarning: endian-ness of dtype and endian kwarg do not match, using endian kwarg\n",
      "  nc4_var = self.ds.createVariable(**default_args)\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "data_request = {\n",
    "   \"dataset_id\" : \"SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2\",\n",
    "   \"longitude\" : [3, 9.65], \n",
    "   \"latitude\" : [41.2, 44],\n",
    "   \"time\" : [\"2013-01-01\", \"2025-01-01\"],\n",
    "   \"variables\" : [\"analysed_sst\"]\n",
    "}\n",
    "\n",
    "# Load xarray dataset\n",
    "sst = copernicusmarine.open_dataset(\n",
    "    dataset_id = data_request[\"dataset_id\"],\n",
    "    minimum_longitude = data_request[\"longitude\"][0],\n",
    "    maximum_longitude = data_request[\"longitude\"][1],\n",
    "    minimum_latitude = data_request[\"latitude\"][0],\n",
    "    maximum_latitude = data_request[\"latitude\"][1],\n",
    "    start_datetime = data_request[\"time\"][0],\n",
    "    end_datetime = data_request[\"time\"][1],\n",
    "    variables = data_request[\"variables\"]\n",
    ")\n",
    "\n",
    "# Export to NCDF \n",
    "sst.to_netcdf(\"./data/raw_data/predictors/SST/SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2_SST_20130101-20250101.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8997d777-8170-4c50-8f57-1865979df745",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Ocean mixed layer thickness 4.2km (1987-2025)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b0cdf0b-7f0c-423d-ae13-48f12aa6ab42",
   "metadata": {},
   "source": [
    "https://data.marine.copernicus.eu/product/MEDSEA_MULTIYEAR_PHY_006_004/description\n",
    "\n",
    "\n",
    "# Monthly data\n",
    "med-cmcc-mld-rean-m # until 2023\n",
    "med-cmcc-mld-int-m # 2021-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69705e96-12de-49e4-b772-92c0ced07e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "data_request = {\n",
    "   \"dataset_id\" : \"SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2\",\n",
    "   \"longitude\" : [3, 9.65], \n",
    "   \"latitude\" : [41.2, 44],\n",
    "   \"time\" : [\"2013-01-01\", \"2025-01-01\"],\n",
    "   \"variables\" : [\"analysed_sst\"]\n",
    "}\n",
    "\n",
    "# Load xarray dataset\n",
    "sst = copernicusmarine.open_dataset(\n",
    "    dataset_id = data_request[\"dataset_id\"],\n",
    "    minimum_longitude = data_request[\"longitude\"][0],\n",
    "    maximum_longitude = data_request[\"longitude\"][1],\n",
    "    minimum_latitude = data_request[\"latitude\"][0],\n",
    "    maximum_latitude = data_request[\"latitude\"][1],\n",
    "    start_datetime = data_request[\"time\"][0],\n",
    "    end_datetime = data_request[\"time\"][1],\n",
    "    variables = data_request[\"variables\"]\n",
    ")\n",
    "\n",
    "# Export to NCDF \n",
    "sst.to_netcdf(\"./data/raw_data/predictors/SST/SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2_SST_20130101-20250101.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d22960e-0a76-4e9b-945f-49cb6ba7c83c",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a801a-2fa1-441f-919b-ac54c9ea78bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Functions - test for weighted mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ef1958-e40c-4c64-98ac-61232e687949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting exactextract\n",
      "  Downloading exactextract-0.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy in /home/marieke/miniconda3/envs/spyder-env/lib/python3.11/site-packages (from exactextract) (1.26.4)\n",
      "Downloading exactextract-0.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: exactextract\n",
      "Successfully installed exactextract-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install exactextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3165f424-f23c-4908-ad80-eba6a6ae54ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_dates(date, time_step):\n",
    "    \"\"\"\n",
    "    Calculate the range of dates for a given time step relative to the provided date.\n",
    "    \"\"\"\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "\n",
    "    end_date = date - timedelta(days=1)\n",
    "\n",
    "    time_deltas = {\n",
    "        'day': 1,\n",
    "        'week': 7,\n",
    "        'month': 30,\n",
    "        'year': 365,\n",
    "        '5years': 5 * 365 + 1,\n",
    "    }\n",
    "\n",
    "    if time_step not in time_deltas:\n",
    "        raise ValueError(\"Unsupported time step. Choose from 'day', 'week', 'month', 'year', or '5years'.\")\n",
    "\n",
    "    start_date = date - timedelta(days=time_deltas[time_step])\n",
    "    return start_date, end_date\n",
    "\n",
    "\n",
    "def compute_stats_exactextract(data_array, shape_geometry, max_buffer_distance=0, step=0.01,\n",
    "                               nodata=None, tmp_dir=None):\n",
    "    \"\"\"\n",
    "    Using exactextract, extract pixel values & coverage fractions for shape_geometry and compute:\n",
    "      - coverage-weighted mean\n",
    "      - min (over pixels with coverage > 0)\n",
    "      - max (over pixels with coverage > 0)\n",
    "\n",
    "    If the polygon returns no valid pixels, will expand the geometry by successive buffers up to max_buffer_distance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_array : xarray.DataArray\n",
    "        single-band raster (can be multi-time -> but pass the aggregated slice already e.g. time-mean or stack)\n",
    "        must have georeference (rioxarray metadata) and CRS EPSG:4326 in your workflow.\n",
    "    shape_geometry : shapely geometry\n",
    "    max_buffer_distance : float\n",
    "        maximum buffer to try (in degrees if CRS is EPSG:4326)\n",
    "    step : float\n",
    "        buffer increment\n",
    "    nodata : numeric or None\n",
    "        nodata value to set in the temporary tiff if desired. If None, attempt to detect from data_array.\n",
    "    tmp_dir : str or None\n",
    "        directory for temporary files. If None, uses system temp.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean_val, min_val, max_val, used_buffer_distance\n",
    "    \"\"\"\n",
    "    # Attempt to infer nodata\n",
    "    if nodata is None:\n",
    "        try:\n",
    "            nodata = data_array.rio.nodata\n",
    "        except Exception:\n",
    "            nodata = None\n",
    "\n",
    "    current_buffer_distance = 0.0\n",
    "\n",
    "    # Ensure data_array has spatial metadata (rioxarray)\n",
    "    if not hasattr(data_array, \"rio\"):\n",
    "        raise ValueError(\"data_array must be a rioxarray-enabled xarray.DataArray with CRS and transform.\")\n",
    "\n",
    "    # reduce to single band if multiple bands remain (assumes 2D or 3D with single band)\n",
    "    # The calling code should pass a DataArray containing the values of interest (e.g. averaged over time)\n",
    "    while current_buffer_distance <= max_buffer_distance + 1e-12:\n",
    "        if current_buffer_distance > 0:\n",
    "            search_geometry = shape_geometry.buffer(current_buffer_distance)\n",
    "        else:\n",
    "            search_geometry = shape_geometry\n",
    "\n",
    "        try:\n",
    "            # Write the data_array to a temporary GeoTIFF file (exactextract expects a raster file)\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".tif\", dir=tmp_dir, delete=False) as tmp:\n",
    "                tmp_path = tmp.name\n",
    "\n",
    "            # rioxarray's to_raster / rio.to_raster\n",
    "            # For safety, ensure data_array is 2D (if it is a time stack, caller should collapse to single 2D)\n",
    "            # If there's an extra dim (e.g., time), the first non-time dim will be written; adjust if needed.\n",
    "            # We will call .rio.to_raster which writes GeoTIFF.\n",
    "            da_to_write = data_array\n",
    "\n",
    "            # If data_array has a 'time' dimension, it must be collapsed before writing. The caller should send\n",
    "            # a single time-slice or aggregated array. For safety, if time present and size==1, select it.\n",
    "            if \"time\" in da_to_write.dims and da_to_write.sizes[\"time\"] == 1:\n",
    "                da_to_write = da_to_write.isel(time=0)\n",
    "\n",
    "            # Ensure 2D\n",
    "            if len(da_to_write.dims) > 2:\n",
    "                # try to reduce by selecting first non-spatial dim except time\n",
    "                nonspatial = [d for d in da_to_write.dims if d not in (\"x\", \"y\", \"lon\", \"lat\")]\n",
    "                if nonspatial:\n",
    "                    da_to_write = da_to_write.isel({nonspatial[0]: 0})\n",
    "                else:\n",
    "                    # fallback: take first 2D slice\n",
    "                    dims = da_to_write.dims\n",
    "                    da_to_write = da_to_write.isel({dims[0]: 0})\n",
    "\n",
    "            # Write GeoTIFF\n",
    "            # Ensure CRS is set\n",
    "            try:\n",
    "                if da_to_write.rio.crs is None:\n",
    "                    da_to_write = da_to_write.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "            except Exception:\n",
    "                # ignore and proceed\n",
    "                pass\n",
    "\n",
    "            # write\n",
    "            da_to_write.rio.to_raster(tmp_path)\n",
    "\n",
    "            # Now call exactextract\n",
    "            # exactextract.extract expects either a path or rasterio dataset and a GeoJSON-like geometry.\n",
    "            geojson_geom = mapping(search_geometry)\n",
    "\n",
    "            # 'mode' : can be 'accurate' to compute fractional coverage. Use that explicitly.\n",
    "            # The exactextract Python API returns a list of records (one per geometry). Each record\n",
    "            # contains arrays 'values' and 'coverage_fraction' (naming mirrors R version).\n",
    "            # We'll be defensive when parsing the returned structure.\n",
    "            ee_result = ee.extract(tmp_path, [geojson_geom], mode=\"accurate\", bands=1)\n",
    "\n",
    "            # ee.extract returns a list with one element per input geometry\n",
    "            if not ee_result or len(ee_result) == 0:\n",
    "                # no data found; try next buffer\n",
    "                os.remove(tmp_path)\n",
    "                current_buffer_distance += step\n",
    "                continue\n",
    "                \n",
    "            print(\"-----------------------------------------------------------------------------------------------\")\n",
    "            print(ee_result)\n",
    "            \n",
    "            rec = ee_result[0]\n",
    "            \n",
    "\n",
    "            # Expected fields: 'values' and 'coverage_fraction' OR 'values' and 'coverage_area' depending on version.\n",
    "            values = None\n",
    "            cov = None\n",
    "\n",
    "            # Try common keys\n",
    "            if isinstance(rec, dict):\n",
    "                # Many examples return keys like 'values' and 'coverage_fraction'\n",
    "                if \"values\" in rec and \"coverage_fraction\" in rec:\n",
    "                    values = rec[\"values\"]\n",
    "                    cov = rec[\"coverage_fraction\"]\n",
    "                elif \"values\" in rec and \"coverage_area\" in rec:\n",
    "                    values = rec[\"values\"]\n",
    "                    cov = rec[\"coverage_area\"]\n",
    "                elif \"value\" in rec and \"coverage_fraction\" in rec:\n",
    "                    values = rec[\"value\"]\n",
    "                    cov = rec[\"coverage_fraction\"]\n",
    "                else:\n",
    "                    # try to inspect keys and pick arrays\n",
    "                    for k in rec:\n",
    "                        if isinstance(rec[k], (list, tuple)) and values is None:\n",
    "                            values = rec[k]\n",
    "                        elif isinstance(rec[k], (list, tuple)) and cov is None and rec[k] is not values:\n",
    "                            cov = rec[k]\n",
    "\n",
    "            # If not dict, maybe it's a pandas-like DataFrame. Try to coerce.\n",
    "            if values is None or cov is None:\n",
    "                # try treating rec as a table-like sequence of rows: search for columns named:\n",
    "                try:\n",
    "                    import pandas as _pd\n",
    "                    df = _pd.DataFrame(rec)\n",
    "                    if \"values\" in df.columns and \"coverage_fraction\" in df.columns:\n",
    "                        values = df[\"values\"].tolist()\n",
    "                        cov = df[\"coverage_fraction\"].tolist()\n",
    "                    elif \"value\" in df.columns and \"coverage_fraction\" in df.columns:\n",
    "                        values = df[\"value\"].tolist()\n",
    "                        cov = df[\"coverage_fraction\"].tolist()\n",
    "                    elif \"values\" in df.columns and \"coverage_area\" in df.columns:\n",
    "                        values = df[\"values\"].tolist()\n",
    "                        cov = df[\"coverage_area\"].tolist()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # cleanup tmp file\n",
    "            try:\n",
    "                os.remove(tmp_path)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            if values is None or cov is None:\n",
    "                # no usable extraction; try next buffer distance\n",
    "                current_buffer_distance += step\n",
    "                continue\n",
    "\n",
    "            # Convert to numpy arrays and mask nodata or NaNs\n",
    "            import numpy as np\n",
    "            values = np.array(values, dtype=float)\n",
    "            cov = np.array(cov, dtype=float)\n",
    "\n",
    "            # Mask invalid values (NaN, or nodata)\n",
    "            mask_valid = ~np.isnan(values)\n",
    "            if nodata is not None:\n",
    "                mask_valid = mask_valid & (values != nodata)\n",
    "\n",
    "            # Also require coverage > 0\n",
    "            mask_valid = mask_valid & (cov > 0)\n",
    "\n",
    "            if mask_valid.sum() == 0:\n",
    "                # no valid pixels; expand buffer\n",
    "                current_buffer_distance += step\n",
    "                continue\n",
    "\n",
    "            values_sel = values[mask_valid]\n",
    "            cov_sel = cov[mask_valid]\n",
    "\n",
    "            # weighted mean\n",
    "            weighted_sum = (values_sel * cov_sel).sum()\n",
    "            total_cov = cov_sel.sum()\n",
    "\n",
    "            if total_cov == 0:\n",
    "                mean_val = None\n",
    "            else:\n",
    "                mean_val = (weighted_sum / total_cov).item()\n",
    "\n",
    "            # For min / max we'll use the min and max among pixels intersected (coverage>0).\n",
    "            min_val = float(values_sel.min())\n",
    "            max_val = float(values_sel.max())\n",
    "\n",
    "            return mean_val, min_val, max_val, current_buffer_distance\n",
    "\n",
    "        except Exception as e:\n",
    "            # ignore and try next buffer distance (but log if you want)\n",
    "            # print(f\"exactextract error at buffer {current_buffer_distance}: {e}\")\n",
    "            try:\n",
    "                os.remove(tmp_path)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            current_buffer_distance += step\n",
    "            continue\n",
    "\n",
    "    # nothing found up to max_buffer_distance\n",
    "    return None, None, None, max_buffer_distance\n",
    "\n",
    "\n",
    "def open_nc(shape_geometry, date, netcdf_path, variable=\"CHL\"):\n",
    "    \"\"\"\n",
    "    Compute CHL statistics for a given geometry and date using a netCDF file.\n",
    "\n",
    "    Now uses exactextract-based weighted means (coverage-weighted).\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    try:\n",
    "        if isinstance(date, str):\n",
    "            date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "\n",
    "        ds = xr.open_dataset(netcdf_path)\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "        target_date = date - timedelta(days=1)\n",
    "        time_steps = [\"day\", \"week\", \"month\", \"year\", \"5years\"]\n",
    "        date_ranges = {label: get_dates(date, label) for label in time_steps}\n",
    "\n",
    "        for label, (start_date, end_date) in date_ranges.items():\n",
    "            ds_time_range = ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "            if ds_time_range.time.size == 0:\n",
    "                results[label] = (None, None, None, 0)\n",
    "                continue\n",
    "\n",
    "            chl_data = ds_time_range[variable]\n",
    "\n",
    "            # count how many days are fully empty (all NaN) in the time slice\n",
    "            empty_days = sum(chl_data.sel(time=t).isnull().all().item() for t in chl_data.time)\n",
    "\n",
    "            # Drop fully-empty time steps; we will aggregate across time as the user's original code did,\n",
    "            # but the original code passed the whole valid_data stack to compute_stats which used rioxarray.clip.\n",
    "            # Here we will compute per-pixel mean over time for the valid pixels (so exactextract receives a single 2D raster)\n",
    "            valid_data = chl_data.dropna(dim=\"time\", how=\"all\")\n",
    "            if valid_data.size > 0:\n",
    "                # We will compute the per-pixel mean across time where values exist (keeping NaNs where all times are NaN)\n",
    "                # result is 2D DataArray with same spatial coords.\n",
    "                # If instead you want to compute zonal statistics per day and then aggregate across days, we can change approach.\n",
    "                per_pixel_mean = valid_data.mean(dim=\"time\", skipna=True)\n",
    "\n",
    "                mean_val, min_val, max_val, used_buffer = compute_stats_exactextract(\n",
    "                    per_pixel_mean, shape_geometry, max_buffer_distance=0.1, step=0.01\n",
    "                )\n",
    "\n",
    "                results[label] = (mean_val, min_val, max_val, empty_days)\n",
    "            else:\n",
    "                results[label] = (None, None, None, empty_days)\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing shape with target date: {date}: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def process_geojson(geojson_path, netcdf_path, output_path, variable=\"CHL\"):\n",
    "    \"\"\"\n",
    "    Process the GeoJSON file and compute statistics for each shape using a netCDF file.\n",
    "    \"\"\"\n",
    "    shapes = gpd.read_file(geojson_path)\n",
    "    shapes = shapes.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "\n",
    "    shapes = shapes[0:3]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(shapes.iterrows(), total=shapes.shape[0], desc=\"Processing shapes\"):\n",
    "        shape_geometry = row.geometry\n",
    "        date = row[\"date\"]\n",
    "        polygon_id = row.get(\"replicates\", None)\n",
    "\n",
    "        chl_stats = open_nc(shape_geometry, date, netcdf_path, variable)\n",
    "\n",
    "        result_entry = {\"replicates\": polygon_id}\n",
    "        for label, (mean, min_val, max_val, empty_days) in chl_stats.items():\n",
    "            result_entry[f\"Cop_CHL_{label}_mean\"] = mean\n",
    "            result_entry[f\"Cop_CHL_{label}_min\"] = min_val\n",
    "            result_entry[f\"Cop_CHL_{label}_max\"] = max_val\n",
    "            result_entry[f\"Cop_CHL_{label}_empty_days\"] = empty_days\n",
    "\n",
    "        results.append(result_entry)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745b96b-ae63-475d-91c4-911ebbdcd543",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7a4aa-f2d4-4d1c-8e40-952f40fbbb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline with exactextract\n",
    "\n",
    "def get_dates(date, time_step):\n",
    "    \"\"\"\n",
    "    Calculate the range of dates for a given time step relative to the provided date.\n",
    "    \"\"\"\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "\n",
    "    end_date = date - timedelta(days=1)\n",
    "\n",
    "    time_deltas = {\n",
    "        'day': 1,\n",
    "        'week': 7,\n",
    "        'month': 30,\n",
    "        'year': 365,\n",
    "        '5years': 5 * 365 + 1,\n",
    "    }\n",
    "\n",
    "    if time_step not in time_deltas:\n",
    "        raise ValueError(\"Unsupported time step. Choose from 'day', 'week', 'month', 'year', or '5years'.\")\n",
    "\n",
    "    start_date = date - timedelta(days=time_deltas[time_step])\n",
    "    return start_date, end_date\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_stats(data_array, shape_geometry):\n",
    "    \"\"\"\n",
    "    Extract values and compute weighted mean - automatically with exactextract- , min and max.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        feature = {\"type\": \"Feature\", \"geometry\": mapping(shape_geometry), \"properties\": {}}\n",
    "        res = exact_extract(data_array, [feature], [\"mean\", \"min\", \"max\"])\n",
    "\n",
    "\n",
    "        if not res or len(res) == 0:\n",
    "            return None, None, None\n",
    "\n",
    "        props = res[0][\"properties\"]\n",
    "\n",
    "        # Multi-band keys: band_1_mean, band_2_mean, etc.\n",
    "        mean_vals = [v for k, v in props.items() if k.endswith(\"_mean\")]\n",
    "        min_vals  = [v for k, v in props.items() if k.endswith(\"_min\")]\n",
    "        max_vals  = [v for k, v in props.items() if k.endswith(\"_max\")]\n",
    "\n",
    "         # Single-band keys: mean, min, max\n",
    "        if not mean_vals:\n",
    "            if \"mean\" in props:\n",
    "                mean_vals = [props[\"mean\"]]\n",
    "        if not min_vals:\n",
    "            if \"min\" in props:\n",
    "                min_vals = [props[\"min\"]]\n",
    "        if not max_vals:\n",
    "            if \"max\" in props:\n",
    "                max_vals = [props[\"max\"]]\n",
    "   \n",
    "\n",
    "        if not mean_vals or not min_vals or not max_vals:\n",
    "            return None, None, None\n",
    "\n",
    "        mean_val = float(np.nanmean(mean_vals))\n",
    "        min_val  = float(np.nanmin(min_vals))\n",
    "        max_val  = float(np.nanmax(max_vals))\n",
    "\n",
    "        return mean_val, min_val, max_val\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"compute_stats ERROR: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def open_nc(shape_geometry, date, netcdf_path, variable=\"CHL\"):\n",
    "    \"\"\"\n",
    "    Compute NCDF statistics for a given geometry and date using a netCDF file.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    try:\n",
    "        if isinstance(date, str):\n",
    "            date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "\n",
    "        ds = xr.open_dataset(netcdf_path)\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "\n",
    "        target_date = date - timedelta(days=1)\n",
    "        time_steps = [\"day\", \"week\", \"month\", \"year\", \"5years\"]\n",
    "        date_ranges = {label: get_dates(date, label) for label in time_steps}\n",
    "\n",
    "\n",
    "        for label, (start_date, end_date) in date_ranges.items():\n",
    "            ds_time_range = ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "            if ds_time_range.time.size == 0:\n",
    "                results[label] = (None, None, None, 0)\n",
    "                continue\n",
    "\n",
    "            chl_data = ds_time_range[variable]\n",
    "            valid_data = chl_data.dropna(dim=\"time\", how=\"all\")\n",
    "\n",
    "\n",
    "            if valid_data.size > 0:                 \n",
    "                mean_val, min_val, max_val = compute_stats(valid_data, shape_geometry)                \n",
    "                results[label] = (mean_val, min_val, max_val)\n",
    "            else:\n",
    "                print(\"valid_data.size == 0\")\n",
    "                results[label] = (None, None, None)\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing shape with target date: {date}: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_geojson(geojson_path, netcdf_path, output_path, variable=\"CHL\"):\n",
    "    \"\"\"\n",
    "    Process the GeoJSON file and compute statistics for each shape using a netCDF file.\n",
    "    \"\"\"\n",
    "    shapes = gpd.read_file(geojson_path)\n",
    "    shapes = shapes.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "    shapes = shapes[0:20]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(shapes.iterrows(), total=shapes.shape[0], desc=\"Processing shapes\"):\n",
    "        shape_geometry = row.geometry\n",
    "        date = row[\"date\"]\n",
    "        polygon_id = row.get(\"replicates\", None)\n",
    "\n",
    "        nc_stats = open_nc(shape_geometry, date, netcdf_path, variable)\n",
    "\n",
    "        print(\"------------replicates--------------\")\n",
    "        print(polygon_id)\n",
    "        print(\"------------nc_stats--------------\")\n",
    "        print(nc_stats)\n",
    "\n",
    "\n",
    "        result_entry = {\"replicates\": polygon_id}\n",
    "        for label, (mean, min_val, max_val) in nc_stats.items():\n",
    "            result_entry[f\"Cop_CHL_{label}_mean\"] = mean\n",
    "            result_entry[f\"Cop_CHL_{label}_min\"] = min_val\n",
    "            result_entry[f\"Cop_CHL_{label}_max\"] = max_val\n",
    "\n",
    "        results.append(result_entry)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff0e6d91-a234-4d3c-af71-98a23bc15ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(date, time_step):\n",
    "    \"\"\"\n",
    "    Calculate the range of dates for a given time step relative to the provided date.\n",
    "    \"\"\"\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "\n",
    "    end_date = date - timedelta(days=1)\n",
    "\n",
    "    time_deltas = {\n",
    "        'day': 1,\n",
    "        'week': 7,\n",
    "        'month': 30,\n",
    "        'year': 365,\n",
    "        '5years': 5 * 365 + 1,\n",
    "    }\n",
    "\n",
    "    if time_step not in time_deltas:\n",
    "        raise ValueError(\"Unsupported time step. Choose from 'day', 'week', 'month', 'year', or '5years'.\")\n",
    "\n",
    "    start_date = date - timedelta(days=time_deltas[time_step])\n",
    "    return start_date, end_date\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_stats(data_array, shape_geometry, max_buffer_distance=0, step=0.01):\n",
    "    \"\"\"\n",
    "    Compute the mean, min, and max of valid points (non-NaN) within the given geometry using `.clip`.\n",
    "    Expand the search radius if no valid points exist.\n",
    "    \"\"\"\n",
    "    current_buffer_distance = 0\n",
    "\n",
    "    while current_buffer_distance <= max_buffer_distance:\n",
    "        search_geometry = shape_geometry.buffer(current_buffer_distance) if current_buffer_distance > 0 else shape_geometry\n",
    "\n",
    "        try:\n",
    "            \n",
    "            # Extraction is done here :\n",
    "            clipped_data = data_array.rio.clip([mapping(search_geometry)], crs=\"EPSG:4326\", drop=True)\n",
    "            # Doc : https://corteva.github.io/rioxarray/html/rioxarray.html#rioxarray.raster_array.RasterArray.clip\n",
    "            # drop = True :  drop the data outside of the extent of the mask geometries Otherwise, it will return the same raster with the data masked. \n",
    "            # all_touched = False (default) : only pixels whose center is within the polygon or that are selected by Bresenham’s line algorithm will be burned in.\n",
    "            \n",
    "            if clipped_data.count().item() > 0:\n",
    "                return (\n",
    "                    clipped_data.mean().item(),\n",
    "                    clipped_data.min().item(),\n",
    "                    clipped_data.max().item(),\n",
    "                    current_buffer_distance\n",
    "                )\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        current_buffer_distance += step\n",
    "\n",
    "    return None, None, None, max_buffer_distance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def open_nc(shape_geometry, date, netcdf_path, variable=\"CHL\"):\n",
    "    \"\"\"\n",
    "    Compute CHL statistics for a given geometry and date using a netCDF file.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    try:\n",
    "        if isinstance(date, str):\n",
    "            date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "\n",
    "        ds = xr.open_dataset(netcdf_path)\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "        target_date = date - timedelta(days=1)\n",
    "        time_steps = [\"day\", \"week\", \"month\", \"year\", \"5years\"]\n",
    "        date_ranges = {label: get_dates(date, label) for label in time_steps}\n",
    "\n",
    "        for label, (start_date, end_date) in date_ranges.items():\n",
    "            ds_time_range = ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "            if ds_time_range.time.size == 0:\n",
    "                results[label] = (None, None, None, 0)\n",
    "                continue\n",
    "\n",
    "            chl_data = ds_time_range[variable]\n",
    "            empty_days = sum(chl_data.sel(time=t).isnull().all().item() for t in chl_data.time)\n",
    "            valid_data = chl_data.dropna(dim=\"time\", how=\"all\")\n",
    "\n",
    "            if valid_data.size > 0:\n",
    "                mean_val, min_val, max_val, max_search_dist = compute_stats(\n",
    "                    valid_data, shape_geometry, max_buffer_distance=0, step=0.01\n",
    "                )\n",
    "                results[label] = (mean_val, min_val, max_val, empty_days)\n",
    "            else:\n",
    "                results[label] = (None, None, None, empty_days)\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing shape with target date: {date}: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_geojson(geojson_path, netcdf_path, output_path, variable=\"CHL\"):\n",
    "    \"\"\"\n",
    "    Process the GeoJSON file and compute statistics for each shape using a netCDF file.\n",
    "    \"\"\"\n",
    "    shapes = gpd.read_file(geojson_path)\n",
    "    shapes = shapes.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "    shapes = shapes[0:3]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(shapes.iterrows(), total=shapes.shape[0], desc=\"Processing shapes\"):\n",
    "        shape_geometry = row.geometry\n",
    "        date = row[\"date\"]\n",
    "        polygon_id = row.get(\"replicates\", None)\n",
    "\n",
    "        chl_stats = open_nc(shape_geometry, date, netcdf_path, variable)\n",
    "\n",
    "        result_entry = {\"replicates\": polygon_id}\n",
    "        for label, (mean, min_val, max_val, empty_days) in chl_stats.items():\n",
    "            result_entry[f\"Cop_CHL_{label}_mean\"] = mean\n",
    "            result_entry[f\"Cop_CHL_{label}_min\"] = min_val\n",
    "            result_entry[f\"Cop_CHL_{label}_max\"] = max_val\n",
    "            result_entry[f\"Cop_CHL_{label}_empty_days\"] = empty_days\n",
    "\n",
    "        results.append(result_entry)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038df6f-cd89-42fc-9859-2125965fb734",
   "metadata": {},
   "source": [
    "## Run extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b7003-7b66-4606-904c-aa0407facf0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87da1b90-c006-4a79-a23d-b2d57484129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(date, time_step):\n",
    "    \"\"\"\n",
    "    Calculate the range of dates for a given time step relative to the provided date.\n",
    "    \"\"\"\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "\n",
    "    end_date = date - timedelta(days=1)\n",
    "\n",
    "    time_deltas = {\n",
    "        'day': 1,\n",
    "        'week': 7,\n",
    "        'month': 30,\n",
    "        'year': 365,\n",
    "        '5years': 5 * 365 + 1,\n",
    "    }\n",
    "\n",
    "    if time_step not in time_deltas:\n",
    "        raise ValueError(\"Unsupported time step. Choose from 'day', 'week', 'month', 'year', or '5years'.\")\n",
    "\n",
    "    start_date = date - timedelta(days=time_deltas[time_step])\n",
    "    return start_date, end_date\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_stats(data_array, shape_geometry):\n",
    "    \"\"\"\n",
    "    Extract values and compute weighted mean - automatically with exactextract- , min and max.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        feature = {\"type\": \"Feature\", \"geometry\": mapping(shape_geometry), \"properties\": {}}\n",
    "        res = exact_extract(data_array, [feature], [\"mean\", \"min\", \"max\"])\n",
    "\n",
    "\n",
    "        if not res or len(res) == 0:\n",
    "            return None, None, None\n",
    "\n",
    "        props = res[0][\"properties\"]\n",
    "\n",
    "        # Multi-band keys: band_1_mean, band_2_mean, etc.\n",
    "        mean_vals = [v for k, v in props.items() if k.endswith(\"_mean\")]\n",
    "        min_vals  = [v for k, v in props.items() if k.endswith(\"_min\")]\n",
    "        max_vals  = [v for k, v in props.items() if k.endswith(\"_max\")]\n",
    "\n",
    "         # Single-band keys: mean, min, max\n",
    "        if not mean_vals:\n",
    "            if \"mean\" in props:\n",
    "                mean_vals = [props[\"mean\"]]\n",
    "        if not min_vals:\n",
    "            if \"min\" in props:\n",
    "                min_vals = [props[\"min\"]]\n",
    "        if not max_vals:\n",
    "            if \"max\" in props:\n",
    "                max_vals = [props[\"max\"]]\n",
    "   \n",
    "\n",
    "        if not mean_vals or not min_vals or not max_vals:\n",
    "            return None, None, None\n",
    "\n",
    "        mean_val = float(np.nanmean(mean_vals))\n",
    "        min_val  = float(np.nanmin(min_vals))\n",
    "        max_val  = float(np.nanmax(max_vals))\n",
    "\n",
    "        return mean_val, min_val, max_val\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"compute_stats ERROR: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def open_nc(shape_geometry, date, netcdf_path, variable=\"CHL\"):\n",
    "    \"\"\"\n",
    "    Compute NCDF statistics for a given geometry and date using a netCDF file.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    try:\n",
    "        if isinstance(date, str):\n",
    "            date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "\n",
    "        ds = xr.open_dataset(netcdf_path)\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "\n",
    "        target_date = date - timedelta(days=1)\n",
    "        time_steps = [\"day\", \"week\", \"month\", \"year\", \"5years\"]\n",
    "        date_ranges = {label: get_dates(date, label) for label in time_steps}\n",
    "\n",
    "\n",
    "        for label, (start_date, end_date) in date_ranges.items():\n",
    "            ds_time_range = ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "            if ds_time_range.time.size == 0:\n",
    "                results[label] = (None, None, None, 0)\n",
    "                continue\n",
    "\n",
    "            chl_data = ds_time_range[variable]\n",
    "            valid_data = chl_data.dropna(dim=\"time\", how=\"all\")\n",
    "\n",
    "\n",
    "            if valid_data.size > 0:                 \n",
    "                mean_val, min_val, max_val = compute_stats(valid_data, shape_geometry)                \n",
    "                results[label] = (mean_val, min_val, max_val)\n",
    "            else:\n",
    "                print(\"valid_data.size == 0\")\n",
    "                results[label] = (None, None, None)\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing shape with target date: {date}: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_geojson(geojson_path, netcdf_path, output_path, variable=\"CHL\"):\n",
    "    \"\"\"\n",
    "    Process the GeoJSON file and compute statistics for each shape using a netCDF file.\n",
    "    \"\"\"\n",
    "    shapes = gpd.read_file(geojson_path)\n",
    "    shapes = shapes.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "    shapes = shapes[0:20]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(shapes.iterrows(), total=shapes.shape[0], desc=\"Processing shapes\"):\n",
    "        shape_geometry = row.geometry\n",
    "        date = row[\"date\"]\n",
    "        polygon_id = row.get(\"replicates\", None)\n",
    "\n",
    "        nc_stats = open_nc(shape_geometry, date, netcdf_path, variable)\n",
    "\n",
    "        print(\"------------replicates--------------\")\n",
    "        print(polygon_id)\n",
    "        print(\"------------nc_stats--------------\")\n",
    "        print(nc_stats)\n",
    "\n",
    "\n",
    "        result_entry = {\"replicates\": polygon_id}\n",
    "        for label, (mean, min_val, max_val) in nc_stats.items():\n",
    "            result_entry[f\"Cop_CHL_{label}_mean\"] = mean\n",
    "            result_entry[f\"Cop_CHL_{label}_min\"] = min_val\n",
    "            result_entry[f\"Cop_CHL_{label}_max\"] = max_val\n",
    "\n",
    "        results.append(result_entry)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f9f819c-a023-434a-9cd8-e870ee6ef033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:   0%|                                 | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:   5%|█▎                       | 1/20 [00:06<02:06,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY180622/SPY201155/SPY201156/SPY201189\n",
      "------------replicates--------------\n",
      "{'day': (0.06718862086574383, 0.06525104492902756, 0.0686667412519455), 'week': (0.07321769710675555, 0.06525104492902756, 0.07775455713272095), 'month': (0.08463646815102051, 0.06468724459409714, 0.1160086989402771), 'year': (0.5199282805972645, 0.04498501121997833, 5.6283979415893555), '5years': (0.4490315943948328, 0.04498501121997833, 5.6283979415893555)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  10%|██▌                      | 2/20 [00:12<01:55,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY180624/SPY181146/SPY181151/SPY181155\n",
      "------------replicates--------------\n",
      "{'day': (0.0942835757467272, 0.09415426850318909, 0.09460443258285522), 'week': (0.07609841884238212, 0.06538700312376022, 0.09460443258285522), 'month': (0.11665163654579479, 0.06316225230693817, 0.2834864854812622), 'year': (0.14774635743532225, 0.023832421749830246, 1.1077903509140015), '5years': (0.13461176351858611, 0.023832421749830246, 1.7574970722198486)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  15%|███▊                     | 3/20 [00:18<01:45,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY180625/SPY181700/SPY181701/SPY181708\n",
      "------------replicates--------------\n",
      "{'day': (0.143948073727683, 0.10495904833078384, 0.1684579700231552), 'week': (0.1643321235981451, 0.08790973573923111, 0.3173648715019226), 'month': (0.27238026017171485, 0.08790973573923111, 0.8021978735923767), 'year': (0.3406483257225056, 0.0512312687933445, 2.160412549972534), '5years': (0.28326905702523314, 0.020442044362425804, 2.160412549972534)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  20%|█████                    | 4/20 [00:24<01:35,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY180629/SPY181145/SPY190597/SPY192300\n",
      "------------replicates--------------\n",
      "{'day': (0.14650312278335986, 0.10953126102685928, 0.1781977117061615), 'week': (0.1793566922719146, 0.1024145558476448, 0.33735331892967224), 'month': (0.3218301585534278, 0.1024145558476448, 0.8908647894859314), 'year': (0.40965885748467584, 0.0449051670730114, 2.7410783767700195), '5years': (0.33464995672031883, 0.01433512195944786, 2.7410783767700195)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  25%|██████▎                  | 5/20 [00:30<01:28,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY180630/SPY180635/SPY181822/SPY181823\n",
      "------------replicates--------------\n",
      "{'day': (0.08129365927809644, 0.06514974683523178, 0.166176438331604), 'week': (0.10513171552727922, 0.06514974683523178, 0.25679343938827515), 'month': (0.11732555607480322, 0.06514974683523178, 0.25679343938827515), 'year': (0.2003313279650582, 0.03359855338931084, 1.6378918886184692), '5years': (0.20769580570962545, 0.02783365175127983, 3.4752748012542725)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  30%|███████▌                 | 6/20 [00:36<01:22,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY180634/SPY180934/SPY181811/SPY181820\n",
      "------------replicates--------------\n",
      "{'day': (0.0637442987041596, 0.0637071430683136, 0.06379544734954834), 'week': (0.0824925073919399, 0.0637071430683136, 0.11400075256824493), 'month': (0.11440941628330445, 0.0637071430683136, 0.22827759385108948), 'year': (0.1827514305084315, 0.029737479984760284, 1.472585678100586), '5years': (0.18312658748674362, 0.029737479984760284, 2.083415985107422)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  35%|████████▊                | 7/20 [00:41<01:15,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY180765/SPY180785/SPY181511/SPY181884/SPY181890/SPY181893/SPY181894/SPY181910\n",
      "------------replicates--------------\n",
      "{'day': (0.4409860842753508, 0.37382540106773376, 0.45488327741622925), 'week': (0.3649728611178591, 0.2412862926721573, 0.45488327741622925), 'month': (0.27626915202455954, 0.15723074972629547, 0.45488327741622925), 'year': (0.1800567877952407, 0.02767486497759819, 1.1560429334640503), '5years': (0.17878812886524686, 0.02767486497759819, 3.670457601547241)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  40%|██████████               | 8/20 [00:47<01:09,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY180927/SPY180930\n",
      "------------replicates--------------\n",
      "{'day': (0.6820290160393356, 0.5909357070922852, 0.8469605445861816), 'week': (0.6973667539991262, 0.2944985628128052, 1.2790412902832031), 'month': (0.6784111361675828, 0.12874218821525574, 1.3767783641815186), 'year': (0.3723649590354914, 0.0509054996073246, 2.871854782104492), '5years': (0.47053450279197606, 0.04640711843967438, 5.414854526519775)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  45%|███████████▎             | 9/20 [00:52<01:02,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY180932/SPY181152/SPY181154/SPY181158\n",
      "------------replicates--------------\n",
      "{'day': (0.08752318610198435, 0.07966101169586182, 0.08896530419588089), 'week': (0.0875666589741707, 0.065118208527565, 0.10748524963855743), 'month': (0.10732313319777316, 0.06251183152198792, 0.22221216559410095), 'year': (0.15135017938734652, 0.02396441251039505, 1.1154208183288574), '5years': (0.13836418905339423, 0.02396441251039505, 1.6913037300109863)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  50%|████████████            | 10/20 [00:58<00:56,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY180933/SPY180935\n",
      "------------replicates--------------\n",
      "{'day': (1.0251717015837203, 0.7933692336082458, 1.1605799198150635), 'week': (1.024429585040404, 0.5186532735824585, 1.9022114276885986), 'month': (0.8640490246425284, 0.18601861596107483, 1.9022114276885986), 'year': (0.48128413508457124, 0.057316455990076065, 3.5908260345458984), '5years': (0.6114463350661249, 0.057316455990076065, 12.805350303649902)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  55%|█████████████▏          | 11/20 [01:04<00:51,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY180936/SPY180937\n",
      "------------replicates--------------\n",
      "{'day': (1.6242952039706178, 1.5390106439590454, 1.651769757270813), 'week': (1.5960856908233878, 0.8311907052993774, 2.4715404510498047), 'month': (1.0824335142530155, 0.3959610164165497, 2.4715404510498047), 'year': (0.5887896991644038, 0.05140399560332298, 3.400531768798828), '5years': (0.7459895947200631, 0.05140399560332298, 8.777917861938477)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  60%|██████████████▍         | 12/20 [01:10<00:45,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY181147/SPY181149/SPY181150/SPY181160\n",
      "------------replicates--------------\n",
      "{'day': (0.0737353573376694, 0.0717928409576416, 0.07753580063581467), 'week': (0.06715847906363429, 0.0553617998957634, 0.08070025593042374), 'month': (0.10611847769951675, 0.05301511287689209, 0.3100566267967224), 'year': (0.14033859327194517, 0.01997200772166252, 1.1281532049179077), '5years': (0.13209645569428738, 0.01997200772166252, 6.392641067504883)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  65%|███████████████▌        | 13/20 [01:15<00:39,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY181148/SPY181156/SPY181157/SPY181159\n",
      "------------replicates--------------\n",
      "{'day': (0.131694957613945, 0.131694957613945, 0.131694957613945), 'week': (0.079471974500588, 0.0620441734790802, 0.131694957613945), 'month': (0.12728502849737802, 0.0620441734790802, 0.34316882491111755), 'year': (0.14732732126769954, 0.02322176657617092, 1.1109230518341064), '5years': (0.13270898503919518, 0.02322176657617092, 1.77898108959198)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  70%|████████████████▊       | 14/20 [01:21<00:34,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY181153/SPY181812/SPY181818/SPY181819\n",
      "------------replicates--------------\n",
      "{'day': (0.10548677359690538, 0.1014319583773613, 0.1510164439678192), 'week': (0.09183098963713585, 0.0842970460653305, 0.1510164439678192), 'month': (0.10055327329396463, 0.06580230593681335, 0.15657129883766174), 'year': (0.16946335979998295, 0.03212473914027214, 1.3873748779296875), '5years': (0.15489835693438392, 0.03212473914027214, 1.9302968978881836)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  75%|██████████████████      | 15/20 [01:27<00:28,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY181699/SPY181824/SPY181828/SPY192310\n",
      "------------replicates--------------\n",
      "{'day': (0.09457855629503577, 0.08332124352455139, 0.10706867277622223), 'week': (0.11016209978058826, 0.07392069697380066, 0.22254154086112976), 'month': (0.15565169525385658, 0.07392069697380066, 0.3342788815498352), 'year': (0.21601303599789426, 0.026480820029973984, 1.9134045839309692), '5years': (0.18716930121821684, 0.026480820029973984, 1.9134045839309692)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  80%|███████████████████▏    | 16/20 [01:33<00:23,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY181702/SPY181705/SPY181707/SPY181713\n",
      "------------replicates--------------\n",
      "{'day': (0.19643147280754839, 0.19151578843593597, 0.20126061141490936), 'week': (0.2090251772661045, 0.1532139927148819, 0.3439825177192688), 'month': (0.20017020572891894, 0.10836302489042282, 0.3439825177192688), 'year': (0.33000640671989523, 0.005742328707128763, 2.4003312587738037), '5years': (0.337400201630093, 0.005742328707128763, 11.53050708770752)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  85%|████████████████████▍   | 17/20 [01:39<00:17,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY181703/SPY181704/SPY181706/SPY181710\n",
      "------------replicates--------------\n",
      "{'day': (0.11675674140826812, 0.11663961410522461, 0.1169876828789711), 'week': (0.1331359526342161, 0.10470551997423172, 0.20119823515415192), 'month': (0.18971540942983225, 0.1015477180480957, 0.34692639112472534), 'year': (0.32826664629033286, 0.007849838584661484, 2.0642359256744385), '5years': (0.31761524591060486, 0.007849838584661484, 4.724327087402344)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  90%|█████████████████████▌  | 18/20 [01:45<00:11,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY181709/SPY181711/SPY181712/SPY181714\n",
      "------------replicates--------------\n",
      "{'day': (0.1439992332309456, 0.12584328651428223, 0.19430223107337952), 'week': (0.18101171908432181, 0.11814986914396286, 0.3777979910373688), 'month': (0.18693118402115777, 0.10224533081054688, 0.3777979910373688), 'year': (0.3155863822259259, 0.006594486068934202, 2.265831708908081), '5years': (0.30878524600318813, 0.006594486068934202, 7.125894069671631)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes:  95%|██████████████████████▊ | 19/20 [01:51<00:05,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY181808/SPY181813/SPY181816/SPY181826\n",
      "------------replicates--------------\n",
      "{'day': (0.12801810956785772, 0.10635823756456375, 0.15980233252048492), 'week': (0.10056857914424719, 0.08761268854141235, 0.15980233252048492), 'month': (0.10588328421672685, 0.0696224793791771, 0.1609726846218109), 'year': (0.1852359207190488, 0.033809613436460495, 1.8106924295425415), '5years': (0.16959241372515169, 0.033809613436460495, 1.8106924295425415)}\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n",
      "valid_data.size > 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes: 100%|████████████████████████| 20/20 [01:57<00:00,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------replicates--------------\n",
      "SPY181809/SPY181815/SPY181821/SPY181825\n",
      "------------replicates--------------\n",
      "{'day': (0.1706240475177765, 0.1706240475177765, 0.1706240475177765), 'week': (0.11138479518038887, 0.08246105909347534, 0.1706240475177765), 'month': (0.11460076570510865, 0.07447385042905807, 0.1706240475177765), 'year': (0.2097957569544446, 0.025837991386651993, 1.8801963329315186), '5years': (0.19308581539028838, 0.025837991386651993, 1.8801963329315186)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "geojson_path=\"./data/processed_data/eDNA/mtdt_5.geojson\"\n",
    "netcdf_path=\"./data/raw_data/predictors/Chl/cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D_20130101-20250101.nc\"\n",
    "output_path=\"./data/processed_data/predictors/mtdt_5_CHL_test.csv\"\n",
    "\n",
    "\n",
    "process_geojson(\n",
    "    geojson_path=geojson_path,\n",
    "    netcdf_path=netcdf_path,\n",
    "    output_path=output_path,\n",
    "    variable=\"CHL\"  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d9145-bdad-4dbe-8268-7fc7a866e530",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chlorophyll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b984b43-db14-4d36-b8aa-e2a61ac57cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file saved to ./data/processed_data/eDNA/mtdt_5.geojson\n"
     ]
    }
   ],
   "source": [
    "# 17/10/2025 : Extract CHL from cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D \n",
    "\n",
    "# 1. Convert .shp to .geojson \n",
    "# Load the file with buffer for extraction\n",
    "gdf = gpd.read_file(\"./data/processed_data/eDNA/mtdt_5.gpkg\")\n",
    "\n",
    "# Save as GeoJSON\n",
    "geojson_path = \"./data/processed_data/eDNA/mtdt_5.geojson\"\n",
    "gdf.to_file(geojson_path, driver=\"GeoJSON\")\n",
    "\n",
    "print(f\"GeoJSON file saved to {geojson_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acc85924-e33e-47e5-8179-65f6b645e591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shapes: 100%|██████████████████████| 788/788 [39:34<00:00,  3.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# 2. Make extraction (using Fct 4 and max buffer size = 0)\n",
    "\n",
    "geojson_path=\"./data/processed_data/eDNA/mtdt_5.geojson\"\n",
    "netcdf_path=\"./data/raw_data/predictors/Chl/cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D_20130101-20250101.nc\"\n",
    "output_path=\"./data/processed_data/predictors/mtdt_5_CHL.csv\"\n",
    "\n",
    "\n",
    "process_geojson(\n",
    "    geojson_path=geojson_path,\n",
    "    netcdf_path=netcdf_path,\n",
    "    output_path=output_path,\n",
    "    variable=\"CHL\"  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef91164-6ef2-4c21-8c5e-39f457833190",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4de06761-45fa-49e1-aad7-de49e75636fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_geojson' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m netcdf_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/raw_data/predictors/SST/SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2_SST_20130101-20250101.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/processed_data/predictors/mtdt_5_SST.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mprocess_geojson\u001b[49m(\n\u001b[1;32m     11\u001b[0m     geojson_path\u001b[38;5;241m=\u001b[39mgeojson_path,\n\u001b[1;32m     12\u001b[0m     netcdf_path\u001b[38;5;241m=\u001b[39mnetcdf_path,\n\u001b[1;32m     13\u001b[0m     output_path\u001b[38;5;241m=\u001b[39moutput_path,\n\u001b[1;32m     14\u001b[0m     variable\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSST\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'process_geojson' is not defined"
     ]
    }
   ],
   "source": [
    "#  Extract SST from SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2_SST_20130101-20250101.nc \n",
    "\n",
    "# 2. Make extraction (using Fct 4 and max buffer size = 0)\n",
    "\n",
    "geojson_path=\"./data/processed_data/eDNA/mtdt_5.geojson\"\n",
    "netcdf_path=\"./data/raw_data/predictors/SST/SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2_SST_20130101-20250101.nc\"\n",
    "output_path=\"./data/processed_data/predictors/mtdt_5_SST.csv\"\n",
    "\n",
    "\n",
    "process_geojson(\n",
    "    geojson_path=geojson_path,\n",
    "    netcdf_path=netcdf_path,\n",
    "    output_path=output_path,\n",
    "    variable=\"SST\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8e030-f587-445a-8f63-15a90ca9f33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da59f4a-b164-42d0-8f73-06e2ceecd54a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
