{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81618af2",
   "metadata": {},
   "source": [
    "## Extraction des variables température, salinité et force du vent sur la zone Med est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836fcd06",
   "metadata": {},
   "source": [
    "### 1. Extraction des variables TEMP et SAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f5a76c",
   "metadata": {},
   "source": [
    "#### Extraction 24h en amont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd4648c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction Temp & Sal sur 24h: 100%|██████████| 70/70 [1:44:36<00:00, 89.66s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, box\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def get_netcdf_paths_for_period(dt, base_folder, hours=24):\n",
    "    start_dt = dt - timedelta(hours=hours)\n",
    "    files = []\n",
    "\n",
    "    for year in range(start_dt.year, dt.year + 1):\n",
    "        year_folder = os.path.join(base_folder, str(year))\n",
    "        pattern = os.path.join(year_folder, \"MARC_F2-MARS3D-MENOR1200_????????T????Z.nc\")\n",
    "        candidates = glob.glob(pattern)\n",
    "\n",
    "        def extract_datetime_from_filename(f):\n",
    "            match = re.search(r\"_(\\d{8}T\\d{4})Z\\.nc$\", f)\n",
    "            if not match:\n",
    "                return None\n",
    "            return datetime.strptime(match.group(1), \"%Y%m%dT%H%M\")\n",
    "\n",
    "        for f in candidates:\n",
    "            f_dt = extract_datetime_from_filename(f)\n",
    "            if f_dt and start_dt <= f_dt <= dt:\n",
    "                files.append((f, f_dt))\n",
    "\n",
    "    files.sort(key=lambda x: x[1])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Aucun fichier trouvé entre {start_dt} et {dt}\")\n",
    "\n",
    "    return [f for f, _ in files]\n",
    "\n",
    "\n",
    "def get_temp_sal_for_poly(poly, depth_sampling, ncdf_path):\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    if not hasattr(ds, 'crs'):\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    bathy = ds['H0']\n",
    "    temp = ds['TEMP']\n",
    "    sal = ds['SAL']\n",
    "\n",
    "    poly_gs = gpd.GeoSeries([poly], crs=\"EPSG:4326\")\n",
    "    poly_proj = poly_gs.to_crs(bathy.rio.crs).iloc[0]\n",
    "\n",
    "    transform = bathy.rio.transform()\n",
    "    height, width = bathy.shape\n",
    "\n",
    "    coords = []\n",
    "    bathy_vals = []\n",
    "\n",
    "    for j in range(height):\n",
    "        for i in range(width):\n",
    "            x_min, y_max = transform * (i, j)\n",
    "            x_max, y_min = transform * (i + 1, j + 1)\n",
    "            pixel_poly = box(x_min, y_min, x_max, y_max)\n",
    "            if poly_proj.intersects(pixel_poly):\n",
    "                b_val = bathy.values[j, i]\n",
    "                if not np.isnan(b_val) and b_val > 0:\n",
    "                    coords.append((j, i))\n",
    "                    bathy_vals.append(b_val)\n",
    "\n",
    "    if not bathy_vals:\n",
    "        pixel_centers = []\n",
    "        for j in range(height):\n",
    "            for i in range(width):\n",
    "                x_c, y_c = transform * (i + 0.5, j + 0.5)\n",
    "                pixel_centers.append((j, i, Point(x_c, y_c)))\n",
    "\n",
    "        distances = []\n",
    "        for (j, i, pt) in pixel_centers:\n",
    "            dist = pt.distance(poly_proj)\n",
    "            b_val = bathy.values[j, i]\n",
    "            if not np.isnan(b_val) and b_val > 0:\n",
    "                distances.append((dist, j, i, b_val))\n",
    "\n",
    "        distances.sort(key=lambda x: x[0])\n",
    "        closest = distances[:3]\n",
    "        coords = [(j, i) for _, j, i, _ in closest]\n",
    "        bathy_vals = [b for _, _, _, b in closest]\n",
    "\n",
    "    layers_phys = [int(depth_sampling / b * 60) for b in bathy_vals]\n",
    "    layers_phys = [60 - l for l in layers_phys]\n",
    "    layers_index = [max(0, min(l - 1, 59)) for l in layers_phys]\n",
    "\n",
    "    temp_values = []\n",
    "    sal_values = []\n",
    "    for (j, i), l in zip(coords, layers_index):\n",
    "        t_val = temp.values[0, l, j, i]\n",
    "        s_val = sal.values[0, l, j, i]\n",
    "        if not np.isnan(t_val):\n",
    "            temp_values.append(t_val)\n",
    "        if not np.isnan(s_val):\n",
    "            sal_values.append(s_val)\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "    return temp_values, sal_values\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_folder = \"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/SAL-TEMP_latlon/3H/\"\n",
    "    gdf = gpd.read_file(\"adne_extract_med_ouest.geojson\")\n",
    "\n",
    "    min_temp = []\n",
    "    max_temp = []\n",
    "    mean_temp = []\n",
    "\n",
    "    min_sal = []\n",
    "    max_sal = []\n",
    "    mean_sal = []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction Temp & Sal sur 24h\"):\n",
    "        dt = row['datetime']\n",
    "        try:\n",
    "            files = get_netcdf_paths_for_period(dt, base_folder, hours=24)\n",
    "            all_temp_values = []\n",
    "            all_sal_values = []\n",
    "\n",
    "            for f in files:\n",
    "                t_vals, s_vals = get_temp_sal_for_poly(row['geometry'], row['depth_sampling'], f)\n",
    "                all_temp_values.extend(t_vals)\n",
    "                all_sal_values.extend(s_vals)\n",
    "\n",
    "            if all_temp_values:\n",
    "                min_temp.append(np.min(all_temp_values))\n",
    "                max_temp.append(np.max(all_temp_values))\n",
    "                mean_temp.append(np.mean(all_temp_values))\n",
    "            else:\n",
    "                min_temp.append(np.nan)\n",
    "                max_temp.append(np.nan)\n",
    "                mean_temp.append(np.nan)\n",
    "\n",
    "            if all_sal_values:\n",
    "                min_sal.append(np.min(all_sal_values))\n",
    "                max_sal.append(np.max(all_sal_values))\n",
    "                mean_sal.append(np.mean(all_sal_values))\n",
    "            else:\n",
    "                min_sal.append(np.nan)\n",
    "                max_sal.append(np.nan)\n",
    "                mean_sal.append(np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Fichier manquant pour {dt}: {e}\")\n",
    "            min_temp.append(np.nan)\n",
    "            max_temp.append(np.nan)\n",
    "            mean_temp.append(np.nan)\n",
    "\n",
    "            min_sal.append(np.nan)\n",
    "            max_sal.append(np.nan)\n",
    "            mean_sal.append(np.nan)\n",
    "\n",
    "    gdf['temp_min_24h'] = min_temp\n",
    "    gdf['temp_max_24h'] = max_temp\n",
    "    gdf['temp_mean_24h'] = mean_temp\n",
    "\n",
    "    gdf['sal_min_24h'] = min_sal\n",
    "    gdf['sal_max_24h'] = max_sal\n",
    "    gdf['sal_mean_24h'] = mean_sal\n",
    "\n",
    "    gdf.to_file(\"adne_extract_med_ouest.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826fc119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction 7 jours: 100%|██████████| 70/70 [00:00<00:00, 5875.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier manquant pour 2020-07-16 00:00:00: Aucun fichier max trouvé entre 2020-07-09 00:00:00 et 2020-07-16 00:00:00\n",
      "Fichier manquant pour 2018-04-05 00:00:00: Aucun fichier max trouvé entre 2018-03-29 00:00:00 et 2018-04-05 00:00:00\n",
      "Fichier manquant pour 2018-04-05 00:00:00: Aucun fichier max trouvé entre 2018-03-29 00:00:00 et 2018-04-05 00:00:00\n",
      "Fichier manquant pour 2018-04-06 00:00:00: Aucun fichier max trouvé entre 2018-03-30 00:00:00 et 2018-04-06 00:00:00\n",
      "Fichier manquant pour 2019-07-11 00:00:00: Aucun fichier max trouvé entre 2019-07-04 00:00:00 et 2019-07-11 00:00:00\n",
      "Fichier manquant pour 2020-05-20 00:00:00: Aucun fichier max trouvé entre 2020-05-13 00:00:00 et 2020-05-20 00:00:00\n",
      "Fichier manquant pour 2020-05-20 00:00:00: Aucun fichier max trouvé entre 2020-05-13 00:00:00 et 2020-05-20 00:00:00\n",
      "Fichier manquant pour 2020-07-16 00:00:00: Aucun fichier max trouvé entre 2020-07-09 00:00:00 et 2020-07-16 00:00:00\n",
      "Fichier manquant pour 2020-05-28 00:00:00: Aucun fichier max trouvé entre 2020-05-21 00:00:00 et 2020-05-28 00:00:00\n",
      "Fichier manquant pour 2021-07-29 00:00:00: Aucun fichier max trouvé entre 2021-07-22 00:00:00 et 2021-07-29 00:00:00\n",
      "Fichier manquant pour 2021-07-29 00:00:00: Aucun fichier max trouvé entre 2021-07-22 00:00:00 et 2021-07-29 00:00:00\n",
      "Fichier manquant pour 2021-07-29 00:00:00: Aucun fichier max trouvé entre 2021-07-22 00:00:00 et 2021-07-29 00:00:00\n",
      "Fichier manquant pour 2021-07-29 00:00:00: Aucun fichier max trouvé entre 2021-07-22 00:00:00 et 2021-07-29 00:00:00\n",
      "Fichier manquant pour 2021-06-03 00:00:00: Aucun fichier max trouvé entre 2021-05-27 00:00:00 et 2021-06-03 00:00:00\n",
      "Fichier manquant pour 2021-06-02 00:00:00: Aucun fichier max trouvé entre 2021-05-26 00:00:00 et 2021-06-02 00:00:00\n",
      "Fichier manquant pour 2021-07-27 00:00:00: Aucun fichier max trouvé entre 2021-07-20 00:00:00 et 2021-07-27 00:00:00\n",
      "Fichier manquant pour 2021-07-27 00:00:00: Aucun fichier max trouvé entre 2021-07-20 00:00:00 et 2021-07-27 00:00:00\n",
      "Fichier manquant pour 2021-07-27 00:00:00: Aucun fichier max trouvé entre 2021-07-20 00:00:00 et 2021-07-27 00:00:00\n",
      "Fichier manquant pour 2021-07-27 00:00:00: Aucun fichier max trouvé entre 2021-07-20 00:00:00 et 2021-07-27 00:00:00\n",
      "Fichier manquant pour 2024-06-10 00:00:00: Aucun fichier max trouvé entre 2024-06-03 00:00:00 et 2024-06-10 00:00:00\n",
      "Fichier manquant pour 2024-06-09 00:00:00: Aucun fichier max trouvé entre 2024-06-02 00:00:00 et 2024-06-09 00:00:00\n",
      "Fichier manquant pour 2022-06-14 00:00:00: Aucun fichier max trouvé entre 2022-06-07 00:00:00 et 2022-06-14 00:00:00\n",
      "Fichier manquant pour 2023-06-19 00:00:00: Aucun fichier max trouvé entre 2023-06-12 00:00:00 et 2023-06-19 00:00:00\n",
      "Fichier manquant pour 2023-06-05 00:00:00: Aucun fichier max trouvé entre 2023-05-29 00:00:00 et 2023-06-05 00:00:00\n",
      "Fichier manquant pour 2023-06-05 00:00:00: Aucun fichier max trouvé entre 2023-05-29 00:00:00 et 2023-06-05 00:00:00\n",
      "Fichier manquant pour 2023-08-22 00:00:00: Aucun fichier max trouvé entre 2023-08-15 00:00:00 et 2023-08-22 00:00:00\n",
      "Fichier manquant pour 2023-08-22 00:00:00: Aucun fichier max trouvé entre 2023-08-15 00:00:00 et 2023-08-22 00:00:00\n",
      "Fichier manquant pour 2023-08-21 00:00:00: Aucun fichier max trouvé entre 2023-08-14 00:00:00 et 2023-08-21 00:00:00\n",
      "Fichier manquant pour 2023-08-23 00:00:00: Aucun fichier max trouvé entre 2023-08-16 00:00:00 et 2023-08-23 00:00:00\n",
      "Fichier manquant pour 2023-09-20 00:00:00: Aucun fichier max trouvé entre 2023-09-13 00:00:00 et 2023-09-20 00:00:00\n",
      "Fichier manquant pour 2023-09-20 00:00:00: Aucun fichier max trouvé entre 2023-09-13 00:00:00 et 2023-09-20 00:00:00\n",
      "Fichier manquant pour 2023-09-20 00:00:00: Aucun fichier max trouvé entre 2023-09-13 00:00:00 et 2023-09-20 00:00:00\n",
      "Fichier manquant pour 2023-06-05 00:00:00: Aucun fichier max trouvé entre 2023-05-29 00:00:00 et 2023-06-05 00:00:00\n",
      "Fichier manquant pour 2023-06-06 00:00:00: Aucun fichier max trouvé entre 2023-05-30 00:00:00 et 2023-06-06 00:00:00\n",
      "Fichier manquant pour 2023-06-05 00:00:00: Aucun fichier max trouvé entre 2023-05-29 00:00:00 et 2023-06-05 00:00:00\n",
      "Fichier manquant pour 2023-06-06 00:00:00: Aucun fichier max trouvé entre 2023-05-30 00:00:00 et 2023-06-06 00:00:00\n",
      "Fichier manquant pour 2023-08-22 00:00:00: Aucun fichier max trouvé entre 2023-08-15 00:00:00 et 2023-08-22 00:00:00\n",
      "Fichier manquant pour 2023-08-22 00:00:00: Aucun fichier max trouvé entre 2023-08-15 00:00:00 et 2023-08-22 00:00:00\n",
      "Fichier manquant pour 2023-09-21 00:00:00: Aucun fichier max trouvé entre 2023-09-14 00:00:00 et 2023-09-21 00:00:00\n",
      "Fichier manquant pour 2023-09-20 00:00:00: Aucun fichier max trouvé entre 2023-09-13 00:00:00 et 2023-09-20 00:00:00\n",
      "Fichier manquant pour 2024-06-10 00:00:00: Aucun fichier max trouvé entre 2024-06-03 00:00:00 et 2024-06-10 00:00:00\n",
      "Fichier manquant pour 2023-08-24 00:00:00: Aucun fichier max trouvé entre 2023-08-17 00:00:00 et 2023-08-24 00:00:00\n",
      "Fichier manquant pour 2023-09-20 00:00:00: Aucun fichier max trouvé entre 2023-09-13 00:00:00 et 2023-09-20 00:00:00\n",
      "Fichier manquant pour 2023-10-02 00:00:00: Aucun fichier max trouvé entre 2023-09-25 00:00:00 et 2023-10-02 00:00:00\n",
      "Fichier manquant pour 2024-11-05 00:00:00: Aucun fichier max trouvé entre 2024-10-29 00:00:00 et 2024-11-05 00:00:00\n",
      "Fichier manquant pour 2024-08-26 00:00:00: Aucun fichier max trouvé entre 2024-08-19 00:00:00 et 2024-08-26 00:00:00\n",
      "Fichier manquant pour 2024-11-05 00:00:00: Aucun fichier max trouvé entre 2024-10-29 00:00:00 et 2024-11-05 00:00:00\n",
      "Fichier manquant pour 2024-08-26 00:00:00: Aucun fichier max trouvé entre 2024-08-19 00:00:00 et 2024-08-26 00:00:00\n",
      "Fichier manquant pour 2024-08-28 00:00:00: Aucun fichier max trouvé entre 2024-08-21 00:00:00 et 2024-08-28 00:00:00\n",
      "Fichier manquant pour 2024-06-08 00:00:00: Aucun fichier max trouvé entre 2024-06-01 00:00:00 et 2024-06-08 00:00:00\n",
      "Fichier manquant pour 2024-06-09 00:00:00: Aucun fichier max trouvé entre 2024-06-02 00:00:00 et 2024-06-09 00:00:00\n",
      "Fichier manquant pour 2024-06-09 00:00:00: Aucun fichier max trouvé entre 2024-06-02 00:00:00 et 2024-06-09 00:00:00\n",
      "Fichier manquant pour 2024-06-09 00:00:00: Aucun fichier max trouvé entre 2024-06-02 00:00:00 et 2024-06-09 00:00:00\n",
      "Fichier manquant pour 2024-06-09 00:00:00: Aucun fichier max trouvé entre 2024-06-02 00:00:00 et 2024-06-09 00:00:00\n",
      "Fichier manquant pour 2024-06-09 00:00:00: Aucun fichier max trouvé entre 2024-06-02 00:00:00 et 2024-06-09 00:00:00\n",
      "Fichier manquant pour 2024-06-09 00:00:00: Aucun fichier max trouvé entre 2024-06-02 00:00:00 et 2024-06-09 00:00:00\n",
      "Fichier manquant pour 2024-06-09 00:00:00: Aucun fichier max trouvé entre 2024-06-02 00:00:00 et 2024-06-09 00:00:00\n",
      "Fichier manquant pour 2024-06-09 00:00:00: Aucun fichier max trouvé entre 2024-06-02 00:00:00 et 2024-06-09 00:00:00\n",
      "Fichier manquant pour 2024-06-08 00:00:00: Aucun fichier max trouvé entre 2024-06-01 00:00:00 et 2024-06-08 00:00:00\n",
      "Fichier manquant pour 2024-06-07 00:00:00: Aucun fichier max trouvé entre 2024-05-31 00:00:00 et 2024-06-07 00:00:00\n",
      "Fichier manquant pour 2024-07-05 00:00:00: Aucun fichier max trouvé entre 2024-06-28 00:00:00 et 2024-07-05 00:00:00\n",
      "Fichier manquant pour 2024-06-24 00:00:00: Aucun fichier max trouvé entre 2024-06-17 00:00:00 et 2024-06-24 00:00:00\n",
      "Fichier manquant pour 2024-06-24 00:00:00: Aucun fichier max trouvé entre 2024-06-17 00:00:00 et 2024-06-24 00:00:00\n",
      "Fichier manquant pour 2024-08-29 00:00:00: Aucun fichier max trouvé entre 2024-08-22 00:00:00 et 2024-08-29 00:00:00\n",
      "Fichier manquant pour 2024-08-27 00:00:00: Aucun fichier max trouvé entre 2024-08-20 00:00:00 et 2024-08-27 00:00:00\n",
      "Fichier manquant pour 2024-08-27 00:00:00: Aucun fichier max trouvé entre 2024-08-20 00:00:00 et 2024-08-27 00:00:00\n",
      "Fichier manquant pour 2024-08-28 00:00:00: Aucun fichier max trouvé entre 2024-08-21 00:00:00 et 2024-08-28 00:00:00\n",
      "Fichier manquant pour 2024-08-27 00:00:00: Aucun fichier max trouvé entre 2024-08-20 00:00:00 et 2024-08-27 00:00:00\n",
      "Fichier manquant pour 2024-08-27 00:00:00: Aucun fichier max trouvé entre 2024-08-20 00:00:00 et 2024-08-27 00:00:00\n",
      "Fichier manquant pour 2024-08-28 00:00:00: Aucun fichier max trouvé entre 2024-08-21 00:00:00 et 2024-08-28 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 jours \n",
    "\n",
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, box\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def get_netcdf_paths_for_period(dt, base_folder, days, stat_type):\n",
    "    \"\"\"Retourne la liste des fichiers journaliers pour les `days` jours avant dt (inclus).\"\"\"\n",
    "    start_dt = dt - timedelta(days=days)\n",
    "    files = []\n",
    "\n",
    "    for day in (start_dt + timedelta(n) for n in range((dt - start_dt).days + 1)):\n",
    "        year_folder = os.path.join(base_folder, str(day.year))\n",
    "        fname = f\"MARS3D_{day.strftime('%Y%m%d')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Aucun fichier {stat_type} trouvé entre {start_dt} et {dt}\"\n",
    "        )\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "\n",
    "def get_temp_sal_for_poly(poly, depth_sampling, ncdf_path):\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    if not hasattr(ds, 'crs'):\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    bathy = ds['H0']\n",
    "    temp = ds['TEMP']\n",
    "    sal = ds['SAL']\n",
    "\n",
    "    poly_gs = gpd.GeoSeries([poly], crs=\"EPSG:4326\")\n",
    "    poly_proj = poly_gs.to_crs(bathy.rio.crs).iloc[0]\n",
    "\n",
    "    transform = bathy.rio.transform()\n",
    "    height, width = bathy.shape\n",
    "\n",
    "    coords = []\n",
    "    bathy_vals = []\n",
    "\n",
    "    for j in range(height):\n",
    "        for i in range(width):\n",
    "            x_min, y_max = transform * (i, j)\n",
    "            x_max, y_min = transform * (i + 1, j + 1)\n",
    "            pixel_poly = box(x_min, y_min, x_max, y_max)\n",
    "            if poly_proj.intersects(pixel_poly):\n",
    "                b_val = bathy.values[j, i]\n",
    "                if not np.isnan(b_val) and b_val > 0:\n",
    "                    coords.append((j, i))\n",
    "                    bathy_vals.append(b_val)\n",
    "\n",
    "    if not bathy_vals:\n",
    "        pixel_centers = []\n",
    "        for j in range(height):\n",
    "            for i in range(width):\n",
    "                x_c, y_c = transform * (i + 0.5, j + 0.5)\n",
    "                pixel_centers.append((j, i, Point(x_c, y_c)))\n",
    "\n",
    "        distances = []\n",
    "        for (j, i, pt) in pixel_centers:\n",
    "            dist = pt.distance(poly_proj)\n",
    "            b_val = bathy.values[j, i]\n",
    "            if not np.isnan(b_val) and b_val > 0:\n",
    "                distances.append((dist, j, i, b_val))\n",
    "\n",
    "        distances.sort(key=lambda x: x[0])\n",
    "        closest = distances[:3]\n",
    "        coords = [(j, i) for _, j, i, _ in closest]\n",
    "        bathy_vals = [b for _, _, _, b in closest]\n",
    "\n",
    "    layers_phys = [int(depth_sampling / b * 60) for b in bathy_vals]\n",
    "    layers_phys = [60 - l for l in layers_phys]\n",
    "    layers_index = [max(0, min(l - 1, 59)) for l in layers_phys]\n",
    "\n",
    "    temp_values = []\n",
    "    sal_values = []\n",
    "    for (j, i), l in zip(coords, layers_index):\n",
    "        if temp.ndim == 4:  # Ancien format avec time\n",
    "            t_val = temp.values[0, l, j, i]\n",
    "            s_val = sal.values[0, l, j, i]\n",
    "        else:  # Nouveau format journalier sans time\n",
    "            t_val = temp.values[l, j, i]\n",
    "            s_val = sal.values[l, j, i]\n",
    "        if not np.isnan(t_val):\n",
    "            temp_values.append(t_val)\n",
    "        if not np.isnan(s_val):\n",
    "            sal_values.append(s_val)\n",
    "\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "    return temp_values, sal_values\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_folder = \"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/SAL-TEMP_latlon/Daily/Med-Ouest\"\n",
    "    gdf = gpd.read_file(\"sal_temp/adne_extract_med_ouest.geojson\")\n",
    "\n",
    "    temp_max7 = []\n",
    "    temp_min7 = []\n",
    "    temp_mean7 = []\n",
    "    sal_max7 = []\n",
    "    sal_min7 = []\n",
    "    sal_mean7 = []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction 7 jours\"):\n",
    "        dt = row['date']\n",
    "\n",
    "        try:\n",
    "            # MAX du max\n",
    "            files_max = get_netcdf_paths_for_period(dt, base_folder, days=7, stat_type=\"max\")\n",
    "            all_temp, all_sal = [], []\n",
    "            for f in files_max:\n",
    "                t_vals, s_vals = get_temp_sal_for_poly(row['geometry'], row['depth_sampling'], f)\n",
    "                all_temp.extend(t_vals)\n",
    "                all_sal.extend(s_vals)\n",
    "            temp_max7.append(np.nanmax(all_temp) if all_temp else np.nan)\n",
    "            sal_max7.append(np.nanmax(all_sal) if all_sal else np.nan)\n",
    "\n",
    "            # MIN du min\n",
    "            files_min = get_netcdf_paths_for_period(dt, base_folder, days=7, stat_type=\"min\")\n",
    "            all_temp, all_sal = [], []\n",
    "            for f in files_min:\n",
    "                t_vals, s_vals = get_temp_sal_for_poly(row['geometry'], row['depth_sampling'], f)\n",
    "                all_temp.extend(t_vals)\n",
    "                all_sal.extend(s_vals)\n",
    "            temp_min7.append(np.nanmin(all_temp) if all_temp else np.nan)\n",
    "            sal_min7.append(np.nanmin(all_sal) if all_sal else np.nan)\n",
    "\n",
    "            # MOYENNE des mean\n",
    "            files_mean = get_netcdf_paths_for_period(dt, base_folder, days=7, stat_type=\"mean\")\n",
    "            all_temp, all_sal = [], []\n",
    "            for f in files_mean:\n",
    "                t_vals, s_vals = get_temp_sal_for_poly(row['geometry'], row['depth_sampling'], f)\n",
    "                all_temp.extend(t_vals)\n",
    "                all_sal.extend(s_vals)\n",
    "            temp_mean7.append(np.nanmean(all_temp) if all_temp else np.nan)\n",
    "            sal_mean7.append(np.nanmean(all_sal) if all_sal else np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Fichier manquant pour {dt}: {e}\")\n",
    "            temp_max7.append(np.nan)\n",
    "            temp_min7.append(np.nan)\n",
    "            temp_mean7.append(np.nan)\n",
    "            sal_max7.append(np.nan)\n",
    "            sal_min7.append(np.nan)\n",
    "            sal_mean7.append(np.nan)\n",
    "\n",
    "    gdf['temp_max_7j'] = temp_max7\n",
    "    gdf['temp_min_7j'] = temp_min7\n",
    "    gdf['temp_mean_7j'] = temp_mean7\n",
    "    gdf['sal_max_7j'] = sal_max7\n",
    "    gdf['sal_min_7j'] = sal_min7\n",
    "    gdf['sal_mean_7j'] = sal_mean7\n",
    "\n",
    "    gdf.to_file(\"sal_temp/adne_extract_med_ouest.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee2fe9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction sur 1 mois glissant: 100%|██████████| 70/70 [24:25<00:00, 20.93s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, box\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "def get_netcdf_paths_for_last_month(dt, base_folder, stat_type):\n",
    "    \"\"\"\n",
    "    Retourne la liste des fichiers journaliers pour un mois glissant\n",
    "    allant de (dt - 1 mois) à dt inclus.\n",
    "    \"\"\"\n",
    "    start_dt = dt - relativedelta(months=1)\n",
    "    end_dt = dt\n",
    "\n",
    "    files = []\n",
    "    for day in (start_dt + timedelta(n) for n in range((end_dt - start_dt).days + 1)):\n",
    "        year_folder = os.path.join(base_folder, str(day.year))\n",
    "        fname = f\"MARS3D_{day.strftime('%Y%m%d')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Aucun fichier {stat_type} trouvé entre {start_dt} et {end_dt}\"\n",
    "        )\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_temp_sal_for_poly(poly, depth_sampling, ncdf_path):\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    if not hasattr(ds, 'crs'):\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    bathy = ds['H0']\n",
    "    temp = ds['TEMP']\n",
    "    sal = ds['SAL']\n",
    "\n",
    "    poly_gs = gpd.GeoSeries([poly], crs=\"EPSG:4326\")\n",
    "    poly_proj = poly_gs.to_crs(bathy.rio.crs).iloc[0]\n",
    "\n",
    "    transform = bathy.rio.transform()\n",
    "    height, width = bathy.shape\n",
    "\n",
    "    coords = []\n",
    "    bathy_vals = []\n",
    "\n",
    "    for j in range(height):\n",
    "        for i in range(width):\n",
    "            x_min, y_max = transform * (i, j)\n",
    "            x_max, y_min = transform * (i + 1, j + 1)\n",
    "            pixel_poly = box(x_min, y_min, x_max, y_max)\n",
    "            if poly_proj.intersects(pixel_poly):\n",
    "                b_val = bathy.values[j, i]\n",
    "                if not np.isnan(b_val) and b_val > 0:\n",
    "                    coords.append((j, i))\n",
    "                    bathy_vals.append(b_val)\n",
    "\n",
    "    if not bathy_vals:\n",
    "        pixel_centers = []\n",
    "        for j in range(height):\n",
    "            for i in range(width):\n",
    "                x_c, y_c = transform * (i + 0.5, j + 0.5)\n",
    "                pixel_centers.append((j, i, Point(x_c, y_c)))\n",
    "\n",
    "        distances = []\n",
    "        for (j, i, pt) in pixel_centers:\n",
    "            dist = pt.distance(poly_proj)\n",
    "            b_val = bathy.values[j, i]\n",
    "            if not np.isnan(b_val) and b_val > 0:\n",
    "                distances.append((dist, j, i, b_val))\n",
    "\n",
    "        distances.sort(key=lambda x: x[0])\n",
    "        closest = distances[:3]\n",
    "        coords = [(j, i) for _, j, i, _ in closest]\n",
    "        bathy_vals = [b for _, _, _, b in closest]\n",
    "\n",
    "    layers_phys = [int(depth_sampling / b * 60) for b in bathy_vals]\n",
    "    layers_phys = [60 - l for l in layers_phys]\n",
    "    layers_index = [max(0, min(l - 1, 59)) for l in layers_phys]\n",
    "\n",
    "    temp_values = []\n",
    "    sal_values = []\n",
    "    for (j, i), l in zip(coords, layers_index):\n",
    "        if temp.ndim == 4:  # Ancien format avec time\n",
    "            t_val = temp.values[0, l, j, i]\n",
    "            s_val = sal.values[0, l, j, i]\n",
    "        else:  # Nouveau format journalier sans time\n",
    "            t_val = temp.values[l, j, i]\n",
    "            s_val = sal.values[l, j, i]\n",
    "        if not np.isnan(t_val):\n",
    "            temp_values.append(t_val)\n",
    "        if not np.isnan(s_val):\n",
    "            sal_values.append(s_val)\n",
    "\n",
    "    ds.close()\n",
    "    return temp_values, sal_values\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_folder = \"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/SAL-TEMP_latlon/Daily/Med-Ouest\"\n",
    "    gdf = gpd.read_file(\"adne_extract_med_ouest.geojson\")\n",
    "\n",
    "    temp_max, temp_min, temp_mean = [], [], []\n",
    "    sal_max, sal_min, sal_mean = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction sur 1 mois glissant\"):\n",
    "        dt = row['date']\n",
    "\n",
    "        try:\n",
    "            # MAX du max\n",
    "            files_max = get_netcdf_paths_for_last_month(dt, base_folder, stat_type=\"max\")\n",
    "            all_temp, all_sal = [], []\n",
    "            for f in files_max:\n",
    "                t_vals, s_vals = get_temp_sal_for_poly(row['geometry'], row['depth_sampling'], f)\n",
    "                all_temp.extend(t_vals)\n",
    "                all_sal.extend(s_vals)\n",
    "            temp_max.append(np.nanmax(all_temp) if all_temp else np.nan)\n",
    "            sal_max.append(np.nanmax(all_sal) if all_sal else np.nan)\n",
    "\n",
    "            # MIN du min\n",
    "            files_min = get_netcdf_paths_for_last_month(dt, base_folder, stat_type=\"min\")\n",
    "            all_temp, all_sal = [], []\n",
    "            for f in files_min:\n",
    "                t_vals, s_vals = get_temp_sal_for_poly(row['geometry'], row['depth_sampling'], f)\n",
    "                all_temp.extend(t_vals)\n",
    "                all_sal.extend(s_vals)\n",
    "            temp_min.append(np.nanmin(all_temp) if all_temp else np.nan)\n",
    "            sal_min.append(np.nanmin(all_sal) if all_sal else np.nan)\n",
    "\n",
    "            # MOYENNE des mean\n",
    "            files_mean = get_netcdf_paths_for_last_month(dt, base_folder, stat_type=\"mean\")\n",
    "            all_temp, all_sal = [], []\n",
    "            for f in files_mean:\n",
    "                t_vals, s_vals = get_temp_sal_for_poly(row['geometry'], row['depth_sampling'], f)\n",
    "                all_temp.extend(t_vals)\n",
    "                all_sal.extend(s_vals)\n",
    "            temp_mean.append(np.nanmean(all_temp) if all_temp else np.nan)\n",
    "            sal_mean.append(np.nanmean(all_sal) if all_sal else np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Fichier manquant pour {dt}: {e}\")\n",
    "            temp_max.append(np.nan)\n",
    "            temp_min.append(np.nan)\n",
    "            temp_mean.append(np.nan)\n",
    "            sal_max.append(np.nan)\n",
    "            sal_min.append(np.nan)\n",
    "            sal_mean.append(np.nan)\n",
    "\n",
    "    gdf['temp_max_1m'] = temp_max\n",
    "    gdf['temp_min_1m'] = temp_min\n",
    "    gdf['temp_mean_1m'] = temp_mean\n",
    "    gdf['sal_max_1m'] = sal_max\n",
    "    gdf['sal_min_1m'] = sal_min\n",
    "    gdf['sal_mean_1m'] = sal_mean\n",
    "\n",
    "    gdf.to_file(\"adne_extract_med_ouest.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca75a2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction 1 an: 100%|██████████| 70/70 [22:49<00:00, 19.57s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, box\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Fonctions utilitaires\n",
    "# -----------------------------\n",
    "\n",
    "def get_monthly_paths(dt_start, dt_end, base_folder, stat_type):\n",
    "    \"\"\"Retourne les fichiers mensuels (min/max/mean) entre dt_start et le mois précédent dt_end.\"\"\"\n",
    "    files = []\n",
    "    months = pd.date_range(start=dt_start, end=dt_end, freq='MS')  # Month Start\n",
    "    for month in months[:-1]:  # tous les mois sauf le dernier\n",
    "        year_folder = os.path.join(base_folder, str(month.year))\n",
    "        fname = f\"MARS3D_{month.strftime('%Y%m')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "    return files\n",
    "\n",
    "def get_daily_paths(dt_start, dt_end, base_folder, stat_type):\n",
    "    \"\"\"Retourne les fichiers journaliers (min/max/mean) entre dt_start et dt_end.\"\"\"\n",
    "    files = []\n",
    "    for day in (dt_start + timedelta(n) for n in range((dt_end - dt_start).days + 1)):\n",
    "        year_folder = os.path.join(base_folder, str(day.year))\n",
    "        fname = f\"MARS3D_{day.strftime('%Y%m%d')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "    return files\n",
    "\n",
    "def get_temp_sal_for_poly(poly, depth_sampling, ncdf_path):\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    if not hasattr(ds, 'crs'):\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    bathy = ds['H0']\n",
    "    temp = ds['TEMP']\n",
    "    sal = ds['SAL']\n",
    "\n",
    "    poly_gs = gpd.GeoSeries([poly], crs=\"EPSG:4326\")\n",
    "    poly_proj = poly_gs.to_crs(bathy.rio.crs).iloc[0]\n",
    "\n",
    "    transform = bathy.rio.transform()\n",
    "    height, width = bathy.shape\n",
    "\n",
    "    coords = []\n",
    "    bathy_vals = []\n",
    "\n",
    "    for j in range(height):\n",
    "        for i in range(width):\n",
    "            x_min, y_max = transform * (i, j)\n",
    "            x_max, y_min = transform * (i + 1, j + 1)\n",
    "            pixel_poly = box(x_min, y_min, x_max, y_min + (y_max - y_min))\n",
    "            if poly_proj.intersects(pixel_poly):\n",
    "                b_val = bathy.values[j, i]\n",
    "                if not np.isnan(b_val) and b_val > 0:\n",
    "                    coords.append((j, i))\n",
    "                    bathy_vals.append(b_val)\n",
    "\n",
    "    if not bathy_vals:\n",
    "        # Si aucun pixel intersecte, prendre les 3 pixels les plus proches\n",
    "        pixel_centers = []\n",
    "        for j in range(height):\n",
    "            for i in range(width):\n",
    "                x_c, y_c = transform * (i + 0.5, j + 0.5)\n",
    "                pixel_centers.append((j, i, Point(x_c, y_c)))\n",
    "\n",
    "        distances = []\n",
    "        for (j, i, pt) in pixel_centers:\n",
    "            dist = pt.distance(poly_proj)\n",
    "            b_val = bathy.values[j, i]\n",
    "            if not np.isnan(b_val) and b_val > 0:\n",
    "                distances.append((dist, j, i, b_val))\n",
    "\n",
    "        distances.sort(key=lambda x: x[0])\n",
    "        closest = distances[:3]\n",
    "        coords = [(j, i) for _, j, i, _ in closest]\n",
    "        bathy_vals = [b for _, _, _, b in closest]\n",
    "\n",
    "    layers_phys = [int(depth_sampling / b * 60) for b in bathy_vals]\n",
    "    layers_phys = [60 - l for l in layers_phys]\n",
    "    layers_index = [max(0, min(l - 1, 59)) for l in layers_phys]\n",
    "\n",
    "    temp_values = []\n",
    "    sal_values = []\n",
    "    for (j, i), l in zip(coords, layers_index):\n",
    "        if temp.ndim == 4:  # Ancien format avec time\n",
    "            t_val = temp.values[0, l, j, i]\n",
    "            s_val = sal.values[0, l, j, i]\n",
    "        else:  # Nouveau format journalier sans time\n",
    "            t_val = temp.values[l, j, i]\n",
    "            s_val = sal.values[l, j, i]\n",
    "        if not np.isnan(t_val):\n",
    "            temp_values.append(t_val)\n",
    "        if not np.isnan(s_val):\n",
    "            sal_values.append(s_val)\n",
    "\n",
    "    ds.close()\n",
    "    return temp_values, sal_values\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    daily_base_folder = \"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/SAL-TEMP_latlon/Daily/Med-Ouest\"\n",
    "    monthly_base_folder = \"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/SAL-TEMP_latlon/Monthly/Med-Ouest\"\n",
    "    gdf = gpd.read_file(\"adne_extract_med_ouest.geojson\")\n",
    "\n",
    "    temp_max, temp_min, temp_mean = [], [], []\n",
    "    sal_max, sal_min, sal_mean = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction 1 an\"):\n",
    "        dt = row['date']\n",
    "\n",
    "        dt_start = dt - timedelta(days=365)\n",
    "        last_month_start = dt.replace(day=1)\n",
    "\n",
    "        try:\n",
    "            all_stats = {}\n",
    "            for stat in ['max', 'min', 'mean']:\n",
    "                # Fichiers mensuels sauf dernier mois\n",
    "                files_monthly = get_monthly_paths(dt_start, last_month_start, monthly_base_folder, stat)\n",
    "                # Fichiers journaliers du dernier mois\n",
    "                files_daily = get_daily_paths(last_month_start, dt, daily_base_folder, stat)\n",
    "                files = files_monthly + files_daily\n",
    "\n",
    "                all_temp, all_sal = [], []\n",
    "                for f in files:\n",
    "                    t_vals, s_vals = get_temp_sal_for_poly(row['geometry'], row['depth_sampling'], f)\n",
    "                    all_temp.extend(t_vals)\n",
    "                    all_sal.extend(s_vals)\n",
    "\n",
    "                if stat == 'max':\n",
    "                    temp_max.append(np.nanmax(all_temp) if all_temp else np.nan)\n",
    "                    sal_max.append(np.nanmax(all_sal) if all_sal else np.nan)\n",
    "                elif stat == 'min':\n",
    "                    temp_min.append(np.nanmin(all_temp) if all_temp else np.nan)\n",
    "                    sal_min.append(np.nanmin(all_sal) if all_sal else np.nan)\n",
    "                elif stat == 'mean':\n",
    "                    temp_mean.append(np.nanmean(all_temp) if all_temp else np.nan)\n",
    "                    sal_mean.append(np.nanmean(all_sal) if all_sal else np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Fichier manquant pour {dt}: {e}\")\n",
    "            temp_max.append(np.nan)\n",
    "            temp_min.append(np.nan)\n",
    "            temp_mean.append(np.nan)\n",
    "            sal_max.append(np.nan)\n",
    "            sal_min.append(np.nan)\n",
    "            sal_mean.append(np.nan)\n",
    "\n",
    "    gdf['temp_max_1y'] = temp_max\n",
    "    gdf['temp_min_1y'] = temp_min\n",
    "    gdf['temp_mean_1y'] = temp_mean\n",
    "    gdf['sal_max_1y'] = sal_max\n",
    "    gdf['sal_min_1y'] = sal_min\n",
    "    gdf['sal_mean_1y'] = sal_mean\n",
    "\n",
    "    gdf.to_file(\"adne_extract_med_ouest.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mars3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
