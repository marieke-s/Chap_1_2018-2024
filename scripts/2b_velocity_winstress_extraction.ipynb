{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13811e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, box\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. Récupération du fichier mensuel du mois précédent\n",
    "# -------------------------------------------------------------\n",
    "def get_monthly_netcdf_path(dt, base_folder, stat_type):\n",
    "    \"\"\"\n",
    "    Retourne le fichier mensuel correspondant au mois précédent.\n",
    "    Exemple : dt = 2019-07-01 → récupère fichier de juin 2019.\n",
    "    \"\"\"\n",
    "    prev_month = dt - relativedelta(months=1)\n",
    "    yyyymm = prev_month.strftime(\"%Y%m\")\n",
    "\n",
    "    year_folder = os.path.join(base_folder, str(prev_month.year))\n",
    "    fname = f\"MARS3D_{yyyymm}_{stat_type}.nc\"\n",
    "    fpath = os.path.join(year_folder, fname)\n",
    "\n",
    "    if not os.path.exists(fpath):\n",
    "        raise FileNotFoundError(f\"Fichier introuvable : {fpath}\")\n",
    "\n",
    "    return fpath\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2. Extraction WS / VEL pour un polygone\n",
    "# -------------------------------------------------------------\n",
    "def get_ws_vel_for_poly(poly, ncdf_path, n_closest=3):\n",
    "    \"\"\"\n",
    "    Extraction WS/VEL pour un polygone et un fichier NetCDF mensuel.\n",
    "    Moyenne pondérée par fraction de pixel intersecté.\n",
    "    Fallback : prendre les n_closest pixels les plus proches qui ont des valeurs NON-NaN.\n",
    "    \"\"\"\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    if not hasattr(ds, 'crs'):\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    ws = ds['WINDSTRESS'].isel(time=0)            # 2D (y,x)\n",
    "    vel = ds['VELOCITY'].isel(time=0, level=-1)   # 2D (y,x)\n",
    "\n",
    "    poly_gs = gpd.GeoSeries([poly], crs=\"EPSG:4326\")\n",
    "    poly_proj = poly_gs.to_crs(ws.rio.crs).iloc[0]\n",
    "\n",
    "    transform = ws.rio.transform()\n",
    "    height, width = ws.shape\n",
    "\n",
    "    coords = []\n",
    "    weights = []\n",
    "\n",
    "    # ---- 1) Recherche des pixels intersectés ----\n",
    "    for j in range(height):\n",
    "        for i in range(width):\n",
    "            x_min, y_max = transform * (i, j)\n",
    "            x_max, y_min = transform * (i + 1, j + 1)\n",
    "            pixel_poly = box(x_min, y_min, x_max, y_max)\n",
    "\n",
    "            intersection = poly_proj.intersection(pixel_poly)\n",
    "            if not intersection.is_empty:\n",
    "                coords.append((j, i))\n",
    "                weights.append(intersection.area / pixel_poly.area)\n",
    "\n",
    "    # Helper : cherche et renvoie jusqu'à n_closest pixels les plus proches AVEC ws non-NaN\n",
    "    def find_n_closest_with_valid_ws(n):\n",
    "        dlist = []\n",
    "        for jj in range(height):\n",
    "            for ii in range(width):\n",
    "                wval = ws.values[jj, ii]\n",
    "                if np.isnan(wval):\n",
    "                    continue\n",
    "                x_c, y_c = transform * (ii + 0.5, jj + 0.5)\n",
    "                d = Point(x_c, y_c).distance(poly_proj)\n",
    "                dlist.append((d, jj, ii))\n",
    "        if not dlist:\n",
    "            return [], []\n",
    "        dlist.sort(key=lambda x: x[0])\n",
    "        chosen = dlist[:n]\n",
    "        coords_fb = [(jj, ii) for _, jj, ii in chosen]\n",
    "        weights_fb = [1.0] * len(coords_fb)  # uniform weight fallback\n",
    "        return coords_fb, weights_fb\n",
    "\n",
    "    # ---- 2) Si pas d'intersection, fallback vers n_closest valides ----\n",
    "    if len(coords) == 0:\n",
    "        coords, weights = find_n_closest_with_valid_ws(n_closest)\n",
    "\n",
    "    # ---- 3) Extraction des valeurs en synchronisant poids (on garde poids seulement si ws valide) ----\n",
    "    ws_vals = []\n",
    "    vel_vals = []\n",
    "    valid_weights = []\n",
    "\n",
    "    for wgt, (j, i) in zip(weights, coords):\n",
    "        w = ws.values[j, i]\n",
    "        v = vel.values[j, i]\n",
    "        if not np.isnan(w):\n",
    "            ws_vals.append(w)\n",
    "            vel_vals.append(v if not np.isnan(v) else np.nan)\n",
    "            valid_weights.append(wgt)\n",
    "\n",
    "    # ---- 4) Si les intersectés existent mais tous NaN, on fait fallback ciblé sur valeurs valides ----\n",
    "    if len(ws_vals) == 0:\n",
    "        coords_fb, weights_fb = find_n_closest_with_valid_ws(n_closest)\n",
    "        if coords_fb:\n",
    "            ws_vals = []\n",
    "            vel_vals = []\n",
    "            valid_weights = []\n",
    "            for wgt, (j, i) in zip(weights_fb, coords_fb):\n",
    "                w = ws.values[j, i]\n",
    "                v = vel.values[j, i]\n",
    "                # ici on a déjà filtré ws non-NaN dans find_n_closest_with_valid_ws,\n",
    "                # mais on vérifie quand même pour être safe\n",
    "                if not np.isnan(w):\n",
    "                    ws_vals.append(w)\n",
    "                    vel_vals.append(v if not np.isnan(v) else np.nan)\n",
    "                    valid_weights.append(wgt)\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "    # ---- 5) Si toujours vide, renvoyer NaN ----\n",
    "    if len(ws_vals) == 0:\n",
    "        return [], [], np.nan, np.nan\n",
    "\n",
    "    ws_vals = np.array(ws_vals)\n",
    "    vel_vals = np.array(vel_vals)\n",
    "    weights_arr = np.array(valid_weights)\n",
    "\n",
    "    ws_mean = np.nansum(ws_vals * weights_arr) / np.nansum(weights_arr)\n",
    "    vel_mean = np.nansum(vel_vals * weights_arr) / np.nansum(weights_arr)\n",
    "\n",
    "    return ws_vals.tolist(), vel_vals.tolist(), ws_mean, vel_mean\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. Programme principal\n",
    "# -------------------------------------------------------------\n",
    "def main():\n",
    "\n",
    "    base_folder =   \"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/CUR-WIND_latlon/Monthly/Med-Est\"\n",
    "\n",
    "    gdf = gpd.read_file(\"grille_med_est.geojson\")\n",
    "\n",
    "    ws_max1m, ws_min1m, ws_mean1m = [], [], []\n",
    "    vel_max1m, vel_min1m, vel_mean1m = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction mensuelle\"):\n",
    "        dt = row['date']\n",
    "\n",
    "        try:\n",
    "            # ---- MAX ----\n",
    "            f_max = get_monthly_netcdf_path(dt, base_folder, \"max\")\n",
    "            t_vals, s_vals, _, _ = get_ws_vel_for_poly(row['geometry'], f_max)\n",
    "            ws_max1m.append(np.nanmax(t_vals) if len(t_vals) else np.nan)\n",
    "            vel_max1m.append(np.nanmax(s_vals) if len(s_vals) else np.nan)\n",
    "\n",
    "            # ---- MIN ----\n",
    "            f_min = get_monthly_netcdf_path(dt, base_folder, \"min\")\n",
    "            t_vals, s_vals, _, _ = get_ws_vel_for_poly(row['geometry'], f_min)\n",
    "            ws_min1m.append(np.nanmin(t_vals) if len(t_vals) else np.nan)\n",
    "            vel_min1m.append(np.nanmin(s_vals) if len(s_vals) else np.nan)\n",
    "\n",
    "            # ---- MEAN ----\n",
    "            f_mean = get_monthly_netcdf_path(dt, base_folder, \"mean\")\n",
    "            _, _, t_mean, s_mean = get_ws_vel_for_poly(row['geometry'], f_mean)\n",
    "            ws_mean1m.append(t_mean)\n",
    "            vel_mean1m.append(s_mean)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"⚠️ Fichier manquant pour {dt}: {e}\")\n",
    "            ws_max1m.append(np.nan)\n",
    "            ws_min1m.append(np.nan)\n",
    "            ws_mean1m.append(np.nan)\n",
    "            vel_max1m.append(np.nan)\n",
    "            vel_min1m.append(np.nan)\n",
    "            vel_mean1m.append(np.nan)\n",
    "\n",
    "    # ---- Sauvegarde ----\n",
    "    gdf['ws_max_surface'] = ws_max1m\n",
    "    gdf['ws_min_surface'] = ws_min1m\n",
    "    gdf['ws_mean_surface'] = ws_mean1m\n",
    "    gdf['vel_max_surface'] = vel_max1m\n",
    "    gdf['vel_min_surface'] = vel_min1m\n",
    "    gdf['vel_mean_surface'] = vel_mean1m\n",
    "\n",
    "    gdf.to_file(\"grille_med_est.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
