{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30360b47",
   "metadata": {},
   "source": [
    "## Extraction des variables température et salinité en surface et en profondeur à partir des données MARS 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23caa2b",
   "metadata": {},
   "source": [
    "#### 1. Extraction 24h en amont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f0ba83",
   "metadata": {},
   "source": [
    "a. En profondeur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, box\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\"\"\" Trouve tous les fichiers NetCDF dans la fenêtre temporelle donnée \"\"\"\n",
    "def get_netcdf_paths_for_period(dt, base_folder, hours=24):\n",
    "    \n",
    "    start_dt = dt - timedelta(hours=hours)\n",
    "    files = []\n",
    "\n",
    "    for year in range(start_dt.year, dt.year + 1):\n",
    "        year_folder = os.path.join(base_folder, str(year))\n",
    "        pattern = os.path.join(year_folder, \"MARC_F2-MARS3D-MENOR1200_????????T????Z.nc\")\n",
    "        candidates = glob.glob(pattern)\n",
    "\n",
    "        def extract_datetime_from_filename(f):\n",
    "            match = re.search(r\"_(\\d{8}T\\d{4})Z\\.nc$\", f)\n",
    "            if not match:\n",
    "                return None\n",
    "            return datetime.strptime(match.group(1), \"%Y%m%dT%H%M\")\n",
    "\n",
    "        for f in candidates:\n",
    "            f_dt = extract_datetime_from_filename(f)\n",
    "            if f_dt and start_dt <= f_dt <= dt:\n",
    "                files.append((f, f_dt))\n",
    "\n",
    "    files.sort(key=lambda x: x[1])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Aucun fichier trouvé entre {start_dt} et {dt}\")\n",
    "\n",
    "    return [f for f, _ in files]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Extraction TEMP/SAL pour un polygone et un fichier NetCDF.\n",
    "    - Calcul de la couche exacte selon bathymétrie locale\n",
    "    - Fallback sur 3 pixels les plus proches si aucun pixel intersecté\n",
    "    - Moyenne pondérée par fraction de surface du polygone intersectant le pixel\n",
    "    - Valeurs min max et mean pondérée en sortie \n",
    "    \"\"\"\n",
    "\n",
    "def get_temp_sal_for_poly(poly, depth_sampling_surface, ncdf_path):\n",
    "    \n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    if not hasattr(ds, \"crs\"):\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "# --- Définition des variables ---\n",
    "# Définition de la couche de profondeur pour récupérer les valeurs à la profondeur d'échantillonage souhaitée\n",
    "    bathy = ds[\"H0\"]\n",
    "    temp = ds[\"TEMP\"]\n",
    "    sal = ds[\"SAL\"]\n",
    "\n",
    "    poly_gs = gpd.GeoSeries([poly], crs=\"EPSG:4326\")\n",
    "    poly_proj = poly_gs.to_crs(bathy.rio.crs).iloc[0]\n",
    "\n",
    "    transform = bathy.rio.transform()\n",
    "    height, width = bathy.shape\n",
    "\n",
    "    coords = []\n",
    "    bathy_vals = []\n",
    "    weights = []\n",
    "\n",
    "    # --- Pixels intersectés par le polygone + aire de l'intersection + profondeur par pixels ---\n",
    "    for j in range(height):\n",
    "        for i in range(width):\n",
    "            x_min, y_max = transform * (i, j)\n",
    "            x_max, y_min = transform * (i + 1, j + 1)\n",
    "            pixel_poly = box(x_min, y_min, x_max, y_max)\n",
    "            intersection = poly_proj.intersection(pixel_poly)\n",
    "            if not intersection.is_empty:\n",
    "                b_val = bathy.values[j, i]\n",
    "                if not np.isnan(b_val) and b_val > 0:\n",
    "                    coords.append((j, i))\n",
    "                    bathy_vals.append(b_val)\n",
    "                    frac = intersection.area / pixel_poly.area\n",
    "                    weights.append(frac)\n",
    "\n",
    "    # --- Fallback : 3 pixels les plus proches si aucun pixel intersecté ---\n",
    "    if not bathy_vals:\n",
    "        pixel_distances = []\n",
    "        for j in range(height):\n",
    "            for i in range(width):\n",
    "                b_val = bathy.values[j, i]\n",
    "                if np.isnan(b_val) or b_val <= 0:\n",
    "                    continue\n",
    "                x_c, y_c = transform * (i + 0.5, j + 0.5)\n",
    "                dist = Point(x_c, y_c).distance(poly_proj)\n",
    "                pixel_distances.append((dist, j, i, b_val))\n",
    "\n",
    "        pixel_distances.sort(key=lambda x: x[0])\n",
    "        closest = pixel_distances[:3]\n",
    "        coords = [(j, i) for _, j, i, _ in closest]\n",
    "        bathy_vals = [b for _, _, _, b in closest]\n",
    "        weights = [1.0 for _ in closest]  # poids uniforme pour fallback\n",
    "\n",
    "    # --- Calcul de la couche verticale selon bathymétrie locale ---\n",
    "    layers_phys = [int(depth_sampling_surface / b * 60) for b in bathy_vals]\n",
    "    layers_phys = [60 - l for l in layers_phys]\n",
    "    layers_index = [max(0, min(l - 1, 59)) for l in layers_phys]\n",
    "\n",
    "    # --- Extraction TEMP/SAL par pixel et moyenne pondérée ---\n",
    "    temp_values, sal_values = [], []\n",
    "    temp_weights, sal_weights = [], []\n",
    "\n",
    "    for (j, i), l, w in zip(coords, layers_index, weights):\n",
    "        t_val = temp.values[0, l, j, i]\n",
    "        s_val = sal.values[0, l, j, i]\n",
    "        if not np.isnan(t_val):\n",
    "            temp_values.append(t_val)\n",
    "            temp_weights.append(w)\n",
    "        if not np.isnan(s_val):\n",
    "            sal_values.append(s_val)\n",
    "            sal_weights.append(w)\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "    if not temp_values:\n",
    "        return [], [], np.nan, np.nan\n",
    "\n",
    "    temp_values = np.array(temp_values)\n",
    "    sal_values = np.array(sal_values)\n",
    "    temp_weights = np.array(temp_weights)\n",
    "    sal_weights = np.array(sal_weights)\n",
    "\n",
    "    temp_weighted_mean = np.nansum(temp_values * temp_weights) / np.nansum(temp_weights)\n",
    "    sal_weighted_mean = np.nansum(sal_values * sal_weights) / np.nansum(sal_weights)\n",
    "\n",
    "    return temp_values, sal_values, temp_weighted_mean, sal_weighted_mean\n",
    "\n",
    "\"\"\" Lacement de la fonction \"\"\"\n",
    "\n",
    "def main():\n",
    "    base_folder = # Chemin des fichiers netcdf 3 hours. Exemple : \"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/SAL-TEMP_latlon/3H/\"\n",
    "    gdf = gpd.read_file() # Chemin du fichier de polygone. Exemple : \"/home/paulinev/Bureau/Mars3D/sal_temp/adne_extract_med_ouest.geojson\"\n",
    "\n",
    "# Initiation des valeurs à enregistrer \n",
    "    min_temp, max_temp, mean_temp = [], [], []\n",
    "    min_sal, max_sal, mean_sal = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction Temp & Sal sur 24h\"):\n",
    "        dt = row['datetime'] # à modifier selon la colonne datetime disponible dans votre dataset  \n",
    "        try:\n",
    "            files = get_netcdf_paths_for_period(dt, base_folder, hours=24)\n",
    "            all_temp_values, all_sal_values = [], []\n",
    "            weighted_temp_means, weighted_sal_means = [], []\n",
    "\n",
    "            for f in files:\n",
    "                t_vals, s_vals, t_wmean, s_wmean = get_temp_sal_for_poly(\n",
    "                    row['geometry'], row['depth_sampling_surface'], f\n",
    "                ) # à modifier selon le nom de la colonne de profondeur de sampling  \n",
    "                all_temp_values.extend(t_vals)\n",
    "                all_sal_values.extend(s_vals)\n",
    "                if not np.isnan(t_wmean):\n",
    "                    weighted_temp_means.append(t_wmean)\n",
    "                if not np.isnan(s_wmean):\n",
    "                    weighted_sal_means.append(s_wmean)\n",
    "\n",
    "            if all_temp_values:\n",
    "                min_temp.append(np.nanmin(all_temp_values))\n",
    "                max_temp.append(np.nanmax(all_temp_values))\n",
    "                mean_temp.append(np.nanmean(weighted_temp_means))\n",
    "            else:\n",
    "                min_temp.append(np.nan)\n",
    "                max_temp.append(np.nan)\n",
    "                mean_temp.append(np.nan)\n",
    "\n",
    "            if all_sal_values:\n",
    "                min_sal.append(np.nanmin(all_sal_values))\n",
    "                max_sal.append(np.nanmax(all_sal_values))\n",
    "                mean_sal.append(np.nanmean(weighted_sal_means))\n",
    "            else:\n",
    "                min_sal.append(np.nan)\n",
    "                max_sal.append(np.nan)\n",
    "                mean_sal.append(np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"⚠️ Fichier manquant pour {dt}: {e}\")\n",
    "            min_temp.append(np.nan)\n",
    "            max_temp.append(np.nan)\n",
    "            mean_temp.append(np.nan)\n",
    "            min_sal.append(np.nan)\n",
    "            max_sal.append(np.nan)\n",
    "            mean_sal.append(np.nan)\n",
    "\n",
    "# Inférence des valeurs aux colonnes \n",
    "    gdf['temp_min_24h'] = min_temp\n",
    "    gdf['temp_max_24h'] = max_temp\n",
    "    gdf['temp_mean_24h'] = mean_temp\n",
    "    gdf['sal_min_24h'] = min_sal\n",
    "    gdf['sal_max_24h'] = max_sal\n",
    "    gdf['sal_mean_24h'] = mean_sal\n",
    "\n",
    "# Enregistrement du fichier \n",
    "    gdf.to_file(\"adne_extract_med_ouest.geojson\", driver=\"GeoJSON\") # modifier le nom de sortie \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e95d3b",
   "metadata": {},
   "source": [
    "b. En surface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fccc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import box\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def get_netcdf_paths_for_period(dt, base_folder, hours=24):\n",
    "    start_dt = dt - timedelta(hours=hours)\n",
    "    files = []\n",
    "\n",
    "    for year in range(start_dt.year, dt.year + 1):\n",
    "        year_folder = os.path.join(base_folder, str(year))\n",
    "        pattern = os.path.join(year_folder, \"MARC_F2-MARS3D-MENOR1200_????????T????Z.nc\")\n",
    "        candidates = glob.glob(pattern)\n",
    "\n",
    "        def extract_datetime_from_filename(f):\n",
    "            match = re.search(r\"_(\\d{8}T\\d{4})Z\\.nc$\", f)\n",
    "            if not match:\n",
    "                return None\n",
    "            return datetime.strptime(match.group(1), \"%Y%m%dT%H%M\")\n",
    "\n",
    "        for f in candidates:\n",
    "            f_dt = extract_datetime_from_filename(f)\n",
    "            if f_dt and start_dt <= f_dt <= dt:\n",
    "                files.append((f, f_dt))\n",
    "\n",
    "    files.sort(key=lambda x: x[1])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Aucun fichier trouvé entre {start_dt} et {dt}\")\n",
    "\n",
    "    return [f for f, _ in files]\n",
    "\n",
    "\n",
    "def get_ws_vel_for_poly(poly, ncdf_path, depth_index=0):\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    if not hasattr(ds, \"crs\"):\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    ws = ds[\"TEMP\"]\n",
    "    vel = ds[\"SAL\"]\n",
    "\n",
    "    # Clip raster avec polygone (vectorisé, rapide)\n",
    "    ws_clip = ws.rio.clip([poly], all_touched=True, drop=False)\n",
    "    vel_clip = vel.rio.clip([poly], all_touched=True, drop=False)\n",
    "\n",
    "    # Extraction des valeurs non-NaN\n",
    "    ws_vals = ws_clip.values[0].flatten()\n",
    "    ws_vals = ws_vals[~np.isnan(ws_vals)]\n",
    "\n",
    "    vel_vals = vel_clip.values[0, depth_index].flatten()\n",
    "    vel_vals = vel_vals[~np.isnan(vel_vals)]\n",
    "\n",
    "    # Fallback si aucun pixel intersecté\n",
    "    if len(ws_vals) == 0 or len(vel_vals) == 0:\n",
    "        # Récupère les centres de pixels non-NaN pour fallback\n",
    "        transform = ws.rio.transform()\n",
    "        height, width = ws.shape[1:]\n",
    "        xs = np.arange(width) + 0.5\n",
    "        ys = np.arange(height) + 0.5\n",
    "        xv, yv = np.meshgrid(xs, ys)\n",
    "        x_coords, y_coords = transform * (xv, yv)\n",
    "\n",
    "        # TEMP fallback\n",
    "        if len(ws_vals) == 0:\n",
    "            ws_all = ws.values[0]\n",
    "            valid_idx = ~np.isnan(ws_all)\n",
    "            coords = np.column_stack([x_coords[valid_idx], y_coords[valid_idx]])\n",
    "            values = ws_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            ws_vals = values[idx]\n",
    "\n",
    "        # SAL fallback\n",
    "        if len(vel_vals) == 0:\n",
    "            vel_all = vel.values[0, depth_index]\n",
    "            valid_idx = ~np.isnan(vel_all)\n",
    "            coords = np.column_stack([x_coords[valid_idx], y_coords[valid_idx]])\n",
    "            values = vel_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            vel_vals = values[idx]\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "    ws_mean = np.mean(ws_vals) if len(ws_vals) > 0 else np.nan\n",
    "    vel_mean = np.mean(vel_vals) if len(vel_vals) > 0 else np.nan\n",
    "\n",
    "    return ws_vals, vel_vals, ws_mean, vel_mean\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_folder = # \"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/CUR-WIND_latlon/3H\"\n",
    "    gdf = gpd.read_file() # à modifier \n",
    "\n",
    "    ws_min, ws_max, ws_mean = [], [], []\n",
    "    vel_min, vel_max, vel_mean = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction WINDSTRESS & VELOCITY sur 24h\"):\n",
    "        dt = row[\"datetime\"] # à modifier \n",
    "        try:\n",
    "            files = get_netcdf_paths_for_period(dt, base_folder, hours=24)\n",
    "            all_ws, all_vel = [], []\n",
    "            weighted_ws, weighted_vel = [], []\n",
    "\n",
    "            for f in files:\n",
    "                ws_vals, vel_vals, ws_wmean, vel_wmean = get_ws_vel_for_poly(row[\"geometry\"], f)\n",
    "                all_ws.extend(ws_vals)\n",
    "                all_vel.extend(vel_vals)\n",
    "                if not np.isnan(ws_wmean):\n",
    "                    weighted_ws.append(ws_wmean)\n",
    "                if not np.isnan(vel_wmean):\n",
    "                    weighted_vel.append(vel_wmean)\n",
    "\n",
    "            ws_min.append(np.nanmin(all_ws) if all_ws else np.nan)\n",
    "            ws_max.append(np.nanmax(all_ws) if all_ws else np.nan)\n",
    "            ws_mean.append(np.nanmean(weighted_ws) if weighted_ws else np.nan)\n",
    "\n",
    "            vel_min.append(np.nanmin(all_vel) if all_vel else np.nan)\n",
    "            vel_max.append(np.nanmax(all_vel) if all_vel else np.nan)\n",
    "            vel_mean.append(np.nanmean(weighted_vel) if weighted_vel else np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"⚠️ Fichier manquant pour {dt}: {e}\")\n",
    "            ws_min.append(np.nan)\n",
    "            ws_max.append(np.nan)\n",
    "            ws_mean.append(np.nan)\n",
    "            vel_min.append(np.nan)\n",
    "            vel_max.append(np.nan)\n",
    "            vel_mean.append(np.nan)\n",
    "\n",
    "    gdf[\"temp_min_24h\"] = ws_min\n",
    "    gdf[\"temp_max_24h\"] = ws_max\n",
    "    gdf[\"temp_mean_24h\"] = ws_mean\n",
    "    gdf[\"sal_min_24h\"] = vel_min\n",
    "    gdf[\"sal_max_24h\"] = vel_max\n",
    "    gdf[\"sal_mean_24h\"] = vel_mean\n",
    "\n",
    "    gdf.to_file(\"adne_extract_corse.geojson\", driver=\"GeoJSON\") # à modifier \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3654740",
   "metadata": {},
   "source": [
    "#### 2. Extraction 7 jours en amont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9922f49",
   "metadata": {},
   "source": [
    "a. En profondeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b163de86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, box\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def get_netcdf_paths_for_period(dt, base_folder, days, stat_type):\n",
    "    \"\"\"Retourne la liste des fichiers journaliers pour les `days` jours avant dt (inclus).\"\"\"\n",
    "    start_dt = dt - timedelta(days=days)\n",
    "    files = []\n",
    "\n",
    "    for day in (start_dt + timedelta(n) for n in range((dt - start_dt).days + 1)):\n",
    "        year_folder = os.path.join(base_folder, str(day.year))\n",
    "        fname = f\"MARS3D_{day.strftime('%Y%m%d')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Aucun fichier {stat_type} trouvé entre {start_dt} et {dt}\"\n",
    "        )\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_temp_sal_for_poly(poly, depth_sampling_surface, ncdf_path):\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    if not hasattr(ds, \"crs\"):\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    bathy = ds[\"H0\"]\n",
    "    temp = ds[\"TEMP\"]\n",
    "    sal = ds[\"SAL\"]\n",
    "\n",
    "    poly_gs = gpd.GeoSeries([poly], crs=\"EPSG:4326\")\n",
    "    poly_proj = poly_gs.to_crs(bathy.rio.crs).iloc[0]\n",
    "\n",
    "    transform = bathy.rio.transform()\n",
    "    height, width = bathy.shape\n",
    "\n",
    "    coords = []\n",
    "    bathy_vals = []\n",
    "    weights = []\n",
    "\n",
    "    # --- Pixels intersectés par le polygone ---\n",
    "    for j in range(height):\n",
    "        for i in range(width):\n",
    "            x_min, y_max = transform * (i, j)\n",
    "            x_max, y_min = transform * (i + 1, j + 1)\n",
    "            pixel_poly = box(x_min, y_min, x_max, y_max)\n",
    "            intersection = poly_proj.intersection(pixel_poly)\n",
    "            if not intersection.is_empty:\n",
    "                b_val = bathy.values[j, i]\n",
    "                if not np.isnan(b_val) and b_val > 0:\n",
    "                    coords.append((j, i))\n",
    "                    bathy_vals.append(b_val)\n",
    "                    weights.append(intersection.area / pixel_poly.area)\n",
    "\n",
    "    # --- Fallback : 3 pixels les plus proches si aucun pixel intersecté ---\n",
    "    if not bathy_vals:\n",
    "        pixel_distances = []\n",
    "        for j in range(height):\n",
    "            for i in range(width):\n",
    "                b_val = bathy.values[j, i]\n",
    "                if np.isnan(b_val) or b_val <= 0:\n",
    "                    continue\n",
    "                x_c, y_c = transform * (i + 0.5, j + 0.5)\n",
    "                dist = Point(x_c, y_c).distance(poly_proj)\n",
    "                pixel_distances.append((dist, j, i, b_val))\n",
    "\n",
    "        pixel_distances.sort(key=lambda x: x[0])\n",
    "        closest = pixel_distances[:3]\n",
    "        coords = [(j, i) for _, j, i, _ in closest]\n",
    "        bathy_vals = [b for _, _, _, b in closest]\n",
    "        weights = [1.0 for _ in closest]\n",
    "\n",
    "    # --- Calcul de la couche verticale ---\n",
    "    layers_phys = [int(depth_sampling_surface / b * 60) for b in bathy_vals]\n",
    "    layers_phys = [60 - l for l in layers_phys]\n",
    "    layers_index = [max(0, min(l - 1, 59)) for l in layers_phys]\n",
    "\n",
    "    # --- Extraction TEMP/SAL par pixel ---\n",
    "    temp_values, sal_values = [], []\n",
    "    for (j, i), l in zip(coords, layers_index):\n",
    "        if temp.ndim == 4:  # ancien format avec time\n",
    "            t_val = temp.values[0, l, j, i]\n",
    "            s_val = sal.values[0, l, j, i]\n",
    "        else:  # format journalier sans time\n",
    "            t_val = temp.values[l, j, i]\n",
    "            s_val = sal.values[l, j, i]\n",
    "\n",
    "        if not np.isnan(t_val):\n",
    "            temp_values.append(t_val)\n",
    "        if not np.isnan(s_val):\n",
    "            sal_values.append(s_val)\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "    if not temp_values:\n",
    "        return [], [], np.nan, np.nan\n",
    "\n",
    "    temp_values = np.array(temp_values)\n",
    "    sal_values = np.array(sal_values)\n",
    "    weights_arr = np.array(weights)\n",
    "\n",
    "    temp_weighted_mean = np.nansum(temp_values * weights_arr) / np.nansum(weights_arr)\n",
    "    sal_weighted_mean = np.nansum(sal_values * weights_arr) / np.nansum(weights_arr)\n",
    "\n",
    "    return temp_values, sal_values, temp_weighted_mean, sal_weighted_mean\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_folder = # \"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/SAL-TEMP_latlon/Daily/Med-Ouest\"\n",
    "    gdf = gpd.read_file() # Nom de la couche de polygones \n",
    "\n",
    "    temp_max7, temp_min7, temp_mean7 = [], [], []\n",
    "    sal_max7, sal_min7, sal_mean7 = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction 7 jours\"):\n",
    "        dt = row['date'] # Modifier si besoin \n",
    "\n",
    "        try:\n",
    "            # --- MAX du max ---\n",
    "            files_max = get_netcdf_paths_for_period(dt, base_folder, days=7, stat_type=\"max\")\n",
    "            all_temp, all_sal, temp_means, sal_means = [], [], [], []\n",
    "            for f in files_max:\n",
    "                t_vals, s_vals, t_wmean, s_wmean = get_temp_sal_for_poly(row['geometry'], row['depth_sampling_surface'], f)# Modifier si besoin \n",
    "                all_temp.extend(t_vals)\n",
    "                all_sal.extend(s_vals)\n",
    "                if not np.isnan(t_wmean):\n",
    "                    temp_means.append(t_wmean)\n",
    "                if not np.isnan(s_wmean):\n",
    "                    sal_means.append(s_wmean)\n",
    "            temp_max7.append(np.nanmax(all_temp) if all_temp else np.nan)\n",
    "            sal_max7.append(np.nanmax(all_sal) if all_sal else np.nan)\n",
    "\n",
    "            # --- MIN du min ---\n",
    "            files_min = get_netcdf_paths_for_period(dt, base_folder, days=7, stat_type=\"min\")\n",
    "            all_temp, all_sal, temp_means, sal_means = [], [], [], []\n",
    "            for f in files_min:\n",
    "                t_vals, s_vals, t_wmean, s_wmean = get_temp_sal_for_poly(row['geometry'], row['depth_sampling_surface'], f) # Modifier si besoin \n",
    "                all_temp.extend(t_vals)\n",
    "                all_sal.extend(s_vals)\n",
    "                if not np.isnan(t_wmean):\n",
    "                    temp_means.append(t_wmean)\n",
    "                if not np.isnan(s_wmean):\n",
    "                    sal_means.append(s_wmean)\n",
    "            temp_min7.append(np.nanmin(all_temp) if all_temp else np.nan)\n",
    "            sal_min7.append(np.nanmin(all_sal) if all_sal else np.nan)\n",
    "\n",
    "            # --- MOYENNE des mean ---\n",
    "            files_mean = get_netcdf_paths_for_period(dt, base_folder, days=7, stat_type=\"mean\")\n",
    "            all_temp, all_sal, temp_means, sal_means = [], [], [], []\n",
    "            for f in files_mean:\n",
    "                t_vals, s_vals, t_wmean, s_wmean = get_temp_sal_for_poly(row['geometry'], row['depth_sampling_surface'], f) # Modifier si besoin \n",
    "                all_temp.extend(t_vals)\n",
    "                all_sal.extend(s_vals)\n",
    "                if not np.isnan(t_wmean):\n",
    "                    temp_means.append(t_wmean)\n",
    "                if not np.isnan(s_wmean):\n",
    "                    sal_means.append(s_wmean)\n",
    "            temp_mean7.append(np.nanmean(temp_means) if temp_means else np.nan)\n",
    "            sal_mean7.append(np.nanmean(sal_means) if sal_means else np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Fichier manquant pour {dt}: {e}\")\n",
    "            temp_max7.append(np.nan)\n",
    "            temp_min7.append(np.nan)\n",
    "            temp_mean7.append(np.nan)\n",
    "            sal_max7.append(np.nan)\n",
    "            sal_min7.append(np.nan)\n",
    "            sal_mean7.append(np.nan)\n",
    "\n",
    "    gdf['temp_max_7j'] = temp_max7\n",
    "    gdf['temp_min_7j'] = temp_min7\n",
    "    gdf['temp_mean_7j'] = temp_mean7\n",
    "    gdf['sal_max_7j'] = sal_max7\n",
    "    gdf['sal_min_7j'] = sal_min7\n",
    "    gdf['sal_mean_7j'] = sal_mean7\n",
    "\n",
    "    gdf.to_file(\"grille_med_ouest.geojson\", driver=\"GeoJSON\") # Modifier si besoin \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e692324",
   "metadata": {},
   "source": [
    "b. En surface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bad84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def get_netcdf_paths_for_period(dt, base_folder, days, stat_type):\n",
    "    start_dt = dt - timedelta(days=days)\n",
    "    files = []\n",
    "    for day in (start_dt + timedelta(n) for n in range((dt - start_dt).days + 1)):\n",
    "        year_folder = os.path.join(base_folder, str(day.year))\n",
    "        fname = f\"MARS3D_{day.strftime('%Y%m%d')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Aucun fichier {stat_type} trouvé entre {start_dt} et {dt}\")\n",
    "    return files\n",
    "\n",
    "def get_ws_vel_for_poly(poly, ncdf_path):\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "\n",
    "    ws = ds[\"TEMP\"].isel(time=0, level=-1)# à modifier \n",
    "    vel = ds[\"SAL\"].isel(time=0, level=-1)# à modifier \n",
    "\n",
    "    if ws.rio.crs is None:\n",
    "        ws = ws.rio.write_crs(\"EPSG:4326\")\n",
    "    if vel.rio.crs is None:\n",
    "        vel = vel.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    # --- Clip vectorisé ---\n",
    "    ws_clip = ws.rio.clip([poly], all_touched=True, drop=False)\n",
    "    vel_clip = vel.rio.clip([poly], all_touched=True, drop=False)\n",
    "\n",
    "    ws_vals = ws_clip.values.flatten()\n",
    "    ws_vals = ws_vals[~np.isnan(ws_vals)]\n",
    "    vel_vals = vel_clip.values.flatten()\n",
    "    vel_vals = vel_vals[~np.isnan(vel_vals)]\n",
    "\n",
    "    # --- Fallback si aucun pixel intersecté ---\n",
    "    if len(ws_vals) == 0 or len(vel_vals) == 0:\n",
    "        transform = ws.rio.transform()\n",
    "        h, w = ws.shape\n",
    "        xs = np.arange(w) + 0.5\n",
    "        ys = np.arange(h) + 0.5\n",
    "        xv, yv = transform * np.meshgrid(xs, ys)\n",
    "        \n",
    "        if len(ws_vals) == 0:\n",
    "            ws_all = ws.values\n",
    "            valid_idx = ~np.isnan(ws_all)\n",
    "            coords = np.column_stack([xv[valid_idx], yv[valid_idx]])\n",
    "            values = ws_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            ws_vals = values[idx]\n",
    "\n",
    "        if len(vel_vals) == 0:\n",
    "            vel_all = vel.values\n",
    "            valid_idx = ~np.isnan(vel_all)\n",
    "            coords = np.column_stack([xv[valid_idx], yv[valid_idx]])\n",
    "            values = vel_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            vel_vals = values[idx]\n",
    "\n",
    "    ws_mean = np.mean(ws_vals) if len(ws_vals) > 0 else np.nan\n",
    "    vel_mean = np.mean(vel_vals) if len(vel_vals) > 0 else np.nan\n",
    "\n",
    "    ds.close()\n",
    "    return ws_vals, vel_vals, ws_mean, vel_mean\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_folder = #\"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/CUR-WIND_latlon/Daily/Corse2\"\n",
    "    gdf = gpd.read_file() # à modifier \n",
    "\n",
    "    ws_max7, ws_min7, ws_mean7 = [], [], []\n",
    "    vel_max7, vel_min7, vel_mean7 = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction 7 jours WS/VEL pondérée\"):\n",
    "        dt = row[\"date\"]\n",
    "        try:\n",
    "            # MAX sur 7 jours\n",
    "            files_max = get_netcdf_paths_for_period(dt, base_folder, days=7, stat_type=\"max\")\n",
    "            all_ws, all_vel, ws_wmean_list, vel_wmean_list = [], [], [], []\n",
    "            for f in files_max:\n",
    "                ws_vals, vel_vals, ws_wmean, vel_wmean = get_ws_vel_for_poly(row[\"geometry\"], f)\n",
    "                all_ws.extend(ws_vals)\n",
    "                all_vel.extend(vel_vals)\n",
    "                if not np.isnan(ws_wmean):\n",
    "                    ws_wmean_list.append(ws_wmean)\n",
    "                if not np.isnan(vel_wmean):\n",
    "                    vel_wmean_list.append(vel_wmean)\n",
    "            ws_max7.append(np.nanmax(all_ws) if all_ws else np.nan)\n",
    "            vel_max7.append(np.nanmax(all_vel) if all_vel else np.nan)\n",
    "\n",
    "            # MIN sur 7 jours\n",
    "            files_min = get_netcdf_paths_for_period(dt, base_folder, days=7, stat_type=\"min\")\n",
    "            all_ws, all_vel = [], []\n",
    "            for f in files_min:\n",
    "                ws_vals, vel_vals, _, _ = get_ws_vel_for_poly(row[\"geometry\"], f)\n",
    "                all_ws.extend(ws_vals)\n",
    "                all_vel.extend(vel_vals)\n",
    "            ws_min7.append(np.nanmin(all_ws) if all_ws else np.nan)\n",
    "            vel_min7.append(np.nanmin(all_vel) if all_vel else np.nan)\n",
    "\n",
    "            # MOYENNE pondérée sur 7 jours\n",
    "            files_mean = get_netcdf_paths_for_period(dt, base_folder, days=7, stat_type=\"mean\")\n",
    "            ws_wmean_all, vel_wmean_all = [], []\n",
    "            for f in files_mean:\n",
    "                _, _, ws_wmean, vel_wmean = get_ws_vel_for_poly(row[\"geometry\"], f)\n",
    "                if not np.isnan(ws_wmean):\n",
    "                    ws_wmean_all.append(ws_wmean)\n",
    "                if not np.isnan(vel_wmean):\n",
    "                    vel_wmean_all.append(vel_wmean)\n",
    "            ws_mean7.append(np.nanmean(ws_wmean_all) if ws_wmean_all else np.nan)\n",
    "            vel_mean7.append(np.nanmean(vel_wmean_all) if vel_wmean_all else np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"⚠️ Fichiers manquants pour {dt}: {e}\")\n",
    "            ws_max7.append(np.nan)\n",
    "            ws_min7.append(np.nan)\n",
    "            ws_mean7.append(np.nan)\n",
    "            vel_max7.append(np.nan)\n",
    "            vel_min7.append(np.nan)\n",
    "            vel_mean7.append(np.nan)\n",
    "\n",
    "    gdf[\"temp_max_7j\"] = ws_max7\n",
    "    gdf[\"temp_min_7j\"] = ws_min7\n",
    "    gdf[\"temp_mean_7j\"] = ws_mean7\n",
    "    gdf[\"sal_max_7j\"] = vel_max7\n",
    "    gdf[\"sal_min_7j\"] = vel_min7\n",
    "    gdf[\"sal_mean_7j\"] = vel_mean7\n",
    "\n",
    "    gdf.to_file(\"adne_extract_corse.geojson\", driver=\"GeoJSON\")# à modifier \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc9179",
   "metadata": {},
   "source": [
    "#### 3. Extraction 1 mois en amont (en utilisant les données daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e31e7",
   "metadata": {},
   "source": [
    "a. En profondeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abd36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, box\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "def get_netcdf_paths_for_last_month(dt, base_folder, stat_type):\n",
    "    \"\"\"\n",
    "    Retourne la liste des fichiers journaliers pour un mois glissant\n",
    "    allant de (dt - 1 mois) à dt inclus.\n",
    "    \"\"\"\n",
    "    start_dt = dt - relativedelta(months=1)\n",
    "    end_dt = dt\n",
    "\n",
    "    files = []\n",
    "    for day in (start_dt + timedelta(n) for n in range((end_dt - start_dt).days + 1)):\n",
    "        year_folder = os.path.join(base_folder, str(day.year))\n",
    "        fname = f\"MARS3D_{day.strftime('%Y%m%d')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Aucun fichier {stat_type} trouvé entre {start_dt} et {end_dt}\"\n",
    "        )\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_temp_sal_for_poly(poly, depth_sampling_surface, ncdf_path):\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    if not hasattr(ds, 'crs'):\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    bathy = ds['H0']\n",
    "    temp = ds['TEMP']\n",
    "    sal = ds['SAL']\n",
    "\n",
    "    poly_gs = gpd.GeoSeries([poly], crs=\"EPSG:4326\")\n",
    "    poly_proj = poly_gs.to_crs(bathy.rio.crs).iloc[0]\n",
    "\n",
    "    transform = bathy.rio.transform()\n",
    "    height, width = bathy.shape\n",
    "\n",
    "    coords = []\n",
    "    bathy_vals = []\n",
    "    weights = []\n",
    "\n",
    "    # --- Pixels intersectés par le polygone ---\n",
    "    for j in range(height):\n",
    "        for i in range(width):\n",
    "            x_min, y_max = transform * (i, j)\n",
    "            x_max, y_min = transform * (i + 1, j + 1)\n",
    "            pixel_poly = box(x_min, y_min, x_max, y_max)\n",
    "            intersection = poly_proj.intersection(pixel_poly)\n",
    "            if not intersection.is_empty:\n",
    "                b_val = bathy.values[j, i]\n",
    "                if not np.isnan(b_val) and b_val > 0:\n",
    "                    coords.append((j, i))\n",
    "                    bathy_vals.append(b_val)\n",
    "                    # fraction de surface intersectée\n",
    "                    weights.append(intersection.area / pixel_poly.area)\n",
    "\n",
    "    # --- Fallback : 3 pixels les plus proches si aucun pixel intersecté ---\n",
    "    if not bathy_vals:\n",
    "        pixel_distances = []\n",
    "        for j in range(height):\n",
    "            for i in range(width):\n",
    "                b_val = bathy.values[j, i]\n",
    "                if np.isnan(b_val) or b_val <= 0:\n",
    "                    continue\n",
    "                x_c, y_c = transform * (i + 0.5, j + 0.5)\n",
    "                dist = Point(x_c, y_c).distance(poly_proj)\n",
    "                pixel_distances.append((dist, j, i, b_val))\n",
    "\n",
    "        pixel_distances.sort(key=lambda x: x[0])\n",
    "        closest = pixel_distances[:3]\n",
    "        coords = [(j, i) for _, j, i, _ in closest]\n",
    "        bathy_vals = [b for _, _, _, b in closest]\n",
    "        weights = [1.0 for _ in closest]\n",
    "\n",
    "    # --- Calcul de la couche verticale ---\n",
    "    layers_phys = [int(depth_sampling_surface / b * 60) for b in bathy_vals]\n",
    "    layers_phys = [60 - l for l in layers_phys]\n",
    "    layers_index = [max(0, min(l - 1, 59)) for l in layers_phys]\n",
    "\n",
    "    # --- Extraction TEMP/SAL par pixel et moyenne pondérée ---\n",
    "    temp_values, sal_values = [], []\n",
    "    for (j, i), l in zip(coords, layers_index):\n",
    "        if temp.ndim == 4:\n",
    "            t_val = temp.values[0, l, j, i]\n",
    "            s_val = sal.values[0, l, j, i]\n",
    "        else:\n",
    "            t_val = temp.values[l, j, i]\n",
    "            s_val = sal.values[l, j, i]\n",
    "        if not np.isnan(t_val):\n",
    "            temp_values.append(t_val)\n",
    "        if not np.isnan(s_val):\n",
    "            sal_values.append(s_val)\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "    if not temp_values:\n",
    "        return [], [], np.nan, np.nan\n",
    "\n",
    "    temp_values = np.array(temp_values)\n",
    "    sal_values = np.array(sal_values)\n",
    "    weights_arr = np.array(weights)\n",
    "\n",
    "    temp_weighted_mean = np.nansum(temp_values * weights_arr) / np.nansum(weights_arr)\n",
    "    sal_weighted_mean = np.nansum(sal_values * weights_arr) / np.nansum(weights_arr)\n",
    "\n",
    "    return temp_values, sal_values, temp_weighted_mean, sal_weighted_mean\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_folder = # \"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/SAL-TEMP_latlon/Daily/Med-Ouest\"\n",
    "    gdf = gpd.read_file() # Fichier de polygones \n",
    "\n",
    "    temp_max1m, temp_min1m, temp_mean1m = [], [], []\n",
    "    sal_max1m, sal_min1m, sal_mean1m = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction 1 mois glissant\"):\n",
    "        dt = row['date'] # a modifier si besoin\n",
    "\n",
    "        try:\n",
    "            # --- MAX du max ---\n",
    "            files_max = get_netcdf_paths_for_last_month(dt, base_folder, stat_type=\"max\")\n",
    "            all_temp, all_sal, temp_means, sal_means = [], [], [], []\n",
    "            for f in files_max:\n",
    "                t_vals, s_vals, t_wmean, s_wmean = get_temp_sal_for_poly(row['geometry'], row['depth_sampling_surface'], f) # a modifier si besoin\n",
    "                all_temp.extend(t_vals)\n",
    "                all_sal.extend(s_vals)\n",
    "                if not np.isnan(t_wmean):\n",
    "                    temp_means.append(t_wmean)\n",
    "                if not np.isnan(s_wmean):\n",
    "                    sal_means.append(s_wmean)\n",
    "            temp_max1m.append(np.nanmax(all_temp) if all_temp else np.nan)\n",
    "            sal_max1m.append(np.nanmax(all_sal) if all_sal else np.nan)\n",
    "\n",
    "            # --- MIN du min ---\n",
    "            files_min = get_netcdf_paths_for_last_month(dt, base_folder, stat_type=\"min\")\n",
    "            all_temp, all_sal, temp_means, sal_means = [], [], [], []\n",
    "            for f in files_min:\n",
    "                t_vals, s_vals, t_wmean, s_wmean = get_temp_sal_for_poly(row['geometry'], row['depth_sampling_surface'], f) # a modifier si besoin\n",
    "                all_temp.extend(t_vals)\n",
    "                all_sal.extend(s_vals)\n",
    "                if not np.isnan(t_wmean):\n",
    "                    temp_means.append(t_wmean)\n",
    "                if not np.isnan(s_wmean):\n",
    "                    sal_means.append(s_wmean)\n",
    "            temp_min1m.append(np.nanmin(all_temp) if all_temp else np.nan)\n",
    "            sal_min1m.append(np.nanmin(all_sal) if all_sal else np.nan)\n",
    "\n",
    "            # --- MOYENNE des mean ---\n",
    "            files_mean = get_netcdf_paths_for_last_month(dt, base_folder, stat_type=\"mean\")\n",
    "            all_temp, all_sal, temp_means, sal_means = [], [], [], []\n",
    "            for f in files_mean:\n",
    "                t_vals, s_vals, t_wmean, s_wmean = get_temp_sal_for_poly(row['geometry'], row['depth_sampling_surface'], f) # a modifier si besoin\n",
    "                all_temp.extend(t_vals)\n",
    "                all_sal.extend(s_vals)\n",
    "                if not np.isnan(t_wmean):\n",
    "                    temp_means.append(t_wmean)\n",
    "                if not np.isnan(s_wmean):\n",
    "                    sal_means.append(s_wmean)\n",
    "            temp_mean1m.append(np.nanmean(temp_means) if temp_means else np.nan)\n",
    "            sal_mean1m.append(np.nanmean(sal_means) if sal_means else np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Fichier manquant pour {dt}: {e}\")\n",
    "            temp_max1m.append(np.nan)\n",
    "            temp_min1m.append(np.nan)\n",
    "            temp_mean1m.append(np.nan)\n",
    "            sal_max1m.append(np.nan)\n",
    "            sal_min1m.append(np.nan)\n",
    "            sal_mean1m.append(np.nan)\n",
    "\n",
    "    gdf['temp_max_1m'] = temp_max1m\n",
    "    gdf['temp_min_1m'] = temp_min1m\n",
    "    gdf['temp_mean_1m'] = temp_mean1m\n",
    "    gdf['sal_max_1m'] = sal_max1m\n",
    "    gdf['sal_min_1m'] = sal_min1m\n",
    "    gdf['sal_mean_1m'] = sal_mean1m\n",
    "\n",
    "    gdf.to_file(\"grille_med_ouest.geojson\", driver=\"GeoJSON\") # a modifier si besoin\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eef187",
   "metadata": {},
   "source": [
    "b. En surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21615827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def get_netcdf_paths_for_last_month(dt, base_folder, stat_type):\n",
    "    start_dt = dt - relativedelta(months=1)\n",
    "    end_dt = dt\n",
    "\n",
    "    files = []\n",
    "    for day in (start_dt + timedelta(n) for n in range((end_dt - start_dt).days + 1)):\n",
    "        year_folder = os.path.join(base_folder, str(day.year))\n",
    "        fname = f\"MARS3D_{day.strftime('%Y%m%d')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Aucun fichier {stat_type} trouvé entre {start_dt} et {end_dt}\")\n",
    "    return files\n",
    "\n",
    "def get_ws_vel_for_poly(poly, ncdf_path):\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    ws = ds[\"TEMP\"].isel(time=0, level=-1) # à modifier selon les couches de netcdf notamment si pas la couche time enlever time=0\n",
    "    vel = ds[\"SAL\"].isel(time=0, level=-1) # à modifier selon les couches de netcdf notamment si pas la couche time enlever time=0\n",
    "\n",
    "    if ws.rio.crs is None:\n",
    "        ws = ws.rio.write_crs(\"EPSG:4326\")\n",
    "    if vel.rio.crs is None:\n",
    "        vel = vel.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    poly_gs = gpd.GeoSeries([poly], crs=\"EPSG:4326\")\n",
    "    poly_proj = poly_gs.to_crs(ws.rio.crs).iloc[0]\n",
    "\n",
    "    # Clip vectorisé\n",
    "    ws_clip = ws.rio.clip([poly], all_touched=True, drop=False)\n",
    "    vel_clip = vel.rio.clip([poly], all_touched=True, drop=False)\n",
    "\n",
    "    ws_vals = ws_clip.values.flatten()\n",
    "    ws_vals = ws_vals[~np.isnan(ws_vals)]\n",
    "    vel_vals = vel_clip.values.flatten()\n",
    "    vel_vals = vel_vals[~np.isnan(vel_vals)]\n",
    "\n",
    "    # Fallback si aucun pixel intersecté\n",
    "    if len(ws_vals) == 0 or len(vel_vals) == 0:\n",
    "        transform = ws.rio.transform()\n",
    "        h, w = ws.shape\n",
    "        xs = np.arange(w) + 0.5\n",
    "        ys = np.arange(h) + 0.5\n",
    "        xv, yv = transform * np.meshgrid(xs, ys)\n",
    "\n",
    "        if len(ws_vals) == 0:\n",
    "            ws_all = ws.values\n",
    "            valid_idx = ~np.isnan(ws_all)\n",
    "            coords = np.column_stack([xv[valid_idx], yv[valid_idx]])\n",
    "            values = ws_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            ws_vals = values[idx]\n",
    "\n",
    "        if len(vel_vals) == 0:\n",
    "            vel_all = vel.values\n",
    "            valid_idx = ~np.isnan(vel_all)\n",
    "            coords = np.column_stack([xv[valid_idx], yv[valid_idx]])\n",
    "            values = vel_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            vel_vals = values[idx]\n",
    "\n",
    "    ws_mean = np.mean(ws_vals) if len(ws_vals) > 0 else np.nan\n",
    "    vel_mean = np.mean(vel_vals) if len(vel_vals) > 0 else np.nan\n",
    "\n",
    "    ds.close()\n",
    "    return ws_vals, vel_vals, ws_mean, vel_mean\n",
    "\n",
    "def main():\n",
    "    base_folder = # \"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/SAL-TEMP_latlon/Daily/Med-Ouest\"\n",
    "    gdf = gpd.read_file() # à modifier \n",
    "\n",
    "    ws_max, ws_min, ws_mean = [], [], []\n",
    "    vel_max, vel_min, vel_mean = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction 1 mois WS/VEL pondérée\"):\n",
    "        dt = row[\"date\"]# à modifier \n",
    "        try:\n",
    "            # MAX\n",
    "            files_max = get_netcdf_paths_for_last_month(dt, base_folder, stat_type=\"max\")\n",
    "            all_ws, all_vel = [], []\n",
    "            for f in files_max:\n",
    "                ws_vals, vel_vals, _, _ = get_ws_vel_for_poly(row[\"geometry\"], f)\n",
    "                all_ws.extend(ws_vals)\n",
    "                all_vel.extend(vel_vals)\n",
    "            ws_max.append(np.nanmax(all_ws) if all_ws else np.nan)\n",
    "            vel_max.append(np.nanmax(all_vel) if all_vel else np.nan)\n",
    "\n",
    "            # MIN\n",
    "            files_min = get_netcdf_paths_for_last_month(dt, base_folder, stat_type=\"min\")\n",
    "            all_ws, all_vel = [], []\n",
    "            for f in files_min:\n",
    "                ws_vals, vel_vals, _, _ = get_ws_vel_for_poly(row[\"geometry\"], f)\n",
    "                all_ws.extend(ws_vals)\n",
    "                all_vel.extend(vel_vals)\n",
    "            ws_min.append(np.nanmin(all_ws) if all_ws else np.nan)\n",
    "            vel_min.append(np.nanmin(all_vel) if all_vel else np.nan)\n",
    "\n",
    "            # MOYENNE pondérée\n",
    "            files_mean = get_netcdf_paths_for_last_month(dt, base_folder, stat_type=\"mean\")\n",
    "            ws_wmean_all, vel_wmean_all = [], []\n",
    "            for f in files_mean:\n",
    "                _, _, ws_wmean, vel_wmean = get_ws_vel_for_poly(row[\"geometry\"], f)\n",
    "                if not np.isnan(ws_wmean):\n",
    "                    ws_wmean_all.append(ws_wmean)\n",
    "                if not np.isnan(vel_wmean):\n",
    "                    vel_wmean_all.append(vel_wmean)\n",
    "            ws_mean.append(np.nanmean(ws_wmean_all) if ws_wmean_all else np.nan)\n",
    "            vel_mean.append(np.nanmean(vel_wmean_all) if vel_wmean_all else np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"⚠️ Fichier manquant pour {dt}: {e}\")\n",
    "            ws_max.append(np.nan)\n",
    "            ws_min.append(np.nan)\n",
    "            ws_mean.append(np.nan)\n",
    "            vel_max.append(np.nan)\n",
    "            vel_min.append(np.nan)\n",
    "            vel_mean.append(np.nan)\n",
    "\n",
    "    gdf[\"temp_max_1m\"] = ws_max\n",
    "    gdf[\"temp_min_1m\"] = ws_min\n",
    "    gdf[\"temp_mean_1m\"] = ws_mean\n",
    "    gdf[\"sal_max_1m\"] = vel_max\n",
    "    gdf[\"sal_min_1m\"] = vel_min\n",
    "    gdf[\"sal_mean_1m\"] = vel_mean\n",
    "\n",
    "    gdf.to_file(\"grille_med_ouest.geojson\", driver=\"GeoJSON\")# à modifier \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3659a",
   "metadata": {},
   "source": [
    "#### 4. Extraction 1 an en amont (en utilisant les données monthly et daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e9d289",
   "metadata": {},
   "source": [
    "a. En profondeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce328297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, box\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Fonctions utilitaires\n",
    "# -----------------------------\n",
    "\n",
    "def get_monthly_paths(dt_start, dt_end, base_folder, stat_type):\n",
    "    \"\"\"Retourne les fichiers mensuels (min/max/mean) entre dt_start et le mois précédent dt_end.\"\"\"\n",
    "    files = []\n",
    "    months = pd.date_range(start=dt_start, end=dt_end, freq='MS')  # Month Start\n",
    "    for month in months[:-1]:  # tous les mois sauf le dernier\n",
    "        year_folder = os.path.join(base_folder, str(month.year))\n",
    "        fname = f\"MARS3D_{month.strftime('%Y%m')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "    return files\n",
    "\n",
    "def get_daily_paths(dt_start, dt_end, base_folder, stat_type):\n",
    "    \"\"\"Retourne les fichiers journaliers (min/max/mean) entre dt_start et dt_end.\"\"\"\n",
    "    files = []\n",
    "    for day in (dt_start + timedelta(n) for n in range((dt_end - dt_start).days + 1)):\n",
    "        year_folder = os.path.join(base_folder, str(day.year))\n",
    "        fname = f\"MARS3D_{day.strftime('%Y%m%d')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "    return files\n",
    "\n",
    "def get_temp_sal_for_poly(poly, depth_sampling_surface, ncdf_path):\n",
    "    \n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    if not hasattr(ds, 'crs'):\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    bathy = ds['H0']\n",
    "    temp = ds['TEMP']\n",
    "    sal = ds['SAL']\n",
    "\n",
    "    poly_gs = gpd.GeoSeries([poly], crs=\"EPSG:4326\")\n",
    "    poly_proj = poly_gs.to_crs(bathy.rio.crs).iloc[0]\n",
    "\n",
    "    transform = bathy.rio.transform()\n",
    "    height, width = bathy.shape\n",
    "\n",
    "    coords = []\n",
    "    bathy_vals = []\n",
    "    weights = []\n",
    "\n",
    "    # --- Pixels intersectés par le polygone ---\n",
    "    for j in range(height):\n",
    "        for i in range(width):\n",
    "            x_min, y_max = transform * (i, j)\n",
    "            x_max, y_min = transform * (i + 1, j + 1)\n",
    "            pixel_poly = box(x_min, y_min, x_max, y_max)\n",
    "            intersection = poly_proj.intersection(pixel_poly)\n",
    "            if not intersection.is_empty:\n",
    "                b_val = bathy.values[j, i]\n",
    "                if not np.isnan(b_val) and b_val > 0:\n",
    "                    coords.append((j, i))\n",
    "                    bathy_vals.append(b_val)\n",
    "                    weights.append(intersection.area / pixel_poly.area)\n",
    "\n",
    "    # --- Fallback : 3 pixels les plus proches si aucun pixel intersecté ---\n",
    "    if not bathy_vals:\n",
    "        pixel_distances = []\n",
    "        for j in range(height):\n",
    "            for i in range(width):\n",
    "                b_val = bathy.values[j, i]\n",
    "                if np.isnan(b_val) or b_val <= 0:\n",
    "                    continue\n",
    "                x_c, y_c = transform * (i + 0.5, j + 0.5)\n",
    "                dist = Point(x_c, y_c).distance(poly_proj)\n",
    "                pixel_distances.append((dist, j, i, b_val))\n",
    "\n",
    "        pixel_distances.sort(key=lambda x: x[0])\n",
    "        closest = pixel_distances[:3]\n",
    "        coords = [(j, i) for _, j, i, _ in closest]\n",
    "        bathy_vals = [b for _, _, _, b in closest]\n",
    "        weights = [1.0 for _ in closest]\n",
    "\n",
    "    # --- Calcul de la couche verticale ---\n",
    "    layers_phys = [int(depth_sampling_surface / b * 60) for b in bathy_vals]\n",
    "    layers_phys = [60 - l for l in layers_phys]\n",
    "    layers_index = [max(0, min(l - 1, 59)) for l in layers_phys]\n",
    "\n",
    "    # --- Extraction TEMP/SAL par pixel et moyenne pondérée ---\n",
    "    temp_values, sal_values = [], []\n",
    "    for (j, i), l in zip(coords, layers_index):\n",
    "        if temp.ndim == 4:\n",
    "            t_val = temp.values[0, l, j, i]\n",
    "            s_val = sal.values[0, l, j, i]\n",
    "        else:\n",
    "            t_val = temp.values[l, j, i]\n",
    "            s_val = sal.values[l, j, i]\n",
    "        if not np.isnan(t_val):\n",
    "            temp_values.append(t_val)\n",
    "        if not np.isnan(s_val):\n",
    "            sal_values.append(s_val)\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "    if not temp_values:\n",
    "        return [], [], np.nan, np.nan\n",
    "\n",
    "    temp_values = np.array(temp_values)\n",
    "    sal_values = np.array(sal_values)\n",
    "    weights_arr = np.array(weights)\n",
    "\n",
    "    temp_weighted_mean = np.nansum(temp_values * weights_arr) / np.nansum(weights_arr)\n",
    "    sal_weighted_mean = np.nansum(sal_values * weights_arr) / np.nansum(weights_arr)\n",
    "\n",
    "    return temp_values, sal_values, temp_weighted_mean, sal_weighted_mean\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    daily_base_folder = # Fichiers netcdf daily \n",
    "    monthly_base_folder = # Fichiers netcdf monthly \n",
    "    gdf = gpd.read_file() # à modifier \n",
    "\n",
    "    temp_max1y, temp_min1y, temp_mean1y = [], [], []\n",
    "    sal_max1y, sal_min1y, sal_mean1y = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction 1 an pondérée\"):\n",
    "        dt = row['date']\n",
    "        dt_start = dt - timedelta(days=365)\n",
    "        last_month_start = dt.replace(day=1)\n",
    "\n",
    "        try:\n",
    "            for stat in ['max', 'min', 'mean']:\n",
    "                # Fichiers mensuels sauf dernier mois\n",
    "                files_monthly = get_monthly_paths(dt_start, last_month_start, monthly_base_folder, stat)\n",
    "                # Fichiers journaliers du dernier mois\n",
    "                files_daily = get_daily_paths(last_month_start, dt, daily_base_folder, stat)\n",
    "                files = files_monthly + files_daily\n",
    "\n",
    "                all_temp, all_sal, temp_means, sal_means = [], [], [], []\n",
    "                for f in files:\n",
    "                    t_vals, s_vals, t_wmean, s_wmean = get_temp_sal_for_poly(row['geometry'], row['depth_sampling_surface'], f) # à modifier \n",
    "                    all_temp.extend(t_vals)\n",
    "                    all_sal.extend(s_vals)\n",
    "                    if not np.isnan(t_wmean):\n",
    "                        temp_means.append(t_wmean)\n",
    "                    if not np.isnan(s_wmean):\n",
    "                        sal_means.append(s_wmean)\n",
    "\n",
    "                if stat == 'max':\n",
    "                    temp_max1y.append(np.nanmax(all_temp) if all_temp else np.nan)\n",
    "                    sal_max1y.append(np.nanmax(all_sal) if all_sal else np.nan)\n",
    "                elif stat == 'min':\n",
    "                    temp_min1y.append(np.nanmin(all_temp) if all_temp else np.nan)\n",
    "                    sal_min1y.append(np.nanmin(all_sal) if all_sal else np.nan)\n",
    "                elif stat == 'mean':\n",
    "                    temp_mean1y.append(np.nanmean(temp_means) if temp_means else np.nan)\n",
    "                    sal_mean1y.append(np.nanmean(sal_means) if sal_means else np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Fichier manquant pour {dt}: {e}\")\n",
    "            temp_max1y.append(np.nan)\n",
    "            temp_min1y.append(np.nan)\n",
    "            temp_mean1y.append(np.nan)\n",
    "            sal_max1y.append(np.nan)\n",
    "            sal_min1y.append(np.nan)\n",
    "            sal_mean1y.append(np.nan)\n",
    "\n",
    "    gdf['temp_max_1y'] = temp_max1y\n",
    "    gdf['temp_min_1y'] = temp_min1y\n",
    "    gdf['temp_mean_1y'] = temp_mean1y\n",
    "    gdf['sal_max_1y'] = sal_max1y\n",
    "    gdf['sal_min_1y'] = sal_min1y\n",
    "    gdf['sal_mean_1y'] = sal_mean1y\n",
    "\n",
    "    gdf.to_file(\"adne_extract_med_ouest.geojson\", driver=\"GeoJSON\") # à modifier \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b89ac",
   "metadata": {},
   "source": [
    "b. En surface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69db2f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from scipy.spatial import cKDTree\n",
    "import pandas as pd\n",
    "\n",
    "# ===============================================================\n",
    "# FONCTIONS UTILITAIRES\n",
    "# ===============================================================\n",
    "\n",
    "def get_monthly_paths(dt_start, dt_end, base_folder, stat_type):\n",
    "    months = pd.date_range(start=dt_start, end=dt_end, freq='MS')[:-1]\n",
    "    files = []\n",
    "    for month in months:\n",
    "        year_folder = os.path.join(base_folder, str(month.year))\n",
    "        fname = f\"MARS3D_{month.strftime('%Y%m')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "    return files\n",
    "\n",
    "def get_daily_paths(dt_start, dt_end, base_folder, stat_type):\n",
    "    files = []\n",
    "    for day in (dt_start + timedelta(n) for n in range((dt_end - dt_start).days + 1)):\n",
    "        year_folder = os.path.join(base_folder, str(day.year))\n",
    "        fname = f\"MARS3D_{day.strftime('%Y%m%d')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "    return files\n",
    "\n",
    "def get_ws_vel_for_poly(poly, ncdf_path):\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    ws = ds['TEMP'].isel(time=0, level=-1) # à modifier \n",
    "    vel = ds['SAL'].isel(time=0, level=-1) # à modifier \n",
    "\n",
    "    if ws.rio.crs is None:\n",
    "        ws = ws.rio.write_crs(\"EPSG:4326\")\n",
    "    if vel.rio.crs is None:\n",
    "        vel = vel.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    poly_gs = gpd.GeoSeries([poly], crs=\"EPSG:4326\")\n",
    "    poly_proj = poly_gs.to_crs(ws.rio.crs).iloc[0]\n",
    "\n",
    "    # --- Clip vectorisé ---\n",
    "    ws_clip = ws.rio.clip([poly], all_touched=True, drop=False)\n",
    "    vel_clip = vel.rio.clip([poly], all_touched=True, drop=False)\n",
    "\n",
    "    ws_vals = ws_clip.values.flatten()\n",
    "    ws_vals = ws_vals[~np.isnan(ws_vals)]\n",
    "    vel_vals = vel_clip.values.flatten()\n",
    "    vel_vals = vel_vals[~np.isnan(vel_vals)]\n",
    "\n",
    "    # --- Fallback 3 pixels les plus proches si aucun pixel ---\n",
    "    if len(ws_vals) == 0 or len(vel_vals) == 0:\n",
    "        transform = ws.rio.transform()\n",
    "        h, w = ws.shape\n",
    "        xs = np.arange(w) + 0.5\n",
    "        ys = np.arange(h) + 0.5\n",
    "        xv, yv = transform * np.meshgrid(xs, ys)\n",
    "\n",
    "        if len(ws_vals) == 0:\n",
    "            ws_all = ws.values\n",
    "            valid_idx = ~np.isnan(ws_all)\n",
    "            coords = np.column_stack([xv[valid_idx], yv[valid_idx]])\n",
    "            values = ws_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            ws_vals = values[idx]\n",
    "\n",
    "        if len(vel_vals) == 0:\n",
    "            vel_all = vel.values\n",
    "            valid_idx = ~np.isnan(vel_all)\n",
    "            coords = np.column_stack([xv[valid_idx], yv[valid_idx]])\n",
    "            values = vel_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            vel_vals = values[idx]\n",
    "\n",
    "    ds.close()\n",
    "    return ws_vals, vel_vals\n",
    "\n",
    "# ===============================================================\n",
    "# SCRIPT PRINCIPAL\n",
    "# ===============================================================\n",
    "\n",
    "def main():\n",
    "    daily_base_folder = #\"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/CUR-WIND_latlon/Daily/Corse2\"\n",
    "    monthly_base_folder = #\"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/CUR-WIND_latlon/Monthly/Corse2\"\n",
    "\n",
    "    gdf = gpd.read_file() # à modifier \n",
    "\n",
    "    ws_max, ws_min, ws_mean = [], [], []\n",
    "    vel_max, vel_min, vel_mean = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction 1 an WS/VEL pondérée\"):\n",
    "        dt = row['date']\n",
    "        dt_start = dt - timedelta(days=365)\n",
    "        last_month_start = dt.replace(day=1)\n",
    "\n",
    "        try:\n",
    "            for stat in ['max', 'min', 'mean']:\n",
    "                files_monthly = get_monthly_paths(dt_start, last_month_start, monthly_base_folder, stat)\n",
    "                files_daily = get_daily_paths(last_month_start, dt, daily_base_folder, stat)\n",
    "                files = files_monthly + files_daily\n",
    "\n",
    "                all_ws, all_vel = [], []\n",
    "                for f in files:\n",
    "                    ws_vals, vel_vals = get_ws_vel_for_poly(row['geometry'], f)\n",
    "                    all_ws.extend(ws_vals)\n",
    "                    all_vel.extend(vel_vals)\n",
    "\n",
    "                if stat == 'max':\n",
    "                    ws_max.append(np.nanmax(all_ws) if all_ws else np.nan)\n",
    "                    vel_max.append(np.nanmax(all_vel) if all_vel else np.nan)\n",
    "                elif stat == 'min':\n",
    "                    ws_min.append(np.nanmin(all_ws) if all_ws else np.nan)\n",
    "                    vel_min.append(np.nanmin(all_vel) if all_vel else np.nan)\n",
    "                elif stat == 'mean':\n",
    "                    ws_mean.append(np.nanmean(all_ws) if all_ws else np.nan)\n",
    "                    vel_mean.append(np.nanmean(all_vel) if all_vel else np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"⚠️ Fichier manquant pour {dt} : {e}\")\n",
    "            ws_max.append(np.nan)\n",
    "            ws_min.append(np.nan)\n",
    "            ws_mean.append(np.nan)\n",
    "            vel_max.append(np.nan)\n",
    "            vel_min.append(np.nan)\n",
    "            vel_mean.append(np.nan)\n",
    "\n",
    "    gdf['temp_max_1y'] = ws_max\n",
    "    gdf['temp_min_1y'] = ws_min\n",
    "    gdf['temp_mean_1y'] = ws_mean\n",
    "    gdf['sal_max_1y'] = vel_max\n",
    "    gdf['sal_min_1y'] = vel_min\n",
    "    gdf['sal_mean_1y'] = vel_mean\n",
    "\n",
    "    gdf.to_file(\"adne_extract_corse.geojson\", driver=\"GeoJSON\") # à modifier \n",
    "\n",
    "# ===============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e74fb3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5. Extraction 1 mois en amont (en utilisant les données monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0330f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, box\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. Récupération du fichier mensuel du mois précédent\n",
    "# -------------------------------------------------------------\n",
    "def get_monthly_netcdf_path(dt, base_folder, stat_type):\n",
    "    \"\"\"\n",
    "    Retourne le fichier mensuel correspondant au mois précédent.\n",
    "    Exemple : dt = 2019-07-01 → récupère fichier de juin 2019.\n",
    "    \"\"\"\n",
    "    prev_month = dt - relativedelta(months=1)\n",
    "    yyyymm = prev_month.strftime(\"%Y%m\")\n",
    "\n",
    "    year_folder = os.path.join(base_folder, str(prev_month.year))\n",
    "    fname = f\"MARS3D_{yyyymm}_{stat_type}.nc\"\n",
    "    fpath = os.path.join(year_folder, fname)\n",
    "\n",
    "    if not os.path.exists(fpath):\n",
    "        raise FileNotFoundError(f\"Fichier introuvable : {fpath}\")\n",
    "\n",
    "    return fpath\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2. Extraction TEMP/SAL pour un polygone et un NetCDF mensuel\n",
    "# -------------------------------------------------------------\n",
    "def get_temp_sal_for_poly(poly, depth_sampling_surface, ncdf_path):\n",
    "    \"\"\"\n",
    "    Extraction TEMP/SAL pour un polygone et un fichier NetCDF mensuel.\n",
    "    Moyenne pondérée par fraction de pixel intersecté.\n",
    "    \"\"\"\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    if not hasattr(ds, 'crs'):\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    bathy = ds['H0']\n",
    "    temp = ds['TEMP']\n",
    "    sal = ds['SAL']\n",
    "\n",
    "    poly_gs = gpd.GeoSeries([poly], crs=\"EPSG:4326\")\n",
    "    poly_proj = poly_gs.to_crs(bathy.rio.crs).iloc[0]\n",
    "\n",
    "    transform = bathy.rio.transform()\n",
    "    height, width = bathy.shape\n",
    "\n",
    "    coords = []\n",
    "    bathy_vals = []\n",
    "    weights = []\n",
    "\n",
    "    # ---- Recherche des pixels intersectés ----\n",
    "    for j in range(height):\n",
    "        for i in range(width):\n",
    "            x_min, y_max = transform * (i, j)\n",
    "            x_max, y_min = transform * (i + 1, j + 1)\n",
    "            pixel_poly = box(x_min, y_min, x_max, y_max)\n",
    "\n",
    "            intersection = poly_proj.intersection(pixel_poly)\n",
    "            if not intersection.is_empty:\n",
    "                b_val = bathy.values[j, i]\n",
    "                if not np.isnan(b_val) and b_val > 0:\n",
    "                    coords.append((j, i))\n",
    "                    bathy_vals.append(b_val)\n",
    "                    weights.append(intersection.area / pixel_poly.area)\n",
    "\n",
    "    # ---- Fallback si aucun pixel intersecté : prendre les 3 plus proches ----\n",
    "    if not bathy_vals:\n",
    "        pixel_distances = []\n",
    "        for j in range(height):\n",
    "            for i in range(width):\n",
    "                b_val = bathy.values[j, i]\n",
    "                if np.isnan(b_val) or b_val <= 0:\n",
    "                    continue\n",
    "\n",
    "                x_c, y_c = transform * (i + 0.5, j + 0.5)\n",
    "                dist = Point(x_c, y_c).distance(poly_proj)\n",
    "                pixel_distances.append((dist, j, i, b_val))\n",
    "\n",
    "        pixel_distances.sort(key=lambda x: x[0])\n",
    "        closest = pixel_distances[:3]\n",
    "        coords = [(j, i) for _, j, i, _ in closest]\n",
    "        bathy_vals = [b for _, _, _, b in closest]\n",
    "        weights = [1.0] * len(coords)\n",
    "\n",
    "    # ---- Détermination de la couche verticale ----\n",
    "    layers_phys = [int(depth_sampling_surface / b * 60) for b in bathy_vals]\n",
    "    layers_phys = [60 - l for l in layers_phys]\n",
    "    layers_index = [max(0, min(l - 1, 59)) for l in layers_phys]\n",
    "\n",
    "    # ---- Extraction TEMP/SAL pixel par pixel ----\n",
    "    temp_values, sal_values = [], []\n",
    "    for (j, i), l in zip(coords, layers_index):\n",
    "        if temp.ndim == 4:\n",
    "            t_val = temp.values[0, l, j, i]\n",
    "            s_val = sal.values[0, l, j, i]\n",
    "        else:\n",
    "            t_val = temp.values[l, j, i]\n",
    "            s_val = sal.values[l, j, i]\n",
    "\n",
    "        if not np.isnan(t_val):\n",
    "            temp_values.append(t_val)\n",
    "        if not np.isnan(s_val):\n",
    "            sal_values.append(s_val)\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "    if not temp_values:\n",
    "        return [], [], np.nan, np.nan\n",
    "\n",
    "    temp_values = np.array(temp_values)\n",
    "    sal_values = np.array(sal_values)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    temp_mean = np.nansum(temp_values * weights) / np.nansum(weights)\n",
    "    sal_mean = np.nansum(sal_values * weights) / np.nansum(weights)\n",
    "\n",
    "    return temp_values, sal_values, temp_mean, sal_mean\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. Programme principal adapté aux fichiers mensuels\n",
    "# -------------------------------------------------------------\n",
    "def main():\n",
    "    base_folder = # à modifier \"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/SAL-TEMP_latlon/Monthly/Med-Ouest\" \n",
    "    gdf = gpd.read_file() # fichier de polygones \n",
    "\n",
    "    temp_max1m, temp_min1m, temp_mean1m = [], [], []\n",
    "    sal_max1m, sal_min1m, sal_mean1m = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction mensuelle\"):\n",
    "        dt = row['date']\n",
    "\n",
    "        try:\n",
    "            # ---- MAX ----\n",
    "            f_max = get_monthly_netcdf_path(dt, base_folder, \"max\")\n",
    "            t_vals, s_vals, _, _ = get_temp_sal_for_poly(row['geometry'], row['depth_sampling_surface'], f_max) # à modifier \n",
    "            temp_max1m.append(np.nanmax(t_vals) if len(t_vals) else np.nan)\n",
    "            sal_max1m.append(np.nanmax(s_vals) if len(s_vals) else np.nan)\n",
    "\n",
    "            # ---- MIN ----\n",
    "            f_min = get_monthly_netcdf_path(dt, base_folder, \"min\")\n",
    "            t_vals, s_vals, _, _ = get_temp_sal_for_poly(row['geometry'], row['depth_sampling_surface'], f_min) # à modifier \n",
    "            temp_min1m.append(np.nanmin(t_vals) if len(t_vals) else np.nan)\n",
    "            sal_min1m.append(np.nanmin(s_vals) if len(s_vals) else np.nan)\n",
    "\n",
    "            # ---- MEAN ----\n",
    "            f_mean = get_monthly_netcdf_path(dt, base_folder, \"mean\")\n",
    "            _, _, t_mean, s_mean = get_temp_sal_for_poly(row['geometry'], row['depth_sampling_surface'], f_mean) # à modifier \n",
    "            temp_mean1m.append(t_mean)\n",
    "            sal_mean1m.append(s_mean)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"⚠️ Fichier manquant pour {dt}: {e}\")\n",
    "            temp_max1m.append(np.nan)\n",
    "            temp_min1m.append(np.nan)\n",
    "            temp_mean1m.append(np.nan)\n",
    "            sal_max1m.append(np.nan)\n",
    "            sal_min1m.append(np.nan)\n",
    "            sal_mean1m.append(np.nan)\n",
    "\n",
    "    # ---- Sauvegarde ----\n",
    "    gdf['temp_max'] = temp_max1m\n",
    "    gdf['temp_min'] = temp_min1m\n",
    "    gdf['temp_mean'] = temp_mean1m\n",
    "    gdf['sal_max'] = sal_max1m\n",
    "    gdf['sal_min'] = sal_min1m\n",
    "    gdf['sal_mean'] = sal_mean1m\n",
    "\n",
    "    gdf.to_file(\"grille_med_ouest.geojson\", driver=\"GeoJSON\") # à modifier \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152198f6",
   "metadata": {},
   "source": [
    "## Extraction des variables vélocité et courant en surface à partir des données MARS 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da09ab6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1. Extraction 24h en amont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import box\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def get_netcdf_paths_for_period(dt, base_folder, hours=24):\n",
    "    start_dt = dt - timedelta(hours=hours)\n",
    "    files = []\n",
    "\n",
    "    for year in range(start_dt.year, dt.year + 1):\n",
    "        year_folder = os.path.join(base_folder, str(year))\n",
    "        pattern = os.path.join(year_folder, \"MARC_F2-MARS3D-MENOR1200_????????T????Z.nc\")\n",
    "        candidates = glob.glob(pattern)\n",
    "\n",
    "        def extract_datetime_from_filename(f):\n",
    "            match = re.search(r\"_(\\d{8}T\\d{4})Z\\.nc$\", f)\n",
    "            if not match:\n",
    "                return None\n",
    "            return datetime.strptime(match.group(1), \"%Y%m%dT%H%M\")\n",
    "\n",
    "        for f in candidates:\n",
    "            f_dt = extract_datetime_from_filename(f)\n",
    "            if f_dt and start_dt <= f_dt <= dt:\n",
    "                files.append((f, f_dt))\n",
    "\n",
    "    files.sort(key=lambda x: x[1])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Aucun fichier trouvé entre {start_dt} et {dt}\")\n",
    "\n",
    "    return [f for f, _ in files]\n",
    "\n",
    "\n",
    "def get_ws_vel_for_poly(poly, ncdf_path, depth_index=0):\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    if not hasattr(ds, \"crs\"):\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    ws = ds[\"WINDSTRESS\"]\n",
    "    vel = ds[\"VELOCITY\"]\n",
    "\n",
    "    # Clip raster avec polygone (vectorisé, rapide)\n",
    "    ws_clip = ws.rio.clip([poly], all_touched=True, drop=False)\n",
    "    vel_clip = vel.rio.clip([poly], all_touched=True, drop=False)\n",
    "\n",
    "    # Extraction des valeurs non-NaN\n",
    "    ws_vals = ws_clip.values[0].flatten()\n",
    "    ws_vals = ws_vals[~np.isnan(ws_vals)]\n",
    "\n",
    "    vel_vals = vel_clip.values[0, depth_index].flatten()\n",
    "    vel_vals = vel_vals[~np.isnan(vel_vals)]\n",
    "\n",
    "    # Fallback si aucun pixel intersecté\n",
    "    if len(ws_vals) == 0 or len(vel_vals) == 0:\n",
    "        # Récupère les centres de pixels non-NaN pour fallback\n",
    "        transform = ws.rio.transform()\n",
    "        height, width = ws.shape[1:]\n",
    "        xs = np.arange(width) + 0.5\n",
    "        ys = np.arange(height) + 0.5\n",
    "        xv, yv = np.meshgrid(xs, ys)\n",
    "        x_coords, y_coords = transform * (xv, yv)\n",
    "\n",
    "        # WINDSTRESS fallback\n",
    "        if len(ws_vals) == 0:\n",
    "            ws_all = ws.values[0]\n",
    "            valid_idx = ~np.isnan(ws_all)\n",
    "            coords = np.column_stack([x_coords[valid_idx], y_coords[valid_idx]])\n",
    "            values = ws_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            ws_vals = values[idx]\n",
    "\n",
    "        # VELOCITY fallback\n",
    "        if len(vel_vals) == 0:\n",
    "            vel_all = vel.values[0, depth_index]\n",
    "            valid_idx = ~np.isnan(vel_all)\n",
    "            coords = np.column_stack([x_coords[valid_idx], y_coords[valid_idx]])\n",
    "            values = vel_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            vel_vals = values[idx]\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "    ws_mean = np.mean(ws_vals) if len(ws_vals) > 0 else np.nan\n",
    "    vel_mean = np.mean(vel_vals) if len(vel_vals) > 0 else np.nan\n",
    "\n",
    "    return ws_vals, vel_vals, ws_mean, vel_mean\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_folder = #/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/CUR-WIND_latlon/3H\"\n",
    "    gdf = gpd.read_file()# à modifier \n",
    "\n",
    "    ws_min, ws_max, ws_mean = [], [], []\n",
    "    vel_min, vel_max, vel_mean = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction WINDSTRESS & VELOCITY sur 24h\"):\n",
    "        dt = row[\"datetime\"]\n",
    "        try:\n",
    "            files = get_netcdf_paths_for_period(dt, base_folder, hours=24)\n",
    "            all_ws, all_vel = [], []\n",
    "            weighted_ws, weighted_vel = [], []\n",
    "\n",
    "            for f in files:\n",
    "                ws_vals, vel_vals, ws_wmean, vel_wmean = get_ws_vel_for_poly(row[\"geometry\"], f)\n",
    "                all_ws.extend(ws_vals)\n",
    "                all_vel.extend(vel_vals)\n",
    "                if not np.isnan(ws_wmean):\n",
    "                    weighted_ws.append(ws_wmean)\n",
    "                if not np.isnan(vel_wmean):\n",
    "                    weighted_vel.append(vel_wmean)\n",
    "\n",
    "            ws_min.append(np.nanmin(all_ws) if all_ws else np.nan)\n",
    "            ws_max.append(np.nanmax(all_ws) if all_ws else np.nan)\n",
    "            ws_mean.append(np.nanmean(weighted_ws) if weighted_ws else np.nan)\n",
    "\n",
    "            vel_min.append(np.nanmin(all_vel) if all_vel else np.nan)\n",
    "            vel_max.append(np.nanmax(all_vel) if all_vel else np.nan)\n",
    "            vel_mean.append(np.nanmean(weighted_vel) if weighted_vel else np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"⚠️ Fichier manquant pour {dt}: {e}\")\n",
    "            ws_min.append(np.nan)\n",
    "            ws_max.append(np.nan)\n",
    "            ws_mean.append(np.nan)\n",
    "            vel_min.append(np.nan)\n",
    "            vel_max.append(np.nan)\n",
    "            vel_mean.append(np.nan)\n",
    "\n",
    "    gdf[\"wind_min_24h\"] = ws_min\n",
    "    gdf[\"wind_max_24h\"] = ws_max\n",
    "    gdf[\"wind_mean_24h\"] = ws_mean\n",
    "    gdf[\"vel_min_24h\"] = vel_min\n",
    "    gdf[\"vel_max_24h\"] = vel_max\n",
    "    gdf[\"vel_mean_24h\"] = vel_mean\n",
    "\n",
    "    gdf.to_file(\"adne_extract_corse.geojson\", driver=\"GeoJSON\")# à modifier \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0737904",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2. Extraction 7 jours en amont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26582be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def get_netcdf_paths_for_period(dt, base_folder, days, stat_type):\n",
    "    start_dt = dt - timedelta(days=days)\n",
    "    files = []\n",
    "    for day in (start_dt + timedelta(n) for n in range((dt - start_dt).days + 1)):\n",
    "        year_folder = os.path.join(base_folder, str(day.year))\n",
    "        fname = f\"MARS3D_{day.strftime('%Y%m%d')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Aucun fichier {stat_type} trouvé entre {start_dt} et {dt}\")\n",
    "    return files\n",
    "\n",
    "def get_ws_vel_for_poly(poly, ncdf_path):\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "\n",
    "    ws = ds[\"WINDSTRESS\"].isel(time=0)\n",
    "    vel = ds[\"VELOCITY\"].isel(time=0, level=-1)\n",
    "\n",
    "    if ws.rio.crs is None:\n",
    "        ws = ws.rio.write_crs(\"EPSG:4326\")\n",
    "    if vel.rio.crs is None:\n",
    "        vel = vel.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    # Clip vectorisé\n",
    "    ws_clip = ws.rio.clip([poly], all_touched=True, drop=False)\n",
    "    vel_clip = vel.rio.clip([poly], all_touched=True, drop=False)\n",
    "\n",
    "    ws_vals = ws_clip.values.flatten()\n",
    "    ws_vals = ws_vals[~np.isnan(ws_vals)]\n",
    "    vel_vals = vel_clip.values.flatten()\n",
    "    vel_vals = vel_vals[~np.isnan(vel_vals)]\n",
    "\n",
    "    # Fallback si aucun pixel intersecté\n",
    "    if len(ws_vals) == 0 or len(vel_vals) == 0:\n",
    "        transform = ws.rio.transform()\n",
    "        h, w = ws.shape\n",
    "        xs = np.arange(w) + 0.5\n",
    "        ys = np.arange(h) + 0.5\n",
    "        xv, yv = transform * np.meshgrid(xs, ys)\n",
    "        \n",
    "        if len(ws_vals) == 0:\n",
    "            ws_all = ws.values\n",
    "            valid_idx = ~np.isnan(ws_all)\n",
    "            coords = np.column_stack([xv[valid_idx], yv[valid_idx]])\n",
    "            values = ws_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            ws_vals = values[idx]\n",
    "\n",
    "        if len(vel_vals) == 0:\n",
    "            vel_all = vel.values\n",
    "            valid_idx = ~np.isnan(vel_all)\n",
    "            coords = np.column_stack([xv[valid_idx], yv[valid_idx]])\n",
    "            values = vel_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            vel_vals = values[idx]\n",
    "\n",
    "    ws_mean = np.mean(ws_vals) if len(ws_vals) > 0 else np.nan\n",
    "    vel_mean = np.mean(vel_vals) if len(vel_vals) > 0 else np.nan\n",
    "\n",
    "    ds.close()\n",
    "    return ws_vals, vel_vals, ws_mean, vel_mean\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_folder = #\"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/CUR-WIND_latlon/Daily/Corse2\"\n",
    "    gdf = gpd.read_file()# à modifier \n",
    "\n",
    "    ws_max7, ws_min7, ws_mean7 = [], [], []\n",
    "    vel_max7, vel_min7, vel_mean7 = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction 7 jours WS/VEL pondérée\"):\n",
    "        dt = row[\"date\"]\n",
    "        try:\n",
    "            # MAX sur 7 jours\n",
    "            files_max = get_netcdf_paths_for_period(dt, base_folder, days=7, stat_type=\"max\")\n",
    "            all_ws, all_vel, ws_wmean_list, vel_wmean_list = [], [], [], []\n",
    "            for f in files_max:\n",
    "                ws_vals, vel_vals, ws_wmean, vel_wmean = get_ws_vel_for_poly(row[\"geometry\"], f)\n",
    "                all_ws.extend(ws_vals)\n",
    "                all_vel.extend(vel_vals)\n",
    "                if not np.isnan(ws_wmean):\n",
    "                    ws_wmean_list.append(ws_wmean)\n",
    "                if not np.isnan(vel_wmean):\n",
    "                    vel_wmean_list.append(vel_wmean)\n",
    "            ws_max7.append(np.nanmax(all_ws) if all_ws else np.nan)\n",
    "            vel_max7.append(np.nanmax(all_vel) if all_vel else np.nan)\n",
    "\n",
    "            # MIN sur 7 jours\n",
    "            files_min = get_netcdf_paths_for_period(dt, base_folder, days=7, stat_type=\"min\")\n",
    "            all_ws, all_vel = [], []\n",
    "            for f in files_min:\n",
    "                ws_vals, vel_vals, _, _ = get_ws_vel_for_poly(row[\"geometry\"], f)\n",
    "                all_ws.extend(ws_vals)\n",
    "                all_vel.extend(vel_vals)\n",
    "            ws_min7.append(np.nanmin(all_ws) if all_ws else np.nan)\n",
    "            vel_min7.append(np.nanmin(all_vel) if all_vel else np.nan)\n",
    "\n",
    "            # MOYENNE pondérée sur 7 jours\n",
    "            files_mean = get_netcdf_paths_for_period(dt, base_folder, days=7, stat_type=\"mean\")\n",
    "            ws_wmean_all, vel_wmean_all = [], []\n",
    "            for f in files_mean:\n",
    "                _, _, ws_wmean, vel_wmean = get_ws_vel_for_poly(row[\"geometry\"], f)\n",
    "                if not np.isnan(ws_wmean):\n",
    "                    ws_wmean_all.append(ws_wmean)\n",
    "                if not np.isnan(vel_wmean):\n",
    "                    vel_wmean_all.append(vel_wmean)\n",
    "            ws_mean7.append(np.nanmean(ws_wmean_all) if ws_wmean_all else np.nan)\n",
    "            vel_mean7.append(np.nanmean(vel_wmean_all) if vel_wmean_all else np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"⚠️ Fichiers manquants pour {dt}: {e}\")\n",
    "            ws_max7.append(np.nan)\n",
    "            ws_min7.append(np.nan)\n",
    "            ws_mean7.append(np.nan)\n",
    "            vel_max7.append(np.nan)\n",
    "            vel_min7.append(np.nan)\n",
    "            vel_mean7.append(np.nan)\n",
    "\n",
    "    gdf[\"wind_max_7j\"] = ws_max7\n",
    "    gdf[\"wind_min_7j\"] = ws_min7\n",
    "    gdf[\"wind_mean_7j\"] = ws_mean7\n",
    "    gdf[\"vel_max_7j\"] = vel_max7\n",
    "    gdf[\"vel_min_7j\"] = vel_min7\n",
    "    gdf[\"vel_mean_7j\"] = vel_mean7\n",
    "\n",
    "    gdf.to_file(\"adne_extract_corse.geojson\", driver=\"GeoJSON\") # à modifier \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3072f39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3. Extraction 1 mois en amont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1747a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def get_netcdf_paths_for_last_month(dt, base_folder, stat_type):\n",
    "    start_dt = dt - relativedelta(months=1)\n",
    "    end_dt = dt\n",
    "\n",
    "    files = []\n",
    "    for day in (start_dt + timedelta(n) for n in range((end_dt - start_dt).days + 1)):\n",
    "        year_folder = os.path.join(base_folder, str(day.year))\n",
    "        fname = f\"MARS3D_{day.strftime('%Y%m%d')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Aucun fichier {stat_type} trouvé entre {start_dt} et {end_dt}\")\n",
    "    return files\n",
    "\n",
    "def get_ws_vel_for_poly(poly, ncdf_path):\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    ws = ds[\"WINDSTRESS\"].isel(time=0)\n",
    "    vel = ds[\"VELOCITY\"].isel(time=0, level=-1)\n",
    "\n",
    "    if ws.rio.crs is None:\n",
    "        ws = ws.rio.write_crs(\"EPSG:4326\")\n",
    "    if vel.rio.crs is None:\n",
    "        vel = vel.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    poly_gs = gpd.GeoSeries([poly], crs=\"EPSG:4326\")\n",
    "    poly_proj = poly_gs.to_crs(ws.rio.crs).iloc[0]\n",
    "\n",
    "    # Clip vectorisé\n",
    "    ws_clip = ws.rio.clip([poly], all_touched=True, drop=False)\n",
    "    vel_clip = vel.rio.clip([poly], all_touched=True, drop=False)\n",
    "\n",
    "    ws_vals = ws_clip.values.flatten()\n",
    "    ws_vals = ws_vals[~np.isnan(ws_vals)]\n",
    "    vel_vals = vel_clip.values.flatten()\n",
    "    vel_vals = vel_vals[~np.isnan(vel_vals)]\n",
    "\n",
    "    # Fallback si aucun pixel intersecté\n",
    "    if len(ws_vals) == 0 or len(vel_vals) == 0:\n",
    "        transform = ws.rio.transform()\n",
    "        h, w = ws.shape\n",
    "        xs = np.arange(w) + 0.5\n",
    "        ys = np.arange(h) + 0.5\n",
    "        xv, yv = transform * np.meshgrid(xs, ys)\n",
    "\n",
    "        if len(ws_vals) == 0:\n",
    "            ws_all = ws.values\n",
    "            valid_idx = ~np.isnan(ws_all)\n",
    "            coords = np.column_stack([xv[valid_idx], yv[valid_idx]])\n",
    "            values = ws_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            ws_vals = values[idx]\n",
    "\n",
    "        if len(vel_vals) == 0:\n",
    "            vel_all = vel.values\n",
    "            valid_idx = ~np.isnan(vel_all)\n",
    "            coords = np.column_stack([xv[valid_idx], yv[valid_idx]])\n",
    "            values = vel_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            vel_vals = values[idx]\n",
    "\n",
    "    ws_mean = np.mean(ws_vals) if len(ws_vals) > 0 else np.nan\n",
    "    vel_mean = np.mean(vel_vals) if len(vel_vals) > 0 else np.nan\n",
    "\n",
    "    ds.close()\n",
    "    return ws_vals, vel_vals, ws_mean, vel_mean\n",
    "\n",
    "def main():\n",
    "    base_folder = #\"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/CUR-WIND_latlon/Daily/Corse2\"\n",
    "    gdf = gpd.read_file()# à modifier \n",
    "\n",
    "    ws_max, ws_min, ws_mean = [], [], []\n",
    "    vel_max, vel_min, vel_mean = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction 1 mois WS/VEL pondérée\"):\n",
    "        dt = row[\"date\"]\n",
    "        try:\n",
    "            # MAX\n",
    "            files_max = get_netcdf_paths_for_last_month(dt, base_folder, stat_type=\"max\")\n",
    "            all_ws, all_vel = [], []\n",
    "            for f in files_max:\n",
    "                ws_vals, vel_vals, _, _ = get_ws_vel_for_poly(row[\"geometry\"], f)\n",
    "                all_ws.extend(ws_vals)\n",
    "                all_vel.extend(vel_vals)\n",
    "            ws_max.append(np.nanmax(all_ws) if all_ws else np.nan)\n",
    "            vel_max.append(np.nanmax(all_vel) if all_vel else np.nan)\n",
    "\n",
    "            # MIN\n",
    "            files_min = get_netcdf_paths_for_last_month(dt, base_folder, stat_type=\"min\")\n",
    "            all_ws, all_vel = [], []\n",
    "            for f in files_min:\n",
    "                ws_vals, vel_vals, _, _ = get_ws_vel_for_poly(row[\"geometry\"], f)\n",
    "                all_ws.extend(ws_vals)\n",
    "                all_vel.extend(vel_vals)\n",
    "            ws_min.append(np.nanmin(all_ws) if all_ws else np.nan)\n",
    "            vel_min.append(np.nanmin(all_vel) if all_vel else np.nan)\n",
    "\n",
    "            # MOYENNE pondérée\n",
    "            files_mean = get_netcdf_paths_for_last_month(dt, base_folder, stat_type=\"mean\")\n",
    "            ws_wmean_all, vel_wmean_all = [], []\n",
    "            for f in files_mean:\n",
    "                _, _, ws_wmean, vel_wmean = get_ws_vel_for_poly(row[\"geometry\"], f)\n",
    "                if not np.isnan(ws_wmean):\n",
    "                    ws_wmean_all.append(ws_wmean)\n",
    "                if not np.isnan(vel_wmean):\n",
    "                    vel_wmean_all.append(vel_wmean)\n",
    "            ws_mean.append(np.nanmean(ws_wmean_all) if ws_wmean_all else np.nan)\n",
    "            vel_mean.append(np.nanmean(vel_wmean_all) if vel_wmean_all else np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"⚠️ Fichier manquant pour {dt}: {e}\")\n",
    "            ws_max.append(np.nan)\n",
    "            ws_min.append(np.nan)\n",
    "            ws_mean.append(np.nan)\n",
    "            vel_max.append(np.nan)\n",
    "            vel_min.append(np.nan)\n",
    "            vel_mean.append(np.nan)\n",
    "\n",
    "    gdf[\"wind_max_1m\"] = ws_max\n",
    "    gdf[\"wind_min_1m\"] = ws_min\n",
    "    gdf[\"wind_mean_1m\"] = ws_mean\n",
    "    gdf[\"vel_max_1m\"] = vel_max\n",
    "    gdf[\"vel_min_1m\"] = vel_min\n",
    "    gdf[\"vel_mean_1m\"] = vel_mean\n",
    "\n",
    "    gdf.to_file(\"adne_extract_corse.geojson\", driver=\"GeoJSON\")# à modifier \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5043e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4. Extraction 1 an en amont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d2ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from scipy.spatial import cKDTree\n",
    "import pandas as pd\n",
    "\n",
    "# ===============================================================\n",
    "# FONCTIONS UTILITAIRES\n",
    "# ===============================================================\n",
    "\n",
    "def get_monthly_paths(dt_start, dt_end, base_folder, stat_type):\n",
    "    months = pd.date_range(start=dt_start, end=dt_end, freq='MS')[:-1]\n",
    "    files = []\n",
    "    for month in months:\n",
    "        year_folder = os.path.join(base_folder, str(month.year))\n",
    "        fname = f\"MARS3D_{month.strftime('%Y%m')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "    return files\n",
    "\n",
    "def get_daily_paths(dt_start, dt_end, base_folder, stat_type):\n",
    "    files = []\n",
    "    for day in (dt_start + timedelta(n) for n in range((dt_end - dt_start).days + 1)):\n",
    "        year_folder = os.path.join(base_folder, str(day.year))\n",
    "        fname = f\"MARS3D_{day.strftime('%Y%m%d')}_{stat_type}.nc\"\n",
    "        fpath = os.path.join(year_folder, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            files.append(fpath)\n",
    "    return files\n",
    "\n",
    "def get_ws_vel_for_poly(poly, ncdf_path):\n",
    "    ds = xr.open_dataset(ncdf_path, engine=\"netcdf4\")\n",
    "    ws = ds['WINDSTRESS'].isel(time=0)\n",
    "    vel = ds['VELOCITY'].isel(time=0, level=-1)\n",
    "\n",
    "    if ws.rio.crs is None:\n",
    "        ws = ws.rio.write_crs(\"EPSG:4326\")\n",
    "    if vel.rio.crs is None:\n",
    "        vel = vel.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    poly_gs = gpd.GeoSeries([poly], crs=\"EPSG:4326\")\n",
    "    poly_proj = poly_gs.to_crs(ws.rio.crs).iloc[0]\n",
    "\n",
    "    # --- Clip vectorisé ---\n",
    "    ws_clip = ws.rio.clip([poly], all_touched=True, drop=False)\n",
    "    vel_clip = vel.rio.clip([poly], all_touched=True, drop=False)\n",
    "\n",
    "    ws_vals = ws_clip.values.flatten()\n",
    "    ws_vals = ws_vals[~np.isnan(ws_vals)]\n",
    "    vel_vals = vel_clip.values.flatten()\n",
    "    vel_vals = vel_vals[~np.isnan(vel_vals)]\n",
    "\n",
    "    # --- Fallback 3 pixels les plus proches si aucun pixel ---\n",
    "    if len(ws_vals) == 0 or len(vel_vals) == 0:\n",
    "        transform = ws.rio.transform()\n",
    "        h, w = ws.shape\n",
    "        xs = np.arange(w) + 0.5\n",
    "        ys = np.arange(h) + 0.5\n",
    "        xv, yv = transform * np.meshgrid(xs, ys)\n",
    "\n",
    "        if len(ws_vals) == 0:\n",
    "            ws_all = ws.values\n",
    "            valid_idx = ~np.isnan(ws_all)\n",
    "            coords = np.column_stack([xv[valid_idx], yv[valid_idx]])\n",
    "            values = ws_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            ws_vals = values[idx]\n",
    "\n",
    "        if len(vel_vals) == 0:\n",
    "            vel_all = vel.values\n",
    "            valid_idx = ~np.isnan(vel_all)\n",
    "            coords = np.column_stack([xv[valid_idx], yv[valid_idx]])\n",
    "            values = vel_all[valid_idx]\n",
    "            tree = cKDTree(coords)\n",
    "            px, py = poly.centroid.x, poly.centroid.y\n",
    "            _, idx = tree.query([px, py], k=min(3, len(values)))\n",
    "            vel_vals = values[idx]\n",
    "\n",
    "    ds.close()\n",
    "    return ws_vals, vel_vals\n",
    "\n",
    "# ===============================================================\n",
    "# SCRIPT PRINCIPAL\n",
    "# ===============================================================\n",
    "\n",
    "def main():\n",
    "    daily_base_folder = # à modifier \"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/CUR-WIND_latlon/Daily/Corse2\"\n",
    "    monthly_base_folder = # à modifier \"/home/paulinev/Bureau/Marbec_data/BiodivMed/MARS3D/Med_MENOR/Aggregated/CUR-WIND_latlon/Monthly/Corse2\"\n",
    "\n",
    "    gdf = gpd.read_file() # à modifier\n",
    "\n",
    "    ws_max, ws_min, ws_mean = [], [], []\n",
    "    vel_max, vel_min, vel_mean = [], [], []\n",
    "\n",
    "    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc=\"Extraction 1 an WS/VEL pondérée\"):\n",
    "        dt = row['date']\n",
    "        dt_start = dt - timedelta(days=365)\n",
    "        last_month_start = dt.replace(day=1)\n",
    "\n",
    "        try:\n",
    "            for stat in ['max', 'min', 'mean']:\n",
    "                files_monthly = get_monthly_paths(dt_start, last_month_start, monthly_base_folder, stat)\n",
    "                files_daily = get_daily_paths(last_month_start, dt, daily_base_folder, stat)\n",
    "                files = files_monthly + files_daily\n",
    "\n",
    "                all_ws, all_vel = [], []\n",
    "                for f in files:\n",
    "                    ws_vals, vel_vals = get_ws_vel_for_poly(row['geometry'], f)\n",
    "                    all_ws.extend(ws_vals)\n",
    "                    all_vel.extend(vel_vals)\n",
    "\n",
    "                if stat == 'max':\n",
    "                    ws_max.append(np.nanmax(all_ws) if all_ws else np.nan)\n",
    "                    vel_max.append(np.nanmax(all_vel) if all_vel else np.nan)\n",
    "                elif stat == 'min':\n",
    "                    ws_min.append(np.nanmin(all_ws) if all_ws else np.nan)\n",
    "                    vel_min.append(np.nanmin(all_vel) if all_vel else np.nan)\n",
    "                elif stat == 'mean':\n",
    "                    ws_mean.append(np.nanmean(all_ws) if all_ws else np.nan)\n",
    "                    vel_mean.append(np.nanmean(all_vel) if all_vel else np.nan)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"⚠️ Fichier manquant pour {dt} : {e}\")\n",
    "            ws_max.append(np.nan)\n",
    "            ws_min.append(np.nan)\n",
    "            ws_mean.append(np.nan)\n",
    "            vel_max.append(np.nan)\n",
    "            vel_min.append(np.nan)\n",
    "            vel_mean.append(np.nan)\n",
    "\n",
    "    gdf['ws_max_1y'] = ws_max\n",
    "    gdf['ws_min_1y'] = ws_min\n",
    "    gdf['ws_mean_1y'] = ws_mean\n",
    "    gdf['vel_max_1y'] = vel_max\n",
    "    gdf['vel_min_1y'] = vel_min\n",
    "    gdf['vel_mean_1y'] = vel_mean\n",
    "\n",
    "    gdf.to_file(\"adne_extract_corse.geojson\", driver=\"GeoJSON\")# à modifier\n",
    "\n",
    "# ===============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
