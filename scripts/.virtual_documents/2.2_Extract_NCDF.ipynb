


import calendar
import copernicusmarine
import dask
from datetime import datetime, timedelta
import exactextract as ee
from exactextract import exact_extract
import geopandas as gpd
import math
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
from datetime import datetime, timedelta
import rasterio
from rasterio.mask import mask
from rasterio.features import geometry_mask
import rioxarray as rxr
from shapely.geometry import mapping, shape
from shapely.geometry import mapping, Point
from scipy.spatial import cKDTree
import time 
from tqdm import tqdm
import xarray as xr



os.chdir("/media/marieke/Shared/Chap-1/Model/Scripts/Chap_1_2018-2024")








# Set parameters
data_request = {
   "dataset_id" : "cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D",
   "longitude" : [3, 9.65], 
   "latitude" : [41.2, 44],
   "time" : ["2013-01-01", "2025-01-01"],
   "variables" : ["CHL"]
}

# Load xarray dataset
chl = copernicusmarine.open_dataset(
    dataset_id = data_request["dataset_id"],
    minimum_longitude = data_request["longitude"][0],
    maximum_longitude = data_request["longitude"][1],
    minimum_latitude = data_request["latitude"][0],
    maximum_latitude = data_request["latitude"][1],
    start_datetime = data_request["time"][0],
    end_datetime = data_request["time"][1],
    variables = data_request["variables"]
)

# Export to NCDF 
chl.to_netcdf("./data/raw_data/predictors/Chl/cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D_20130101-20250101.nc")












# Set parameters
data_request = {
   "dataset_id" : "med-ogs-bio-rean-d",
   "longitude" : [3, 9.65], 
   "latitude" : [41.2, 44],
   "time" : ["2013-01-01", "2025-01-01"],
   "variables" : ["02"]
}

# Load xarray dataset
ox = copernicusmarine.open_dataset(
    dataset_id = data_request["dataset_id"],
    minimum_longitude = data_request["longitude"][0],
    maximum_longitude = data_request["longitude"][1],
    minimum_latitude = data_request["latitude"][0],
    maximum_latitude = data_request["latitude"][1],
    start_datetime = data_request["time"][0],
    end_datetime = data_request["time"][1],
    variables = data_request["variables"]
)

# Export to NCDF 
ph.to_netcdf("./data/raw_data/predictors/oxygen/med-ogs-bio-rean-d _20130101-20250101.nc")






# Set parameters
data_request = {
   "dataset_id" : "cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D",
   "longitude" : [3, 9.65], 
   "latitude" : [41.2, 44],
   "time" : ["2013-01-01", "2025-01-01"],
   "variables" : ["CHL"]
}

# Load xarray dataset
chl = copernicusmarine.open_dataset(
    dataset_id = data_request["dataset_id"],
    minimum_longitude = data_request["longitude"][0],
    maximum_longitude = data_request["longitude"][1],
    minimum_latitude = data_request["latitude"][0],
    maximum_latitude = data_request["latitude"][1],
    start_datetime = data_request["time"][0],
    end_datetime = data_request["time"][1],
    variables = data_request["variables"]
)

# Export to NCDF 
chl.to_netcdf("./data/raw_data/predictors/Chl/cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D_20130101-20250101.nc")









# Set parameters
data_request = {
   "dataset_id" : "SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2",
   "longitude" : [3, 9.65], 
   "latitude" : [41.2, 44],
   "time" : ["2013-01-01", "2025-01-01"],
   "variables" : ["analysed_sst"]
}

# Load xarray dataset
sst = copernicusmarine.open_dataset(
    dataset_id = data_request["dataset_id"],
    minimum_longitude = data_request["longitude"][0],
    maximum_longitude = data_request["longitude"][1],
    minimum_latitude = data_request["latitude"][0],
    maximum_latitude = data_request["latitude"][1],
    start_datetime = data_request["time"][0],
    end_datetime = data_request["time"][1],
    variables = data_request["variables"]
)

# Export to NCDF 
sst.to_netcdf("./data/raw_data/predictors/SST/SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2_SST_20130101-20250101.nc")









# Set parameters
data_request = {
   "dataset_id" : "SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2",
   "longitude" : [3, 9.65], 
   "latitude" : [41.2, 44],
   "time" : ["2013-01-01", "2025-01-01"],
   "variables" : ["analysed_sst"]
}

# Load xarray dataset
sst = copernicusmarine.open_dataset(
    dataset_id = data_request["dataset_id"],
    minimum_longitude = data_request["longitude"][0],
    maximum_longitude = data_request["longitude"][1],
    minimum_latitude = data_request["latitude"][0],
    maximum_latitude = data_request["latitude"][1],
    start_datetime = data_request["time"][0],
    end_datetime = data_request["time"][1],
    variables = data_request["variables"]
)

# Export to NCDF 
sst.to_netcdf("./data/raw_data/predictors/SST/SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2_SST_20130101-20250101.nc")









!pip install exactextract


def get_dates(date, time_step):
    """
    Calculate the range of dates for a given time step relative to the provided date.
    """
    from datetime import datetime, timedelta

    if isinstance(date, str):
        date = datetime.strptime(date, "%Y-%m-%d")

    end_date = date - timedelta(days=1)

    time_deltas = {
        'day': 1,
        'week': 7,
        'month': 30,
        'year': 365,
        '5years': 5 * 365 + 1,
    }

    if time_step not in time_deltas:
        raise ValueError("Unsupported time step. Choose from 'day', 'week', 'month', 'year', or '5years'.")

    start_date = date - timedelta(days=time_deltas[time_step])
    return start_date, end_date


def compute_stats_exactextract(data_array, shape_geometry, max_buffer_distance=0, step=0.01,
                               nodata=None, tmp_dir=None):
    """
    Using exactextract, extract pixel values & coverage fractions for shape_geometry and compute:
      - coverage-weighted mean
      - min (over pixels with coverage > 0)
      - max (over pixels with coverage > 0)

    If the polygon returns no valid pixels, will expand the geometry by successive buffers up to max_buffer_distance.

    Parameters
    ----------
    data_array : xarray.DataArray
        single-band raster (can be multi-time -> but pass the aggregated slice already e.g. time-mean or stack)
        must have georeference (rioxarray metadata) and CRS EPSG:4326 in your workflow.
    shape_geometry : shapely geometry
    max_buffer_distance : float
        maximum buffer to try (in degrees if CRS is EPSG:4326)
    step : float
        buffer increment
    nodata : numeric or None
        nodata value to set in the temporary tiff if desired. If None, attempt to detect from data_array.
    tmp_dir : str or None
        directory for temporary files. If None, uses system temp.

    Returns
    -------
    mean_val, min_val, max_val, used_buffer_distance
    """
    # Attempt to infer nodata
    if nodata is None:
        try:
            nodata = data_array.rio.nodata
        except Exception:
            nodata = None

    current_buffer_distance = 0.0

    # Ensure data_array has spatial metadata (rioxarray)
    if not hasattr(data_array, "rio"):
        raise ValueError("data_array must be a rioxarray-enabled xarray.DataArray with CRS and transform.")

    # reduce to single band if multiple bands remain (assumes 2D or 3D with single band)
    # The calling code should pass a DataArray containing the values of interest (e.g. averaged over time)
    while current_buffer_distance <= max_buffer_distance + 1e-12:
        if current_buffer_distance > 0:
            search_geometry = shape_geometry.buffer(current_buffer_distance)
        else:
            search_geometry = shape_geometry

        try:
            # Write the data_array to a temporary GeoTIFF file (exactextract expects a raster file)
            with tempfile.NamedTemporaryFile(suffix=".tif", dir=tmp_dir, delete=False) as tmp:
                tmp_path = tmp.name

            # rioxarray's to_raster / rio.to_raster
            # For safety, ensure data_array is 2D (if it is a time stack, caller should collapse to single 2D)
            # If there's an extra dim (e.g., time), the first non-time dim will be written; adjust if needed.
            # We will call .rio.to_raster which writes GeoTIFF.
            da_to_write = data_array

            # If data_array has a 'time' dimension, it must be collapsed before writing. The caller should send
            # a single time-slice or aggregated array. For safety, if time present and size==1, select it.
            if "time" in da_to_write.dims and da_to_write.sizes["time"] == 1:
                da_to_write = da_to_write.isel(time=0)

            # Ensure 2D
            if len(da_to_write.dims) > 2:
                # try to reduce by selecting first non-spatial dim except time
                nonspatial = [d for d in da_to_write.dims if d not in ("x", "y", "lon", "lat")]
                if nonspatial:
                    da_to_write = da_to_write.isel({nonspatial[0]: 0})
                else:
                    # fallback: take first 2D slice
                    dims = da_to_write.dims
                    da_to_write = da_to_write.isel({dims[0]: 0})

            # Write GeoTIFF
            # Ensure CRS is set
            try:
                if da_to_write.rio.crs is None:
                    da_to_write = da_to_write.rio.write_crs("EPSG:4326", inplace=False)
            except Exception:
                # ignore and proceed
                pass

            # write
            da_to_write.rio.to_raster(tmp_path)

            # Now call exactextract
            # exactextract.extract expects either a path or rasterio dataset and a GeoJSON-like geometry.
            geojson_geom = mapping(search_geometry)

            # 'mode' : can be 'accurate' to compute fractional coverage. Use that explicitly.
            # The exactextract Python API returns a list of records (one per geometry). Each record
            # contains arrays 'values' and 'coverage_fraction' (naming mirrors R version).
            # We'll be defensive when parsing the returned structure.
            ee_result = ee.extract(tmp_path, [geojson_geom], mode="accurate", bands=1)

            # ee.extract returns a list with one element per input geometry
            if not ee_result or len(ee_result) == 0:
                # no data found; try next buffer
                os.remove(tmp_path)
                current_buffer_distance += step
                continue
                
            print("-----------------------------------------------------------------------------------------------")
            print(ee_result)
            
            rec = ee_result[0]
            

            # Expected fields: 'values' and 'coverage_fraction' OR 'values' and 'coverage_area' depending on version.
            values = None
            cov = None

            # Try common keys
            if isinstance(rec, dict):
                # Many examples return keys like 'values' and 'coverage_fraction'
                if "values" in rec and "coverage_fraction" in rec:
                    values = rec["values"]
                    cov = rec["coverage_fraction"]
                elif "values" in rec and "coverage_area" in rec:
                    values = rec["values"]
                    cov = rec["coverage_area"]
                elif "value" in rec and "coverage_fraction" in rec:
                    values = rec["value"]
                    cov = rec["coverage_fraction"]
                else:
                    # try to inspect keys and pick arrays
                    for k in rec:
                        if isinstance(rec[k], (list, tuple)) and values is None:
                            values = rec[k]
                        elif isinstance(rec[k], (list, tuple)) and cov is None and rec[k] is not values:
                            cov = rec[k]

            # If not dict, maybe it's a pandas-like DataFrame. Try to coerce.
            if values is None or cov is None:
                # try treating rec as a table-like sequence of rows: search for columns named:
                try:
                    import pandas as _pd
                    df = _pd.DataFrame(rec)
                    if "values" in df.columns and "coverage_fraction" in df.columns:
                        values = df["values"].tolist()
                        cov = df["coverage_fraction"].tolist()
                    elif "value" in df.columns and "coverage_fraction" in df.columns:
                        values = df["value"].tolist()
                        cov = df["coverage_fraction"].tolist()
                    elif "values" in df.columns and "coverage_area" in df.columns:
                        values = df["values"].tolist()
                        cov = df["coverage_area"].tolist()
                except Exception:
                    pass

            # cleanup tmp file
            try:
                os.remove(tmp_path)
            except Exception:
                pass

            if values is None or cov is None:
                # no usable extraction; try next buffer distance
                current_buffer_distance += step
                continue

            # Convert to numpy arrays and mask nodata or NaNs
            import numpy as np
            values = np.array(values, dtype=float)
            cov = np.array(cov, dtype=float)

            # Mask invalid values (NaN, or nodata)
            mask_valid = ~np.isnan(values)
            if nodata is not None:
                mask_valid = mask_valid & (values != nodata)

            # Also require coverage > 0
            mask_valid = mask_valid & (cov > 0)

            if mask_valid.sum() == 0:
                # no valid pixels; expand buffer
                current_buffer_distance += step
                continue

            values_sel = values[mask_valid]
            cov_sel = cov[mask_valid]

            # weighted mean
            weighted_sum = (values_sel * cov_sel).sum()
            total_cov = cov_sel.sum()

            if total_cov == 0:
                mean_val = None
            else:
                mean_val = (weighted_sum / total_cov).item()

            # For min / max we'll use the min and max among pixels intersected (coverage>0).
            min_val = float(values_sel.min())
            max_val = float(values_sel.max())

            return mean_val, min_val, max_val, current_buffer_distance

        except Exception as e:
            # ignore and try next buffer distance (but log if you want)
            # print(f"exactextract error at buffer {current_buffer_distance}: {e}")
            try:
                os.remove(tmp_path)
            except Exception:
                pass

            current_buffer_distance += step
            continue

    # nothing found up to max_buffer_distance
    return None, None, None, max_buffer_distance


def open_nc(shape_geometry, date, netcdf_path, variable="CHL"):
    """
    Compute CHL statistics for a given geometry and date using a netCDF file.

    Now uses exactextract-based weighted means (coverage-weighted).
    """
    results = {}

    try:
        if isinstance(date, str):
            date = datetime.strptime(date, "%Y-%m-%d")

        ds = xr.open_dataset(netcdf_path)
        ds = ds.rio.write_crs("EPSG:4326", inplace=True)

        target_date = date - timedelta(days=1)
        time_steps = ["day", "week", "month", "year", "5years"]
        date_ranges = {label: get_dates(date, label) for label in time_steps}

        for label, (start_date, end_date) in date_ranges.items():
            ds_time_range = ds.sel(time=slice(start_date, end_date))

            if ds_time_range.time.size == 0:
                results[label] = (None, None, None, 0)
                continue

            chl_data = ds_time_range[variable]

            # count how many days are fully empty (all NaN) in the time slice
            empty_days = sum(chl_data.sel(time=t).isnull().all().item() for t in chl_data.time)

            # Drop fully-empty time steps; we will aggregate across time as the user's original code did,
            # but the original code passed the whole valid_data stack to compute_stats which used rioxarray.clip.
            # Here we will compute per-pixel mean over time for the valid pixels (so exactextract receives a single 2D raster)
            valid_data = chl_data.dropna(dim="time", how="all")
            if valid_data.size > 0:
                # We will compute the per-pixel mean across time where values exist (keeping NaNs where all times are NaN)
                # result is 2D DataArray with same spatial coords.
                # If instead you want to compute zonal statistics per day and then aggregate across days, we can change approach.
                per_pixel_mean = valid_data.mean(dim="time", skipna=True)

                mean_val, min_val, max_val, used_buffer = compute_stats_exactextract(
                    per_pixel_mean, shape_geometry, max_buffer_distance=0.1, step=0.01
                )

                results[label] = (mean_val, min_val, max_val, empty_days)
            else:
                results[label] = (None, None, None, empty_days)

        return results

    except Exception as e:
        print(f"Error processing shape with target date: {date}: {e}")
        return {}


def process_geojson(geojson_path, netcdf_path, output_path, variable="CHL"):
    """
    Process the GeoJSON file and compute statistics for each shape using a netCDF file.
    """
    shapes = gpd.read_file(geojson_path)
    shapes = shapes.set_crs("EPSG:4326", allow_override=True)

    shapes = shapes[0:3]

    results = []

    for _, row in tqdm(shapes.iterrows(), total=shapes.shape[0], desc="Processing shapes"):
        shape_geometry = row.geometry
        date = row["date"]
        polygon_id = row.get("replicates", None)

        chl_stats = open_nc(shape_geometry, date, netcdf_path, variable)

        result_entry = {"replicates": polygon_id}
        for label, (mean, min_val, max_val, empty_days) in chl_stats.items():
            result_entry[f"Cop_CHL_{label}_mean"] = mean
            result_entry[f"Cop_CHL_{label}_min"] = min_val
            result_entry[f"Cop_CHL_{label}_max"] = max_val
            result_entry[f"Cop_CHL_{label}_empty_days"] = empty_days

        results.append(result_entry)

    results_df = pd.DataFrame(results)
    results_df.to_csv(output_path, index=False)









def get_dates(date, time_step):
    """
    Calculate the range of dates for a given time step relative to the provided date.
    """
    from datetime import datetime, timedelta

    if isinstance(date, str):
        date = datetime.strptime(date, "%Y-%m-%d")

    end_date = date - timedelta(days=1)

    time_deltas = {
        'day': 1,
        'week': 7,
        'month': 30,
        'year': 365,
        '5years': 5 * 365 + 1,
    }

    if time_step not in time_deltas:
        raise ValueError("Unsupported time step. Choose from 'day', 'week', 'month', 'year', or '5years'.")

    start_date = date - timedelta(days=time_deltas[time_step])
    return start_date, end_date




def compute_stats(data_array, shape_geometry, max_buffer_distance=0, step=0.01):
    """
    Compute the mean, min, and max of valid points (non-NaN) within the given geometry using `.clip`.
    Expand the search radius if no valid points exist.
    """
    current_buffer_distance = 0

    while current_buffer_distance <= max_buffer_distance:
        search_geometry = shape_geometry.buffer(current_buffer_distance) if current_buffer_distance > 0 else shape_geometry

        try:
            
            # Extraction is done here :
            clipped_data = data_array.rio.clip([mapping(search_geometry)], crs="EPSG:4326", drop=True)
            # Doc : https://corteva.github.io/rioxarray/html/rioxarray.html#rioxarray.raster_array.RasterArray.clip
            # drop = True :  drop the data outside of the extent of the mask geometries Otherwise, it will return the same raster with the data masked. 
            # all_touched = False (default) : only pixels whose center is within the polygon or that are selected by Bresenham’s line algorithm will be burned in.
            
            if clipped_data.count().item() > 0:
                return (
                    clipped_data.mean().item(),
                    clipped_data.min().item(),
                    clipped_data.max().item(),
                    current_buffer_distance
                )
        except Exception as e:
            pass

        current_buffer_distance += step

    return None, None, None, max_buffer_distance





def open_nc(shape_geometry, date, netcdf_path, variable="CHL"):
    """
    Compute CHL statistics for a given geometry and date using a netCDF file.
    """
    results = {}

    try:
        if isinstance(date, str):
            date = datetime.strptime(date, "%Y-%m-%d")

        ds = xr.open_dataset(netcdf_path)
        ds = ds.rio.write_crs("EPSG:4326", inplace=True)

        target_date = date - timedelta(days=1)
        time_steps = ["day", "week", "month", "year", "5years"]
        date_ranges = {label: get_dates(date, label) for label in time_steps}

        for label, (start_date, end_date) in date_ranges.items():
            ds_time_range = ds.sel(time=slice(start_date, end_date))

            if ds_time_range.time.size == 0:
                results[label] = (None, None, None, 0)
                continue

            chl_data = ds_time_range[variable]
            empty_days = sum(chl_data.sel(time=t).isnull().all().item() for t in chl_data.time)
            valid_data = chl_data.dropna(dim="time", how="all")

            if valid_data.size > 0:
                mean_val, min_val, max_val, max_search_dist = compute_stats(
                    valid_data, shape_geometry, max_buffer_distance=0, step=0.01
                )
                results[label] = (mean_val, min_val, max_val, empty_days)
            else:
                results[label] = (None, None, None, empty_days)

        return results

    except Exception as e:
        print(f"Error processing shape with target date: {date}: {e}")
        return {}





def process_geojson(geojson_path, netcdf_path, output_path, variable="CHL"):
    """
    Process the GeoJSON file and compute statistics for each shape using a netCDF file.
    """
    shapes = gpd.read_file(geojson_path)
    shapes = shapes.set_crs("EPSG:4326", allow_override=True)
    shapes = shapes[0:3]

    results = []

    for _, row in tqdm(shapes.iterrows(), total=shapes.shape[0], desc="Processing shapes"):
        shape_geometry = row.geometry
        date = row["date"]
        polygon_id = row.get("replicates", None)

        chl_stats = open_nc(shape_geometry, date, netcdf_path, variable)

        result_entry = {"replicates": polygon_id}
        for label, (mean, min_val, max_val, empty_days) in chl_stats.items():
            result_entry[f"Cop_CHL_{label}_mean"] = mean
            result_entry[f"Cop_CHL_{label}_min"] = min_val
            result_entry[f"Cop_CHL_{label}_max"] = max_val
            result_entry[f"Cop_CHL_{label}_empty_days"] = empty_days

        results.append(result_entry)

    results_df = pd.DataFrame(results)
    results_df.to_csv(output_path, index=False)








def get_dates(date, time_step):
    """
    Calculate the range of dates for a given time step relative to the provided date.
    """
    from datetime import datetime, timedelta

    if isinstance(date, str):
        date = datetime.strptime(date, "%Y-%m-%d")

    end_date = date - timedelta(days=1)

    time_deltas = {
        'day': 1,
        'week': 7,
        'month': 30,
        'year': 365,
        '5years': 5 * 365 + 1,
    }

    if time_step not in time_deltas:
        raise ValueError("Unsupported time step. Choose from 'day', 'week', 'month', 'year', or '5years'.")

    start_date = date - timedelta(days=time_deltas[time_step])
    return start_date, end_date






def compute_stats(data_array, shape_geometry):
    """
    Extract values and compute weighted mean - automatically with exactextract- , min and max.
    """
    
    try:
        feature = {"type": "Feature", "geometry": mapping(shape_geometry), "properties": {}}
        res = exact_extract(data_array, [feature], ["mean", "min", "max"])


        if not res or len(res) == 0:
            return None, None, None

        props = res[0]["properties"]

        # Multi-band keys: band_1_mean, band_2_mean, etc.
        mean_vals = [v for k, v in props.items() if k.endswith("_mean")]
        min_vals  = [v for k, v in props.items() if k.endswith("_min")]
        max_vals  = [v for k, v in props.items() if k.endswith("_max")]

         # Single-band keys: mean, min, max
        if not mean_vals:
            if "mean" in props:
                mean_vals = [props["mean"]]
        if not min_vals:
            if "min" in props:
                min_vals = [props["min"]]
        if not max_vals:
            if "max" in props:
                max_vals = [props["max"]]
   

        if not mean_vals or not min_vals or not max_vals:
            return None, None, None

        mean_val = float(np.nanmean(mean_vals))
        min_val  = float(np.nanmin(min_vals))
        max_val  = float(np.nanmax(max_vals))

        return mean_val, min_val, max_val

    except Exception as e:
        print(f"compute_stats ERROR: {e}")
        return None, None, None







def open_nc(shape_geometry, date, netcdf_path, variable="CHL"):
    """
    Compute NCDF statistics for a given geometry and date using a netCDF file.
    """
    results = {}

    try:
        if isinstance(date, str):
            date = datetime.strptime(date, "%Y-%m-%d")

        ds = xr.open_dataset(netcdf_path)
        ds = ds.rio.write_crs("EPSG:4326", inplace=True)


        target_date = date - timedelta(days=1)
        time_steps = ["day", "week", "month", "year", "5years"]
        date_ranges = {label: get_dates(date, label) for label in time_steps}


        for label, (start_date, end_date) in date_ranges.items():
            ds_time_range = ds.sel(time=slice(start_date, end_date))

            if ds_time_range.time.size == 0:
                results[label] = (None, None, None, 0)
                continue

            chl_data = ds_time_range[variable]
            valid_data = chl_data.dropna(dim="time", how="all")


            if valid_data.size > 0:                 
                mean_val, min_val, max_val = compute_stats(valid_data, shape_geometry)                
                results[label] = (mean_val, min_val, max_val)
            else:
                print("valid_data.size == 0")
                results[label] = (None, None, None)

        return results

    except Exception as e:
        print(f"Error processing shape with target date: {date}: {e}")
        return {}





def process_geojson(geojson_path, netcdf_path, output_path, variable="CHL"):
    """
    Process the GeoJSON file and compute statistics for each shape using a netCDF file.
    """
    shapes = gpd.read_file(geojson_path)
    shapes = shapes.set_crs("EPSG:4326", allow_override=True)
    shapes = shapes[0:20]

    results = []

    for _, row in tqdm(shapes.iterrows(), total=shapes.shape[0], desc="Processing shapes"):
        shape_geometry = row.geometry
        date = row["date"]
        polygon_id = row.get("replicates", None)

        nc_stats = open_nc(shape_geometry, date, netcdf_path, variable)

        print("------------replicates--------------")
        print(polygon_id)
        print("------------nc_stats--------------")
        print(nc_stats)


        result_entry = {"replicates": polygon_id}
        for label, (mean, min_val, max_val) in nc_stats.items():
            result_entry[f"Cop_CHL_{label}_mean"] = mean
            result_entry[f"Cop_CHL_{label}_min"] = min_val
            result_entry[f"Cop_CHL_{label}_max"] = max_val

        results.append(result_entry)

    results_df = pd.DataFrame(results)
    results_df.to_csv(output_path, index=False)



geojson_path="./data/processed_data/eDNA/mtdt_5.geojson"
netcdf_path="./data/raw_data/predictors/Chl/cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D_20130101-20250101.nc"
output_path="./data/processed_data/predictors/mtdt_5_CHL_test.csv"


process_geojson(
    geojson_path=geojson_path,
    netcdf_path=netcdf_path,
    output_path=output_path,
    variable="CHL"  
)






# 17/10/2025 : Extract CHL from cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D 

# 1. Convert .shp to .geojson 
# Load the file with buffer for extraction
gdf = gpd.read_file("./data/processed_data/eDNA/mtdt_5.gpkg")

# Save as GeoJSON
geojson_path = "./data/processed_data/eDNA/mtdt_5.geojson"
gdf.to_file(geojson_path, driver="GeoJSON")

print(f"GeoJSON file saved to {geojson_path}")



# 2. Make extraction (using Fct 4 and max buffer size = 0)

geojson_path="./data/processed_data/eDNA/mtdt_5.geojson"
netcdf_path="./data/raw_data/predictors/Chl/cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D_20130101-20250101.nc"
output_path="./data/processed_data/predictors/mtdt_5_CHL.csv"


process_geojson(
    geojson_path=geojson_path,
    netcdf_path=netcdf_path,
    output_path=output_path,
    variable="CHL"  
)






#  Extract SST from SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2_SST_20130101-20250101.nc 

# 2. Make extraction (using Fct 4 and max buffer size = 0)

geojson_path="./data/processed_data/eDNA/mtdt_5.geojson"
netcdf_path="./data/raw_data/predictors/SST/SST_MED_SST_L4_NRT_OBSERVATIONS_010_004_c_V2_SST_20130101-20250101.nc"
output_path="./data/processed_data/predictors/mtdt_5_SST.csv"


process_geojson(
    geojson_path=geojson_path,
    netcdf_path=netcdf_path,
    output_path=output_path,
    variable="SST"  
)






